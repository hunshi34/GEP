{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:08.764884Z",
     "start_time": "2025-03-14T02:05:08.643478Z"
    }
   },
   "source": "! export CUDA_VISIBLE_DEVICES=0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:10.260722Z",
     "start_time": "2025-03-14T02:05:08.882064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    # 获取 CUDA 版本（这是运行时使用的 CUDA 版本）\n",
    "    print(f\"CUDA version used by PyTorch: {torch.version.cuda}\")"
   ],
   "id": "abe80e37fcc12368",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version used by PyTorch: 12.1\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:10.415002Z",
     "start_time": "2025-03-14T02:05:10.411585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ],
   "id": "dfb3e50c0e9b64b7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.462554Z",
     "start_time": "2025-03-14T02:05:41.789644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import pickle\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "# import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from transformers import AutoTokenizer,DebertaV2Tokenizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model.scgpt_memVP import scgpt_memvp\n",
    "from scgpt.model import AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "id": "2d25becdaf951753",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xh/.conda/envs/memvp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 33\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorchtext\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvocab\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Vocab\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorchtext\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_torchtext\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     31\u001B[0m     Vocab \u001B[38;5;28;01mas\u001B[39;00m VocabPybind,\n\u001B[1;32m     32\u001B[0m )\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer,DebertaV2Tokenizer\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m confusion_matrix\n\u001B[1;32m     36\u001B[0m sys\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39minsert(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'transformers'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.475979Z",
     "start_time": "2025-01-02T03:56:36.989938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class config:\n",
    "    seed=0\n",
    "    dataset_name=\"immune\"\n",
    "    do_train=True\n",
    "    load_model=\"/home/xh/memVP/scgpt/checkpoint/scgpt-human\"\n",
    "    mask_ratio=0.0\n",
    "    epochs=20\n",
    "    n_bins=51\n",
    "    MVC=False # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0 # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0\n",
    "    lr=1e-4\n",
    "    batch_size=8\n",
    "    layer_size=128\n",
    "    nlayers=4  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=4  # number of heads in nn.MultiheadAttention\n",
    "    dropout=0.2  # dropout probability\n",
    "    schedule_ratio=0.9  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5\n",
    "    fast_transformer=True\n",
    "    pre_norm=False\n",
    "    amp=True  # Automatic Mixed Precision\n",
    "    include_zero_gene = False\n",
    "    freeze = False #freeze\n",
    "    DSBN = False # Domain-spec batchnorm\n",
    "    text_decoder_path=\"/HDDDATA/XieeeHuiii/checkpoints/deberta\"\n",
    " #    select_features =['n_genes', 'n_genes_by_counts', 'total_counts',\n",
    " # 'total_counts_mt', 'pct_counts_mt', 'Resample',\n",
    " # 'Collection_Day', 'Sex', 'Age_interval', 'Swab_result', 'Status', 'Smoker', 'Status_on_day_collection',\n",
    " # 'Status_on_day_collection_summary', 'Days_from_onset', 'Site', 'time_after_LPS', 'Worst_Clinical_Status',\n",
    " # 'Outcome']#covid\n",
    "    select_features=['species', 'disease__ontology_label',  'organ__ontology_label',\n",
    "       'library_preparation_protocol__ontology_label', 'sex', 'Age_range']\n"
   ],
   "id": "e50dc997067495b7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.476243Z",
     "start_time": "2025-01-02T03:56:37.142237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# # 初始化wandb\n",
    "# run = wandb.init(\n",
    "#     config=hyperparameter_defaults,\n",
    "#     project=\"memVP\",\n",
    "#     reinit=True,\n",
    "# )\n",
    "# \n",
    "# # 获取配置\n",
    "# config = wandb.config\n",
    "# # print(config)\n",
    "# # \n",
    "# # \n",
    "# set_seed(config.seed)"
   ],
   "id": "4b3a42701f3d1729",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.476373Z",
     "start_time": "2025-01-02T03:56:37.291575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "include_zero_gene = config.include_zero_gene  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 1025\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "\n",
    "# settings for training\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = True  # celltype classification objective\n",
    "ADV = False  # Adversarial training for batch correction\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = config.MVC  # Masked value prediction for cell embedding\n",
    "print(config.ecs_thres)\n",
    "ECS = config.ecs_thres > 0  # Elastic cell similarity objective\n",
    "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "mvc_decoder_style = \"inner product\"\n",
    "\n",
    "ecs_threshold = config.ecs_thres\n",
    "dab_weight = config.dab_weight\n",
    "\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for optimizer\n",
    "lr = config.lr  # TODO: test learning rate ratio between two tasks\n",
    "lr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "schedule_interval = 1\n",
    "\n",
    "# settings for the model\n",
    "fast_transformer = config.fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability\n",
    "\n",
    "# logging\n",
    "log_interval = 100  # iterations\n",
    "save_eval_interval = config.save_eval_interval  # epochs\n",
    "do_eval_scib_metrics = True"
   ],
   "id": "54797c03154b67fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.476496Z",
     "start_time": "2025-01-02T03:56:37.413127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False"
   ],
   "id": "6d31e01d372ad09c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.476580Z",
     "start_time": "2025-01-02T03:56:37.520995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ],
   "id": "e6c6e3a061f168ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/dev_immune-Jan02-11-56\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.476679Z",
     "start_time": "2025-01-02T03:56:37.675601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if dataset_name == \"ms\":\n",
    "    data_dir = Path(\"../data/ms\")\n",
    "    adata = sc.read(data_dir / \"c_data.h5ad\")\n",
    "    adata_test = sc.read(data_dir / \"filtered_ms_adata.h5ad\")\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "elif dataset_name == \"covid\":        \n",
    "    data_dir = Path(\"/home/xh/data/covid-19\")\n",
    "    adata = sc.read(data_dir / \"filtered_covid_portal_train_20k.h5ad\")\n",
    "    adata_test = sc.read(data_dir / \"filtered_covid_portal_test_10k.h5ad\")\n",
    "    with open('/home/xh/memVP/checkpoint/MemVP-covid-2/select_genes.json', 'r') as f:\n",
    "        gene_list = json.load(f)\n",
    "    adata = adata[:,gene_list]\n",
    "    adata_test=adata_test[:,gene_list]\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"full_clustering\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"full_clustering\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    # adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    # adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "    \n",
    "elif dataset_name == \"pbmc\":\n",
    "    data_dir = Path(\"/home/xh/data/pkmc-160k\")\n",
    "    adata = sc.read(data_dir / \"pbmc_gene.h5ad\")\n",
    "    train_bool = [x in ['P1', 'P3', 'P4', 'P7'] for x in adata.obs['donor']]\n",
    "    adata_test = adata[np.invert(train_bool)]  # 直接过滤adata，保留不在训练集中的数据\n",
    "    adata = adata[train_bool]\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"celltype.l3\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"celltype.l3\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    # adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    # adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "elif  dataset_name == \"heart\":\n",
    "    data_dir = Path(\"/home/xh/data/others/\")\n",
    "    adata = sc.read(data_dir / \"train_heart_data.h5ad\")\n",
    "    adata_test = sc.read(data_dir / \"test_heart_data.h5ad\") \n",
    "    with open('/home/xh/memVP/checkpoint/MemVP-heart/select_genes.json', 'r') as f:\n",
    "        gene_list = json.load(f)\n",
    "    adata = adata[:,gene_list]\n",
    "    adata_test=adata_test[:,gene_list]\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"cell_type_leiden0.6\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"cell_type_leiden0.6\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    # adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    # adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "elif dataset_name == \"Macrophages\":\n",
    "    data_dir = Path(\"/home/xh/data/Macrophages\")\n",
    "    adata = sc.read(data_dir / \"train_data.h5ad\")\n",
    "    adata_test = sc.read(data_dir / \"test_data.h5ad\") \n",
    "    with open('/home/xh/memVP/checkpoint/MemVP-Macrophages/select_genes.json', 'r') as f:\n",
    "        gene_list = json.load(f)\n",
    "    adata = adata[:,gene_list]\n",
    "    adata_test=adata_test[:,gene_list]\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"new_Cell_Type\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"new_Cell_Type\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    # adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    # adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "elif dataset_name == \"cancer\":\n",
    "    data_dir = Path(\"/home/xh/data/cancer\")\n",
    "    adata = sc.read(data_dir / \"train_data.h5ad\")\n",
    "    adata_test = sc.read(data_dir / \"test_data.h5ad\") \n",
    "    with open('/home/xh/memVP/checkpoint/MemVP-cancer/select_genes.json', 'r') as f:\n",
    "        gene_list = json.load(f)\n",
    "    adata = adata[:,gene_list]\n",
    "    adata_test=adata_test[:,gene_list]\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"CellType\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"CellType\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    # adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    # adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "elif dataset_name == \"immune\":\n",
    "    data_dir = Path(\"/HDDDATA/XieeeHuiii/Data/data/immune\")\n",
    "    adata = sc.read(data_dir / \"train_data.h5ad\")\n",
    "    adata_test = sc.read(data_dir / \"test_data.h5ad\") \n",
    "    with open('/home/xh/memVP/checkpoint/checkpoint/MemVP-immune/select_genes.json', 'r') as f:\n",
    "        gene_list = json.load(f)\n",
    "    adata = adata[:,gene_list]\n",
    "    adata_test=adata_test[:,gene_list]\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"Manually_curated_celltype\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"Manually_curated_celltype\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    # adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    # adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "# make the batch category column\n",
    "batch_id_labels = adata.obs[\"batch_id\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "celltype_id_labels = adata.obs[\"celltype\"].astype(\"category\").cat.codes.values\n",
    "celltypes = adata.obs[\"celltype\"].unique()\n",
    "num_types = len(np.unique(celltype_id_labels))\n",
    "id2type = dict(enumerate(adata.obs[\"celltype\"].astype(\"category\").cat.categories))\n",
    "adata.obs[\"celltype_id\"] = celltype_id_labels\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ],
   "id": "add90adfe7b87555",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.476796Z",
     "start_time": "2025-01-02T03:56:38.681302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]"
   ],
   "id": "92cb3efd9b813377",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 1024/1024 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from /home/xh/memVP/scgpt/checkpoint/scgpt-human/best_model.pt, the model args will override the config /home/xh/memVP/scgpt/checkpoint/scgpt-human/args.json.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.477137Z",
     "start_time": "2025-01-02T03:56:38.914855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "\n",
    "adata_test = adata[adata.obs[\"str_batch\"] == \"1\"]\n",
    "adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n",
    "preprocessor(adata_test, batch_key=None)"
   ],
   "id": "f77ab1531d7e9e6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.477260Z",
     "start_time": "2025-01-02T03:56:43.749915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_text_prompt(adata,select_data,tokenizer,max_length=256):\n",
    "    text_prompt=[]\n",
    "    text_mask=[]\n",
    "    for idx in range(len(adata)):\n",
    "        prompt='A general description of a immune cell.'\n",
    "        for feature in select_data:\n",
    "            if feature in adata:\n",
    "                feature_value = str(adata[feature].iloc[idx])  # 使用 idx 获取值\n",
    "                prompt += f\"{feature} of this cell is {feature_value}.\"\n",
    "        # print(prompt)\n",
    "        out_put=tokenizer(prompt,padding=\"max_length\",truncation=True,max_length=max_length,return_tensors=\"pt\")\n",
    "        text_prompt.append(out_put.input_ids)\n",
    "        text_mask.append(out_put.attention_mask)\n",
    "    text_mask=torch.stack(text_mask,dim=0)\n",
    "    text_prompt=torch.stack(text_prompt)\n",
    "    return (text_prompt,text_mask)\n",
    "    \n",
    "    \n",
    "    "
   ],
   "id": "b3b87c64ecde0208",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.477377Z",
     "start_time": "2025-01-02T03:56:43.824538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "all_obs=adata.obs\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata.obs[\"celltype_id\"].tolist()  # make sure count from 0\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    "    train_prompt,\n",
    "    valid_prompt,\n",
    ") = train_test_split(\n",
    "    all_counts, celltypes_labels,batch_ids,all_obs, test_size=0.1, shuffle=True\n",
    ")\n",
    "tokenizer=DebertaV2Tokenizer.from_pretrained(config.text_decoder_path)\n",
    "train_data=all_counts\n",
    "train_celltype_labels=celltypes_labels\n",
    "train_batch_labels=batch_ids\n",
    "train_prompt=get_text_prompt(all_obs,config.select_features,tokenizer)\n",
    "valid_prompt=get_text_prompt(valid_prompt,config.select_features,tokenizer)\n",
    "# print(train_prompt[0][0])"
   ],
   "id": "5172d880843e5b1a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.477488Z",
     "start_time": "2025-01-02T03:56:54.837357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ],
   "id": "540dbf2b7289df81",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.477561Z",
     "start_time": "2025-01-02T03:56:54.902505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")"
   ],
   "id": "f3e688e78d1498bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 18466, \n",
      "\t feature length: 452\n",
      "scGPT - INFO - valid set number of samples: 1847, \n",
      "\t feature length: 378\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.477639Z",
     "start_time": "2025-01-02T03:56:56.247325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    tensor_celltype_labels_train = torch.from_numpy(train_celltype_labels).long()\n",
    "    tensor_celltype_labels_valid = torch.from_numpy(valid_celltype_labels).long()\n",
    "\n",
    "    if sort_seq_batch:  # TODO: update to random pick seq source in each traning batch\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "        tensor_celltype_labels_train = tensor_celltype_labels_train[train_sort_ids]\n",
    "        train_ids=train_prompt[0][train_sort_ids]\n",
    "        train_mask=train_prompt[1][train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "        tensor_celltype_labels_valid = tensor_celltype_labels_valid[valid_sort_ids]\n",
    "        valid_ids=valid_prompt[0][valid_sort_ids]\n",
    "        valid_mask=valid_prompt[1][valid_sort_ids]\n",
    "    else:\n",
    "        train_ids=train_prompt[0]\n",
    "        train_mask=train_prompt[1]\n",
    "        valid_ids=valid_prompt[0]\n",
    "        valid_mask=valid_prompt[1]\n",
    "        \n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "        \"celltype_labels\": tensor_celltype_labels_train,\n",
    "        \"text\":train_ids,\n",
    "        \"mask\":train_mask,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "        \"celltype_labels\": tensor_celltype_labels_valid,\n",
    "        \"text\":valid_ids,\n",
    "        \"mask\":valid_mask,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n"
   ],
   "id": "5aba0c7b5ba0935f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.477775Z",
     "start_time": "2025-01-02T03:56:56.325651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = scgpt_memvp(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=num_types if CLS else 1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=MVC,\n",
    "    do_dab=DAB,\n",
    "    use_batch_labels=INPUT_BATCH_LABELS,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=config.DSBN,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    mvc_decoder_style=mvc_decoder_style,\n",
    "    ecs_threshold=ecs_threshold,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    "    text_encoder_path=config.text_decoder_path\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "# Freeze all pre-decoder weights\n",
    "for name, para in model.named_parameters():\n",
    "    if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name:\n",
    "    # if config.freeze and \"encoder\" in name:\n",
    "    #     print(f\"freezing weights for: {name}\")\n",
    "        para.requires_grad = False\n",
    "    else:\n",
    "        print(name)\n",
    "    if 'adapter' in name :\n",
    "        # print(name)\n",
    "        para.requires_grad=True\n",
    "\n",
    "\n",
    "post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "logger.info(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "logger.info(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "# wandb.log(\n",
    "#         {\n",
    "#             \"info/pre_freeze_param_count\": pre_freeze_param_count,\n",
    "#             \"info/post_freeze_param_count\": post_freeze_param_count,\n",
    "#         },\n",
    "# )\n",
    "\n",
    "model.to(device)\n",
    "# wandb.watch(model)\n",
    "\n",
    "if ADV:\n",
    "    discriminator = AdversarialDiscriminator(\n",
    "        d_model=embsize,\n",
    "        n_cls=num_batch_types,\n",
    "    ).to(device)\n"
   ],
   "id": "ba76afba428a2b85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "adapter_emb1\n",
      "adapter_emb2\n",
      "encoder.embedding.weight\n",
      "encoder.enc_norm.weight\n",
      "encoder.enc_norm.bias\n",
      "value_encoder.linear1.weight\n",
      "value_encoder.linear1.bias\n",
      "value_encoder.linear2.weight\n",
      "value_encoder.linear2.bias\n",
      "value_encoder.norm.weight\n",
      "value_encoder.norm.bias\n",
      "transformer_encoder.layers.0.self_attn.in_proj_weight\n",
      "transformer_encoder.layers.0.self_attn.in_proj_bias\n",
      "transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "transformer_encoder.layers.0.linear1.weight\n",
      "transformer_encoder.layers.0.linear1.bias\n",
      "transformer_encoder.layers.0.linear2.weight\n",
      "transformer_encoder.layers.0.linear2.bias\n",
      "transformer_encoder.layers.0.norm1.weight\n",
      "transformer_encoder.layers.0.norm1.bias\n",
      "transformer_encoder.layers.0.norm2.weight\n",
      "transformer_encoder.layers.0.norm2.bias\n",
      "transformer_encoder.layers.0.adapter.fc1.weight\n",
      "transformer_encoder.layers.0.adapter.fc2.weight\n",
      "transformer_encoder.layers.1.self_attn.in_proj_weight\n",
      "transformer_encoder.layers.1.self_attn.in_proj_bias\n",
      "transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "transformer_encoder.layers.1.linear1.weight\n",
      "transformer_encoder.layers.1.linear1.bias\n",
      "transformer_encoder.layers.1.linear2.weight\n",
      "transformer_encoder.layers.1.linear2.bias\n",
      "transformer_encoder.layers.1.norm1.weight\n",
      "transformer_encoder.layers.1.norm1.bias\n",
      "transformer_encoder.layers.1.norm2.weight\n",
      "transformer_encoder.layers.1.norm2.bias\n",
      "transformer_encoder.layers.1.adapter.fc1.weight\n",
      "transformer_encoder.layers.1.adapter.fc2.weight\n",
      "transformer_encoder.layers.2.self_attn.in_proj_weight\n",
      "transformer_encoder.layers.2.self_attn.in_proj_bias\n",
      "transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "transformer_encoder.layers.2.linear1.weight\n",
      "transformer_encoder.layers.2.linear1.bias\n",
      "transformer_encoder.layers.2.linear2.weight\n",
      "transformer_encoder.layers.2.linear2.bias\n",
      "transformer_encoder.layers.2.norm1.weight\n",
      "transformer_encoder.layers.2.norm1.bias\n",
      "transformer_encoder.layers.2.norm2.weight\n",
      "transformer_encoder.layers.2.norm2.bias\n",
      "transformer_encoder.layers.2.adapter.fc1.weight\n",
      "transformer_encoder.layers.2.adapter.fc2.weight\n",
      "transformer_encoder.layers.3.self_attn.in_proj_weight\n",
      "transformer_encoder.layers.3.self_attn.in_proj_bias\n",
      "transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "transformer_encoder.layers.3.linear1.weight\n",
      "transformer_encoder.layers.3.linear1.bias\n",
      "transformer_encoder.layers.3.linear2.weight\n",
      "transformer_encoder.layers.3.linear2.bias\n",
      "transformer_encoder.layers.3.norm1.weight\n",
      "transformer_encoder.layers.3.norm1.bias\n",
      "transformer_encoder.layers.3.norm2.weight\n",
      "transformer_encoder.layers.3.norm2.bias\n",
      "transformer_encoder.layers.3.adapter.fc1.weight\n",
      "transformer_encoder.layers.3.adapter.fc2.weight\n",
      "transformer_encoder.layers.4.self_attn.in_proj_weight\n",
      "transformer_encoder.layers.4.self_attn.in_proj_bias\n",
      "transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "transformer_encoder.layers.4.linear1.weight\n",
      "transformer_encoder.layers.4.linear1.bias\n",
      "transformer_encoder.layers.4.linear2.weight\n",
      "transformer_encoder.layers.4.linear2.bias\n",
      "transformer_encoder.layers.4.norm1.weight\n",
      "transformer_encoder.layers.4.norm1.bias\n",
      "transformer_encoder.layers.4.norm2.weight\n",
      "transformer_encoder.layers.4.norm2.bias\n",
      "transformer_encoder.layers.4.adapter.fc1.weight\n",
      "transformer_encoder.layers.4.adapter.fc2.weight\n",
      "transformer_encoder.layers.5.self_attn.in_proj_weight\n",
      "transformer_encoder.layers.5.self_attn.in_proj_bias\n",
      "transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "transformer_encoder.layers.5.linear1.weight\n",
      "transformer_encoder.layers.5.linear1.bias\n",
      "transformer_encoder.layers.5.linear2.weight\n",
      "transformer_encoder.layers.5.linear2.bias\n",
      "transformer_encoder.layers.5.norm1.weight\n",
      "transformer_encoder.layers.5.norm1.bias\n",
      "transformer_encoder.layers.5.norm2.weight\n",
      "transformer_encoder.layers.5.norm2.bias\n",
      "transformer_encoder.layers.5.adapter.fc1.weight\n",
      "transformer_encoder.layers.5.adapter.fc2.weight\n",
      "transformer_encoder.layers.6.self_attn.in_proj_weight\n",
      "transformer_encoder.layers.6.self_attn.in_proj_bias\n",
      "transformer_encoder.layers.6.self_attn.out_proj.weight\n",
      "transformer_encoder.layers.6.self_attn.out_proj.bias\n",
      "transformer_encoder.layers.6.linear1.weight\n",
      "transformer_encoder.layers.6.linear1.bias\n",
      "transformer_encoder.layers.6.linear2.weight\n",
      "transformer_encoder.layers.6.linear2.bias\n",
      "transformer_encoder.layers.6.norm1.weight\n",
      "transformer_encoder.layers.6.norm1.bias\n",
      "transformer_encoder.layers.6.norm2.weight\n",
      "transformer_encoder.layers.6.norm2.bias\n",
      "transformer_encoder.layers.6.adapter.fc1.weight\n",
      "transformer_encoder.layers.6.adapter.fc2.weight\n",
      "transformer_encoder.layers.7.self_attn.in_proj_weight\n",
      "transformer_encoder.layers.7.self_attn.in_proj_bias\n",
      "transformer_encoder.layers.7.self_attn.out_proj.weight\n",
      "transformer_encoder.layers.7.self_attn.out_proj.bias\n",
      "transformer_encoder.layers.7.linear1.weight\n",
      "transformer_encoder.layers.7.linear1.bias\n",
      "transformer_encoder.layers.7.linear2.weight\n",
      "transformer_encoder.layers.7.linear2.bias\n",
      "transformer_encoder.layers.7.norm1.weight\n",
      "transformer_encoder.layers.7.norm1.bias\n",
      "transformer_encoder.layers.7.norm2.weight\n",
      "transformer_encoder.layers.7.norm2.bias\n",
      "transformer_encoder.layers.7.adapter.fc1.weight\n",
      "transformer_encoder.layers.7.adapter.fc2.weight\n",
      "transformer_encoder.layers.8.self_attn.in_proj_weight\n",
      "transformer_encoder.layers.8.self_attn.in_proj_bias\n",
      "transformer_encoder.layers.8.self_attn.out_proj.weight\n",
      "transformer_encoder.layers.8.self_attn.out_proj.bias\n",
      "transformer_encoder.layers.8.linear1.weight\n",
      "transformer_encoder.layers.8.linear1.bias\n",
      "transformer_encoder.layers.8.linear2.weight\n",
      "transformer_encoder.layers.8.linear2.bias\n",
      "transformer_encoder.layers.8.norm1.weight\n",
      "transformer_encoder.layers.8.norm1.bias\n",
      "transformer_encoder.layers.8.norm2.weight\n",
      "transformer_encoder.layers.8.norm2.bias\n",
      "transformer_encoder.layers.8.adapter.fc1.weight\n",
      "transformer_encoder.layers.8.adapter.fc2.weight\n",
      "transformer_encoder.layers.9.self_attn.in_proj_weight\n",
      "transformer_encoder.layers.9.self_attn.in_proj_bias\n",
      "transformer_encoder.layers.9.self_attn.out_proj.weight\n",
      "transformer_encoder.layers.9.self_attn.out_proj.bias\n",
      "transformer_encoder.layers.9.linear1.weight\n",
      "transformer_encoder.layers.9.linear1.bias\n",
      "transformer_encoder.layers.9.linear2.weight\n",
      "transformer_encoder.layers.9.linear2.bias\n",
      "transformer_encoder.layers.9.norm1.weight\n",
      "transformer_encoder.layers.9.norm1.bias\n",
      "transformer_encoder.layers.9.norm2.weight\n",
      "transformer_encoder.layers.9.norm2.bias\n",
      "transformer_encoder.layers.9.adapter.fc1.weight\n",
      "transformer_encoder.layers.9.adapter.fc2.weight\n",
      "transformer_encoder.layers.10.self_attn.in_proj_weight\n",
      "transformer_encoder.layers.10.self_attn.in_proj_bias\n",
      "transformer_encoder.layers.10.self_attn.out_proj.weight\n",
      "transformer_encoder.layers.10.self_attn.out_proj.bias\n",
      "transformer_encoder.layers.10.linear1.weight\n",
      "transformer_encoder.layers.10.linear1.bias\n",
      "transformer_encoder.layers.10.linear2.weight\n",
      "transformer_encoder.layers.10.linear2.bias\n",
      "transformer_encoder.layers.10.norm1.weight\n",
      "transformer_encoder.layers.10.norm1.bias\n",
      "transformer_encoder.layers.10.norm2.weight\n",
      "transformer_encoder.layers.10.norm2.bias\n",
      "transformer_encoder.layers.10.adapter.fc1.weight\n",
      "transformer_encoder.layers.10.adapter.fc2.weight\n",
      "transformer_encoder.layers.11.self_attn.in_proj_weight\n",
      "transformer_encoder.layers.11.self_attn.in_proj_bias\n",
      "transformer_encoder.layers.11.self_attn.out_proj.weight\n",
      "transformer_encoder.layers.11.self_attn.out_proj.bias\n",
      "transformer_encoder.layers.11.linear1.weight\n",
      "transformer_encoder.layers.11.linear1.bias\n",
      "transformer_encoder.layers.11.linear2.weight\n",
      "transformer_encoder.layers.11.linear2.bias\n",
      "transformer_encoder.layers.11.norm1.weight\n",
      "transformer_encoder.layers.11.norm1.bias\n",
      "transformer_encoder.layers.11.norm2.weight\n",
      "transformer_encoder.layers.11.norm2.bias\n",
      "transformer_encoder.layers.11.adapter.fc1.weight\n",
      "transformer_encoder.layers.11.adapter.fc2.weight\n",
      "decoder.fc.0.weight\n",
      "decoder.fc.0.bias\n",
      "decoder.fc.2.weight\n",
      "decoder.fc.2.bias\n",
      "decoder.fc.4.weight\n",
      "decoder.fc.4.bias\n",
      "cls_decoder._decoder.0.weight\n",
      "cls_decoder._decoder.0.bias\n",
      "cls_decoder._decoder.2.weight\n",
      "cls_decoder._decoder.2.bias\n",
      "cls_decoder._decoder.3.weight\n",
      "cls_decoder._decoder.3.bias\n",
      "cls_decoder._decoder.5.weight\n",
      "cls_decoder._decoder.5.bias\n",
      "cls_decoder.out_layer.weight\n",
      "cls_decoder.out_layer.bias\n",
      "text_encoder.embeddings.word_embeddings.weight\n",
      "text_encoder.embeddings.LayerNorm.weight\n",
      "text_encoder.embeddings.LayerNorm.bias\n",
      "text_encoder.encoder.layer.0.attention.self.query_proj.weight\n",
      "text_encoder.encoder.layer.0.attention.self.query_proj.bias\n",
      "text_encoder.encoder.layer.0.attention.self.key_proj.weight\n",
      "text_encoder.encoder.layer.0.attention.self.key_proj.bias\n",
      "text_encoder.encoder.layer.0.attention.self.value_proj.weight\n",
      "text_encoder.encoder.layer.0.attention.self.value_proj.bias\n",
      "text_encoder.encoder.layer.0.attention.output.dense.weight\n",
      "text_encoder.encoder.layer.0.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.0.intermediate.dense.weight\n",
      "text_encoder.encoder.layer.0.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.0.output.dense.weight\n",
      "text_encoder.encoder.layer.0.output.dense.bias\n",
      "text_encoder.encoder.layer.0.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.0.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.1.attention.self.query_proj.weight\n",
      "text_encoder.encoder.layer.1.attention.self.query_proj.bias\n",
      "text_encoder.encoder.layer.1.attention.self.key_proj.weight\n",
      "text_encoder.encoder.layer.1.attention.self.key_proj.bias\n",
      "text_encoder.encoder.layer.1.attention.self.value_proj.weight\n",
      "text_encoder.encoder.layer.1.attention.self.value_proj.bias\n",
      "text_encoder.encoder.layer.1.attention.output.dense.weight\n",
      "text_encoder.encoder.layer.1.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.1.intermediate.dense.weight\n",
      "text_encoder.encoder.layer.1.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.1.output.dense.weight\n",
      "text_encoder.encoder.layer.1.output.dense.bias\n",
      "text_encoder.encoder.layer.1.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.1.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.2.attention.self.query_proj.weight\n",
      "text_encoder.encoder.layer.2.attention.self.query_proj.bias\n",
      "text_encoder.encoder.layer.2.attention.self.key_proj.weight\n",
      "text_encoder.encoder.layer.2.attention.self.key_proj.bias\n",
      "text_encoder.encoder.layer.2.attention.self.value_proj.weight\n",
      "text_encoder.encoder.layer.2.attention.self.value_proj.bias\n",
      "text_encoder.encoder.layer.2.attention.output.dense.weight\n",
      "text_encoder.encoder.layer.2.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.2.intermediate.dense.weight\n",
      "text_encoder.encoder.layer.2.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.2.output.dense.weight\n",
      "text_encoder.encoder.layer.2.output.dense.bias\n",
      "text_encoder.encoder.layer.2.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.2.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.3.attention.self.query_proj.weight\n",
      "text_encoder.encoder.layer.3.attention.self.query_proj.bias\n",
      "text_encoder.encoder.layer.3.attention.self.key_proj.weight\n",
      "text_encoder.encoder.layer.3.attention.self.key_proj.bias\n",
      "text_encoder.encoder.layer.3.attention.self.value_proj.weight\n",
      "text_encoder.encoder.layer.3.attention.self.value_proj.bias\n",
      "text_encoder.encoder.layer.3.attention.output.dense.weight\n",
      "text_encoder.encoder.layer.3.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.3.intermediate.dense.weight\n",
      "text_encoder.encoder.layer.3.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.3.output.dense.weight\n",
      "text_encoder.encoder.layer.3.output.dense.bias\n",
      "text_encoder.encoder.layer.3.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.3.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.4.attention.self.query_proj.weight\n",
      "text_encoder.encoder.layer.4.attention.self.query_proj.bias\n",
      "text_encoder.encoder.layer.4.attention.self.key_proj.weight\n",
      "text_encoder.encoder.layer.4.attention.self.key_proj.bias\n",
      "text_encoder.encoder.layer.4.attention.self.value_proj.weight\n",
      "text_encoder.encoder.layer.4.attention.self.value_proj.bias\n",
      "text_encoder.encoder.layer.4.attention.output.dense.weight\n",
      "text_encoder.encoder.layer.4.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.4.intermediate.dense.weight\n",
      "text_encoder.encoder.layer.4.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.4.output.dense.weight\n",
      "text_encoder.encoder.layer.4.output.dense.bias\n",
      "text_encoder.encoder.layer.4.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.4.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.5.attention.self.query_proj.weight\n",
      "text_encoder.encoder.layer.5.attention.self.query_proj.bias\n",
      "text_encoder.encoder.layer.5.attention.self.key_proj.weight\n",
      "text_encoder.encoder.layer.5.attention.self.key_proj.bias\n",
      "text_encoder.encoder.layer.5.attention.self.value_proj.weight\n",
      "text_encoder.encoder.layer.5.attention.self.value_proj.bias\n",
      "text_encoder.encoder.layer.5.attention.output.dense.weight\n",
      "text_encoder.encoder.layer.5.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.5.intermediate.dense.weight\n",
      "text_encoder.encoder.layer.5.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.5.output.dense.weight\n",
      "text_encoder.encoder.layer.5.output.dense.bias\n",
      "text_encoder.encoder.layer.5.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.5.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.6.attention.self.query_proj.weight\n",
      "text_encoder.encoder.layer.6.attention.self.query_proj.bias\n",
      "text_encoder.encoder.layer.6.attention.self.key_proj.weight\n",
      "text_encoder.encoder.layer.6.attention.self.key_proj.bias\n",
      "text_encoder.encoder.layer.6.attention.self.value_proj.weight\n",
      "text_encoder.encoder.layer.6.attention.self.value_proj.bias\n",
      "text_encoder.encoder.layer.6.attention.output.dense.weight\n",
      "text_encoder.encoder.layer.6.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.6.intermediate.dense.weight\n",
      "text_encoder.encoder.layer.6.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.6.output.dense.weight\n",
      "text_encoder.encoder.layer.6.output.dense.bias\n",
      "text_encoder.encoder.layer.6.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.6.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.7.attention.self.query_proj.weight\n",
      "text_encoder.encoder.layer.7.attention.self.query_proj.bias\n",
      "text_encoder.encoder.layer.7.attention.self.key_proj.weight\n",
      "text_encoder.encoder.layer.7.attention.self.key_proj.bias\n",
      "text_encoder.encoder.layer.7.attention.self.value_proj.weight\n",
      "text_encoder.encoder.layer.7.attention.self.value_proj.bias\n",
      "text_encoder.encoder.layer.7.attention.output.dense.weight\n",
      "text_encoder.encoder.layer.7.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.7.intermediate.dense.weight\n",
      "text_encoder.encoder.layer.7.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.7.output.dense.weight\n",
      "text_encoder.encoder.layer.7.output.dense.bias\n",
      "text_encoder.encoder.layer.7.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.7.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.8.attention.self.query_proj.weight\n",
      "text_encoder.encoder.layer.8.attention.self.query_proj.bias\n",
      "text_encoder.encoder.layer.8.attention.self.key_proj.weight\n",
      "text_encoder.encoder.layer.8.attention.self.key_proj.bias\n",
      "text_encoder.encoder.layer.8.attention.self.value_proj.weight\n",
      "text_encoder.encoder.layer.8.attention.self.value_proj.bias\n",
      "text_encoder.encoder.layer.8.attention.output.dense.weight\n",
      "text_encoder.encoder.layer.8.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.8.intermediate.dense.weight\n",
      "text_encoder.encoder.layer.8.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.8.output.dense.weight\n",
      "text_encoder.encoder.layer.8.output.dense.bias\n",
      "text_encoder.encoder.layer.8.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.8.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.9.attention.self.query_proj.weight\n",
      "text_encoder.encoder.layer.9.attention.self.query_proj.bias\n",
      "text_encoder.encoder.layer.9.attention.self.key_proj.weight\n",
      "text_encoder.encoder.layer.9.attention.self.key_proj.bias\n",
      "text_encoder.encoder.layer.9.attention.self.value_proj.weight\n",
      "text_encoder.encoder.layer.9.attention.self.value_proj.bias\n",
      "text_encoder.encoder.layer.9.attention.output.dense.weight\n",
      "text_encoder.encoder.layer.9.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.9.intermediate.dense.weight\n",
      "text_encoder.encoder.layer.9.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.9.output.dense.weight\n",
      "text_encoder.encoder.layer.9.output.dense.bias\n",
      "text_encoder.encoder.layer.9.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.9.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.10.attention.self.query_proj.weight\n",
      "text_encoder.encoder.layer.10.attention.self.query_proj.bias\n",
      "text_encoder.encoder.layer.10.attention.self.key_proj.weight\n",
      "text_encoder.encoder.layer.10.attention.self.key_proj.bias\n",
      "text_encoder.encoder.layer.10.attention.self.value_proj.weight\n",
      "text_encoder.encoder.layer.10.attention.self.value_proj.bias\n",
      "text_encoder.encoder.layer.10.attention.output.dense.weight\n",
      "text_encoder.encoder.layer.10.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.10.intermediate.dense.weight\n",
      "text_encoder.encoder.layer.10.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.10.output.dense.weight\n",
      "text_encoder.encoder.layer.10.output.dense.bias\n",
      "text_encoder.encoder.layer.10.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.10.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.11.attention.self.query_proj.weight\n",
      "text_encoder.encoder.layer.11.attention.self.query_proj.bias\n",
      "text_encoder.encoder.layer.11.attention.self.key_proj.weight\n",
      "text_encoder.encoder.layer.11.attention.self.key_proj.bias\n",
      "text_encoder.encoder.layer.11.attention.self.value_proj.weight\n",
      "text_encoder.encoder.layer.11.attention.self.value_proj.bias\n",
      "text_encoder.encoder.layer.11.attention.output.dense.weight\n",
      "text_encoder.encoder.layer.11.attention.output.dense.bias\n",
      "text_encoder.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "text_encoder.encoder.layer.11.intermediate.dense.weight\n",
      "text_encoder.encoder.layer.11.intermediate.dense.bias\n",
      "text_encoder.encoder.layer.11.output.dense.weight\n",
      "text_encoder.encoder.layer.11.output.dense.bias\n",
      "text_encoder.encoder.layer.11.output.LayerNorm.weight\n",
      "text_encoder.encoder.layer.11.output.LayerNorm.bias\n",
      "text_encoder.encoder.rel_embeddings.weight\n",
      "text_encoder.encoder.LayerNorm.weight\n",
      "text_encoder.encoder.LayerNorm.bias\n",
      "adapter_proj.fc1.weight\n",
      "adapter_proj.fc1.bias\n",
      "adapter_proj.fc2.weight\n",
      "adapter_proj.fc2.bias\n",
      "scGPT - INFO - Total Pre freeze Params 51879598\n",
      "scGPT - INFO - Total Post freeze Params 51879598\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.477921Z",
     "start_time": "2025-01-02T03:56:59.214010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, schedule_interval, gamma=config.schedule_ratio\n",
    ")\n",
    "if DAB_separate_optim:\n",
    "    optimizer_dab = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler_dab = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_dab, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "if ADV:\n",
    "    criterion_adv = nn.CrossEntropyLoss()  # consider using label smoothing\n",
    "    optimizer_E = torch.optim.Adam(model.parameters(), lr=lr_ADV)\n",
    "    scheduler_E = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_E, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_ADV)\n",
    "    scheduler_D = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_D, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ],
   "id": "c8c45aa718d06241",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.478123Z",
     "start_time": "2025-01-02T03:56:59.509164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    (\n",
    "        total_loss,\n",
    "        total_mse,\n",
    "        total_cls,\n",
    "        total_cce,\n",
    "        total_mvc,\n",
    "        total_ecs,\n",
    "        total_dab,\n",
    "        total_adv_E,\n",
    "        total_adv_D,\n",
    "        total_zero_log_prob,\n",
    "        total_mvc_zero_log_prob,\n",
    "    ) = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "    total_error = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "        celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "        text=batch_data[\"text\"].squeeze(1).to(device)\n",
    "        mask = batch_data[\"mask\"].squeeze(1).to(device)\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                text=text,\n",
    "                mask=mask,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = 0.0\n",
    "            metrics_to_log = {}\n",
    "            if MLM:\n",
    "                loss_mse = criterion(\n",
    "                    output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mse\n",
    "                metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            if explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            if CLS:\n",
    "                loss_cls = criterion_cls(output_dict[\"cls_output\"], celltype_labels)\n",
    "                loss = loss + loss_cls\n",
    "                metrics_to_log.update({\"train/cls\": loss_cls.item()})\n",
    "\n",
    "                error_rate = 1 - (\n",
    "                    (output_dict[\"cls_output\"].argmax(1) == celltype_labels)\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                ) / celltype_labels.size(0)\n",
    "            if CCE:\n",
    "                loss_cce = 10 * output_dict[\"loss_cce\"]\n",
    "                loss = loss + loss_cce\n",
    "                metrics_to_log.update({\"train/cce\": loss_cce.item()})\n",
    "            if MVC:\n",
    "                loss_mvc = criterion(\n",
    "                    output_dict[\"mvc_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mvc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_mvc.item()})\n",
    "            if MVC and explicit_zero_prob:\n",
    "                loss_mvc_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mvc_zero_log_prob\n",
    "                metrics_to_log.update({\"train/mvc_nzlp\": loss_mvc_zero_log_prob.item()})\n",
    "            if ECS:\n",
    "                loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "                loss = loss + loss_ecs\n",
    "                metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "            if DAB:\n",
    "                # try weighting and separate optimizer\n",
    "                loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "                loss = loss + dab_weight * loss_dab\n",
    "                metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if ADV:\n",
    "            # rerun the model for adversarial training\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                text=text,\n",
    "                mask=mask,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            # TRAINING DISCRIMINATOR\n",
    "            loss_adv_D = criterion_adv(\n",
    "                discriminator(output_dict[\"cell_emb\"].detach()), batch_labels\n",
    "            )\n",
    "            if epoch > adv_D_delay_epochs:\n",
    "                discriminator.zero_grad()\n",
    "                loss_adv_D.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "            # TRAINING ENCODER\n",
    "            loss_adv_E = -criterion_adv(\n",
    "                discriminator(output_dict[\"cell_emb\"]), batch_labels\n",
    "            )\n",
    "            # NOTE: the loss is negative here because we want to maximize\n",
    "            # the cross_entropy_loss, in other words, disguise against the discriminator\n",
    "            if epoch > adv_E_delay_epochs:\n",
    "                model.zero_grad()\n",
    "                discriminator.zero_grad()\n",
    "                loss_adv_E.backward()\n",
    "                optimizer_E.step()\n",
    "\n",
    "        # wandb.log(metrics_to_log)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item() if MLM else 0.0\n",
    "        total_cls += loss_cls.item() if CLS else 0.0\n",
    "        total_cce += loss_cce.item() if CCE else 0.0\n",
    "        total_mvc += loss_mvc.item() if MVC else 0.0\n",
    "        total_ecs += loss_ecs.item() if ECS else 0.0\n",
    "        total_dab += loss_dab.item() if DAB else 0.0\n",
    "        total_adv_E += loss_adv_E.item() if ADV else 0.0\n",
    "        total_adv_D += loss_adv_D.item() if ADV else 0.0\n",
    "        total_zero_log_prob += loss_zero_log_prob.item() if explicit_zero_prob else 0.0\n",
    "        total_mvc_zero_log_prob += (\n",
    "            loss_mvc_zero_log_prob.item() if MVC and explicit_zero_prob else 0.0\n",
    "        )\n",
    "        total_error += error_rate\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_cls = total_cls / log_interval if CLS else 0.0\n",
    "            cur_cce = total_cce / log_interval if CCE else 0.0\n",
    "            cur_mvc = total_mvc / log_interval if MVC else 0.0\n",
    "            cur_ecs = total_ecs / log_interval if ECS else 0.0\n",
    "            cur_dab = total_dab / log_interval if DAB else 0.0\n",
    "            cur_adv_E = total_adv_E / log_interval if ADV else 0.0\n",
    "            cur_adv_D = total_adv_D / log_interval if ADV else 0.0\n",
    "            cur_zero_log_prob = (\n",
    "                total_zero_log_prob / log_interval if explicit_zero_prob else 0.0\n",
    "            )\n",
    "            cur_mvc_zero_log_prob = (\n",
    "                total_mvc_zero_log_prob / log_interval\n",
    "                if MVC and explicit_zero_prob\n",
    "                else 0.0\n",
    "            )\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | \"\n",
    "                + (f\"mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\" if MLM else \"\")\n",
    "                + (f\"cls {cur_cls:5.2f} | \" if CLS else \"\")\n",
    "                + (f\"err {cur_error:5.2f} | \" if CLS else \"\")\n",
    "                + (f\"cce {cur_cce:5.2f} |\" if CCE else \"\")\n",
    "                + (f\"mvc {cur_mvc:5.2f} |\" if MVC else \"\")\n",
    "                + (f\"ecs {cur_ecs:5.2f} |\" if ECS else \"\")\n",
    "                + (f\"dab {cur_dab:5.2f} |\" if DAB else \"\")\n",
    "                + (f\"adv_E {cur_adv_E:5.2f} |\" if ADV else \"\")\n",
    "                + (f\"adv_D {cur_adv_D:5.2f} |\" if ADV else \"\")\n",
    "                + (f\"nzlp {cur_zero_log_prob:5.2f} |\" if explicit_zero_prob else \"\")\n",
    "                + (\n",
    "                    f\"mvc_nzlp {cur_mvc_zero_log_prob:5.2f} |\"\n",
    "                    if MVC and explicit_zero_prob\n",
    "                    else \"\"\n",
    "                )\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_cls = 0\n",
    "            total_cce = 0\n",
    "            total_mvc = 0\n",
    "            total_ecs = 0\n",
    "            total_dab = 0\n",
    "            total_adv_E = 0\n",
    "            total_adv_D = 0\n",
    "            total_zero_log_prob = 0\n",
    "            total_mvc_zero_log_prob = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def define_wandb_metrcis():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"test/avg_bio\", summary=\"max\")\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "            celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "            text=batch_data[\"text\"].squeeze(1).to(device)\n",
    "            mask=batch_data[\"mask\"].squeeze(1).to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                    text=text,\n",
    "                    mask=mask,\n",
    "                    CLS=CLS,  # evaluation does not need CLS or CCE\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=do_sample_in_train,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "                output_values = output_dict[\"cls_output\"]\n",
    "                loss = criterion_cls(output_values, celltype_labels)\n",
    "\n",
    "                if DAB:\n",
    "                    loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            accuracy = (output_values.argmax(1) == celltype_labels).sum().item()\n",
    "            total_error += (1 - accuracy / len(input_gene_ids)) * len(input_gene_ids)\n",
    "            total_dab += loss_dab.item() * len(input_gene_ids) if DAB else 0.0\n",
    "            total_num += len(input_gene_ids)\n",
    "            preds = output_values.argmax(1).cpu().numpy()\n",
    "            predictions.append(preds)\n",
    "\n",
    "    # wandb.log(\n",
    "    #     {\n",
    "    #         \"valid/mse\": total_loss / total_num,\n",
    "    #         \"valid/err\": total_error / total_num,\n",
    "    #         \"valid/dab\": total_dab / total_num,\n",
    "    #         \"valid/sum_mse_dab\": (total_loss + dab_weight * total_dab) / total_num,\n",
    "    #         \"epoch\": epoch,\n",
    "    #     },\n",
    "    # )\n",
    "    print(\"valid/mse\",total_loss / total_num,\n",
    "            \"valid/err\", total_error / total_num,\n",
    "            \"valid/dab\", total_dab / total_num,\n",
    "            \"valid/sum_mse_dab\", (total_loss + dab_weight * total_dab) / total_num,\n",
    "            \"epoch\", epoch)\n",
    "\n",
    "    if return_raw:\n",
    "        return np.concatenate(predictions, axis=0)\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num"
   ],
   "id": "2e21092d13c4a8e9",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.478270Z",
     "start_time": "2025-01-02T03:56:59.622486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "# define_wandb_metrcis()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model,\n",
    "            loader=train_loader,\n",
    "        )\n",
    "    val_loss, val_err = evaluate(\n",
    "        model,\n",
    "        loader=valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | err {val_err:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    if DAB_separate_optim:\n",
    "        scheduler_dab.step()\n",
    "    if ADV:\n",
    "        scheduler_D.step()\n",
    "        scheduler_E.step()"
   ],
   "id": "d89e6508675fc40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   1, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   1 | 100/2309 batches | lr 0.0001 | ms/batch 101.48 | loss  3.39 | cls  3.39 | err  0.93 | \n",
      "scGPT - INFO - | epoch   1 | 200/2309 batches | lr 0.0001 | ms/batch 90.68 | loss  2.68 | cls  2.68 | err  0.76 | \n",
      "scGPT - INFO - | epoch   1 | 300/2309 batches | lr 0.0001 | ms/batch 90.88 | loss  2.09 | cls  2.09 | err  0.62 | \n",
      "scGPT - INFO - | epoch   1 | 400/2309 batches | lr 0.0001 | ms/batch 90.85 | loss  1.84 | cls  1.84 | err  0.54 | \n",
      "scGPT - INFO - | epoch   1 | 500/2309 batches | lr 0.0001 | ms/batch 90.73 | loss  1.69 | cls  1.69 | err  0.51 | \n",
      "scGPT - INFO - | epoch   1 | 600/2309 batches | lr 0.0001 | ms/batch 90.69 | loss  1.61 | cls  1.61 | err  0.49 | \n",
      "scGPT - INFO - | epoch   1 | 700/2309 batches | lr 0.0001 | ms/batch 90.62 | loss  1.60 | cls  1.60 | err  0.47 | \n",
      "scGPT - INFO - | epoch   1 | 800/2309 batches | lr 0.0001 | ms/batch 91.00 | loss  1.45 | cls  1.45 | err  0.47 | \n",
      "scGPT - INFO - | epoch   1 | 900/2309 batches | lr 0.0001 | ms/batch 90.60 | loss  1.43 | cls  1.43 | err  0.44 | \n",
      "scGPT - INFO - | epoch   1 | 1000/2309 batches | lr 0.0001 | ms/batch 90.59 | loss  1.35 | cls  1.35 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 1100/2309 batches | lr 0.0001 | ms/batch 90.77 | loss  1.27 | cls  1.27 | err  0.36 | \n",
      "scGPT - INFO - | epoch   1 | 1200/2309 batches | lr 0.0001 | ms/batch 90.54 | loss  1.42 | cls  1.42 | err  0.43 | \n",
      "scGPT - INFO - | epoch   1 | 1300/2309 batches | lr 0.0001 | ms/batch 90.08 | loss  1.35 | cls  1.35 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 1400/2309 batches | lr 0.0001 | ms/batch 89.53 | loss  1.19 | cls  1.19 | err  0.36 | \n",
      "scGPT - INFO - | epoch   1 | 1500/2309 batches | lr 0.0001 | ms/batch 79.59 | loss  1.09 | cls  1.09 | err  0.34 | \n",
      "scGPT - INFO - | epoch   1 | 1600/2309 batches | lr 0.0001 | ms/batch 80.41 | loss  1.19 | cls  1.19 | err  0.34 | \n",
      "scGPT - INFO - | epoch   1 | 1700/2309 batches | lr 0.0001 | ms/batch 88.25 | loss  1.13 | cls  1.13 | err  0.35 | \n",
      "scGPT - INFO - | epoch   1 | 1800/2309 batches | lr 0.0001 | ms/batch 90.80 | loss  1.17 | cls  1.17 | err  0.36 | \n",
      "scGPT - INFO - | epoch   1 | 1900/2309 batches | lr 0.0001 | ms/batch 91.08 | loss  1.16 | cls  1.16 | err  0.35 | \n",
      "scGPT - INFO - | epoch   1 | 2000/2309 batches | lr 0.0001 | ms/batch 90.67 | loss  1.13 | cls  1.13 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 2100/2309 batches | lr 0.0001 | ms/batch 90.98 | loss  1.08 | cls  1.08 | err  0.32 | \n",
      "scGPT - INFO - | epoch   1 | 2200/2309 batches | lr 0.0001 | ms/batch 91.06 | loss  1.04 | cls  1.04 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 2300/2309 batches | lr 0.0001 | ms/batch 90.94 | loss  1.07 | cls  1.07 | err  0.32 | \n",
      "valid/mse 1.0642690115770135 valid/err 0.33297238765565784 valid/dab 0.0 valid/sum_mse_dab 1.0642690115770135 epoch 1\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 217.08s | valid loss/mse 1.0643 | err 0.3330\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 1.0643\n",
      "random masking at epoch   2, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   2 | 100/2309 batches | lr 0.0001 | ms/batch 90.00 | loss  1.11 | cls  1.11 | err  0.34 | \n",
      "scGPT - INFO - | epoch   2 | 200/2309 batches | lr 0.0001 | ms/batch 90.48 | loss  1.05 | cls  1.05 | err  0.31 | \n",
      "scGPT - INFO - | epoch   2 | 300/2309 batches | lr 0.0001 | ms/batch 87.19 | loss  0.96 | cls  0.96 | err  0.29 | \n",
      "scGPT - INFO - | epoch   2 | 400/2309 batches | lr 0.0001 | ms/batch 88.69 | loss  1.00 | cls  1.00 | err  0.28 | \n",
      "scGPT - INFO - | epoch   2 | 500/2309 batches | lr 0.0001 | ms/batch 81.29 | loss  0.93 | cls  0.93 | err  0.27 | \n",
      "scGPT - INFO - | epoch   2 | 600/2309 batches | lr 0.0001 | ms/batch 87.45 | loss  1.03 | cls  1.03 | err  0.31 | \n",
      "scGPT - INFO - | epoch   2 | 700/2309 batches | lr 0.0001 | ms/batch 91.30 | loss  0.99 | cls  0.99 | err  0.30 | \n",
      "scGPT - INFO - | epoch   2 | 800/2309 batches | lr 0.0001 | ms/batch 91.00 | loss  0.95 | cls  0.95 | err  0.28 | \n",
      "scGPT - INFO - | epoch   2 | 900/2309 batches | lr 0.0001 | ms/batch 90.85 | loss  0.98 | cls  0.98 | err  0.29 | \n",
      "scGPT - INFO - | epoch   2 | 1000/2309 batches | lr 0.0001 | ms/batch 90.83 | loss  0.97 | cls  0.97 | err  0.27 | \n",
      "scGPT - INFO - | epoch   2 | 1100/2309 batches | lr 0.0001 | ms/batch 91.42 | loss  0.86 | cls  0.86 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 1200/2309 batches | lr 0.0001 | ms/batch 91.48 | loss  1.04 | cls  1.04 | err  0.31 | \n",
      "scGPT - INFO - | epoch   2 | 1300/2309 batches | lr 0.0001 | ms/batch 91.07 | loss  0.97 | cls  0.97 | err  0.30 | \n",
      "scGPT - INFO - | epoch   2 | 1400/2309 batches | lr 0.0001 | ms/batch 91.60 | loss  0.94 | cls  0.94 | err  0.29 | \n",
      "scGPT - INFO - | epoch   2 | 1500/2309 batches | lr 0.0001 | ms/batch 91.56 | loss  0.82 | cls  0.82 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 1600/2309 batches | lr 0.0001 | ms/batch 91.57 | loss  0.89 | cls  0.89 | err  0.27 | \n",
      "scGPT - INFO - | epoch   2 | 1700/2309 batches | lr 0.0001 | ms/batch 91.19 | loss  0.85 | cls  0.85 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 1800/2309 batches | lr 0.0001 | ms/batch 91.02 | loss  0.86 | cls  0.86 | err  0.28 | \n",
      "scGPT - INFO - | epoch   2 | 1900/2309 batches | lr 0.0001 | ms/batch 90.88 | loss  0.77 | cls  0.77 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 2000/2309 batches | lr 0.0001 | ms/batch 92.42 | loss  0.84 | cls  0.84 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 2100/2309 batches | lr 0.0001 | ms/batch 90.92 | loss  0.86 | cls  0.86 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 2200/2309 batches | lr 0.0001 | ms/batch 91.69 | loss  0.80 | cls  0.80 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 2300/2309 batches | lr 0.0001 | ms/batch 90.81 | loss  0.79 | cls  0.79 | err  0.22 | \n",
      "valid/mse 0.9418181836379175 valid/err 0.2902003248511099 valid/dab 0.0 valid/sum_mse_dab 0.9418181836379175 epoch 2\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 217.42s | valid loss/mse 0.9418 | err 0.2902\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.9418\n",
      "random masking at epoch   3, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   3 | 100/2309 batches | lr 0.0001 | ms/batch 93.16 | loss  0.88 | cls  0.88 | err  0.26 | \n",
      "scGPT - INFO - | epoch   3 | 200/2309 batches | lr 0.0001 | ms/batch 85.41 | loss  0.80 | cls  0.80 | err  0.24 | \n",
      "scGPT - INFO - | epoch   3 | 300/2309 batches | lr 0.0001 | ms/batch 90.96 | loss  0.79 | cls  0.79 | err  0.24 | \n",
      "scGPT - INFO - | epoch   3 | 400/2309 batches | lr 0.0001 | ms/batch 79.73 | loss  0.81 | cls  0.81 | err  0.23 | \n",
      "scGPT - INFO - | epoch   3 | 500/2309 batches | lr 0.0001 | ms/batch 80.49 | loss  0.72 | cls  0.72 | err  0.22 | \n",
      "scGPT - INFO - | epoch   3 | 600/2309 batches | lr 0.0001 | ms/batch 80.36 | loss  0.80 | cls  0.80 | err  0.25 | \n",
      "scGPT - INFO - | epoch   3 | 700/2309 batches | lr 0.0001 | ms/batch 80.61 | loss  0.80 | cls  0.80 | err  0.24 | \n",
      "scGPT - INFO - | epoch   3 | 800/2309 batches | lr 0.0001 | ms/batch 80.44 | loss  0.80 | cls  0.80 | err  0.24 | \n",
      "scGPT - INFO - | epoch   3 | 900/2309 batches | lr 0.0001 | ms/batch 80.39 | loss  0.81 | cls  0.81 | err  0.25 | \n",
      "scGPT - INFO - | epoch   3 | 1000/2309 batches | lr 0.0001 | ms/batch 80.83 | loss  0.79 | cls  0.79 | err  0.23 | \n",
      "scGPT - INFO - | epoch   3 | 1100/2309 batches | lr 0.0001 | ms/batch 82.15 | loss  0.71 | cls  0.71 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 1200/2309 batches | lr 0.0001 | ms/batch 81.91 | loss  0.82 | cls  0.82 | err  0.23 | \n",
      "scGPT - INFO - | epoch   3 | 1300/2309 batches | lr 0.0001 | ms/batch 83.66 | loss  0.83 | cls  0.83 | err  0.26 | \n",
      "scGPT - INFO - | epoch   3 | 1400/2309 batches | lr 0.0001 | ms/batch 90.53 | loss  0.79 | cls  0.79 | err  0.25 | \n",
      "scGPT - INFO - | epoch   3 | 1500/2309 batches | lr 0.0001 | ms/batch 90.99 | loss  0.69 | cls  0.69 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 1600/2309 batches | lr 0.0001 | ms/batch 90.75 | loss  0.70 | cls  0.70 | err  0.22 | \n",
      "scGPT - INFO - | epoch   3 | 1700/2309 batches | lr 0.0001 | ms/batch 90.45 | loss  0.68 | cls  0.68 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 1800/2309 batches | lr 0.0001 | ms/batch 90.06 | loss  0.75 | cls  0.75 | err  0.23 | \n",
      "scGPT - INFO - | epoch   3 | 1900/2309 batches | lr 0.0001 | ms/batch 93.50 | loss  0.67 | cls  0.67 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 2000/2309 batches | lr 0.0001 | ms/batch 91.05 | loss  0.70 | cls  0.70 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 2100/2309 batches | lr 0.0001 | ms/batch 91.27 | loss  0.75 | cls  0.75 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 2200/2309 batches | lr 0.0001 | ms/batch 90.36 | loss  0.65 | cls  0.65 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 2300/2309 batches | lr 0.0001 | ms/batch 89.61 | loss  0.68 | cls  0.68 | err  0.19 | \n",
      "valid/mse 0.6908621795447246 valid/err 0.2073632918245804 valid/dab 0.0 valid/sum_mse_dab 0.6908621795447246 epoch 3\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 208.54s | valid loss/mse 0.6909 | err 0.2074\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.6909\n",
      "random masking at epoch   4, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   4 | 100/2309 batches | lr 0.0001 | ms/batch 94.10 | loss  0.74 | cls  0.74 | err  0.23 | \n",
      "scGPT - INFO - | epoch   4 | 200/2309 batches | lr 0.0001 | ms/batch 91.79 | loss  0.68 | cls  0.68 | err  0.20 | \n",
      "scGPT - INFO - | epoch   4 | 300/2309 batches | lr 0.0001 | ms/batch 89.89 | loss  0.67 | cls  0.67 | err  0.21 | \n",
      "scGPT - INFO - | epoch   4 | 400/2309 batches | lr 0.0001 | ms/batch 87.96 | loss  0.72 | cls  0.72 | err  0.21 | \n",
      "scGPT - INFO - | epoch   4 | 500/2309 batches | lr 0.0001 | ms/batch 89.96 | loss  0.60 | cls  0.60 | err  0.18 | \n",
      "scGPT - INFO - | epoch   4 | 600/2309 batches | lr 0.0001 | ms/batch 90.16 | loss  0.73 | cls  0.73 | err  0.20 | \n",
      "scGPT - INFO - | epoch   4 | 700/2309 batches | lr 0.0001 | ms/batch 89.56 | loss  0.67 | cls  0.67 | err  0.19 | \n",
      "scGPT - INFO - | epoch   4 | 800/2309 batches | lr 0.0001 | ms/batch 90.20 | loss  0.65 | cls  0.65 | err  0.18 | \n",
      "scGPT - INFO - | epoch   4 | 900/2309 batches | lr 0.0001 | ms/batch 91.43 | loss  0.70 | cls  0.70 | err  0.21 | \n",
      "scGPT - INFO - | epoch   4 | 1000/2309 batches | lr 0.0001 | ms/batch 91.44 | loss  0.68 | cls  0.68 | err  0.19 | \n",
      "scGPT - INFO - | epoch   4 | 1100/2309 batches | lr 0.0001 | ms/batch 90.81 | loss  0.62 | cls  0.62 | err  0.17 | \n",
      "scGPT - INFO - | epoch   4 | 1200/2309 batches | lr 0.0001 | ms/batch 92.20 | loss  0.74 | cls  0.74 | err  0.22 | \n",
      "scGPT - INFO - | epoch   4 | 1300/2309 batches | lr 0.0001 | ms/batch 93.29 | loss  0.77 | cls  0.77 | err  0.24 | \n",
      "scGPT - INFO - | epoch   4 | 1400/2309 batches | lr 0.0001 | ms/batch 90.67 | loss  0.70 | cls  0.70 | err  0.22 | \n",
      "scGPT - INFO - | epoch   4 | 1500/2309 batches | lr 0.0001 | ms/batch 90.85 | loss  0.63 | cls  0.63 | err  0.19 | \n",
      "scGPT - INFO - | epoch   4 | 1600/2309 batches | lr 0.0001 | ms/batch 90.55 | loss  0.59 | cls  0.59 | err  0.17 | \n",
      "scGPT - INFO - | epoch   4 | 1700/2309 batches | lr 0.0001 | ms/batch 91.25 | loss  0.58 | cls  0.58 | err  0.18 | \n",
      "scGPT - INFO - | epoch   4 | 1800/2309 batches | lr 0.0001 | ms/batch 91.30 | loss  0.61 | cls  0.61 | err  0.18 | \n",
      "scGPT - INFO - | epoch   4 | 1900/2309 batches | lr 0.0001 | ms/batch 91.02 | loss  0.61 | cls  0.61 | err  0.17 | \n",
      "scGPT - INFO - | epoch   4 | 2000/2309 batches | lr 0.0001 | ms/batch 90.73 | loss  0.62 | cls  0.62 | err  0.18 | \n",
      "scGPT - INFO - | epoch   4 | 2100/2309 batches | lr 0.0001 | ms/batch 90.80 | loss  0.68 | cls  0.68 | err  0.19 | \n",
      "scGPT - INFO - | epoch   4 | 2200/2309 batches | lr 0.0001 | ms/batch 91.48 | loss  0.58 | cls  0.58 | err  0.17 | \n",
      "scGPT - INFO - | epoch   4 | 2300/2309 batches | lr 0.0001 | ms/batch 94.39 | loss  0.61 | cls  0.61 | err  0.17 | \n",
      "valid/mse 0.5954406276127423 valid/err 0.1797509474824039 valid/dab 0.0 valid/sum_mse_dab 0.5954406276127423 epoch 4\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 219.64s | valid loss/mse 0.5954 | err 0.1798\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.5954\n",
      "random masking at epoch   5, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   5 | 100/2309 batches | lr 0.0001 | ms/batch 94.08 | loss  0.65 | cls  0.65 | err  0.20 | \n",
      "scGPT - INFO - | epoch   5 | 200/2309 batches | lr 0.0001 | ms/batch 91.46 | loss  0.62 | cls  0.62 | err  0.18 | \n",
      "scGPT - INFO - | epoch   5 | 300/2309 batches | lr 0.0001 | ms/batch 91.32 | loss  0.60 | cls  0.60 | err  0.19 | \n",
      "scGPT - INFO - | epoch   5 | 400/2309 batches | lr 0.0001 | ms/batch 91.64 | loss  0.65 | cls  0.65 | err  0.19 | \n",
      "scGPT - INFO - | epoch   5 | 500/2309 batches | lr 0.0001 | ms/batch 91.26 | loss  0.51 | cls  0.51 | err  0.14 | \n",
      "scGPT - INFO - | epoch   5 | 600/2309 batches | lr 0.0001 | ms/batch 91.12 | loss  0.62 | cls  0.62 | err  0.17 | \n",
      "scGPT - INFO - | epoch   5 | 700/2309 batches | lr 0.0001 | ms/batch 91.36 | loss  0.65 | cls  0.65 | err  0.18 | \n",
      "scGPT - INFO - | epoch   5 | 800/2309 batches | lr 0.0001 | ms/batch 91.06 | loss  0.59 | cls  0.59 | err  0.17 | \n",
      "scGPT - INFO - | epoch   5 | 900/2309 batches | lr 0.0001 | ms/batch 91.34 | loss  0.67 | cls  0.67 | err  0.19 | \n",
      "scGPT - INFO - | epoch   5 | 1000/2309 batches | lr 0.0001 | ms/batch 90.89 | loss  0.63 | cls  0.63 | err  0.17 | \n",
      "scGPT - INFO - | epoch   5 | 1100/2309 batches | lr 0.0001 | ms/batch 90.90 | loss  0.56 | cls  0.56 | err  0.15 | \n",
      "scGPT - INFO - | epoch   5 | 1200/2309 batches | lr 0.0001 | ms/batch 91.02 | loss  0.65 | cls  0.65 | err  0.19 | \n",
      "scGPT - INFO - | epoch   5 | 1300/2309 batches | lr 0.0001 | ms/batch 90.75 | loss  0.67 | cls  0.67 | err  0.21 | \n",
      "scGPT - INFO - | epoch   5 | 1400/2309 batches | lr 0.0001 | ms/batch 91.03 | loss  0.58 | cls  0.58 | err  0.18 | \n",
      "scGPT - INFO - | epoch   5 | 1500/2309 batches | lr 0.0001 | ms/batch 91.09 | loss  0.54 | cls  0.54 | err  0.16 | \n",
      "scGPT - INFO - | epoch   5 | 1600/2309 batches | lr 0.0001 | ms/batch 90.91 | loss  0.53 | cls  0.53 | err  0.16 | \n",
      "scGPT - INFO - | epoch   5 | 1700/2309 batches | lr 0.0001 | ms/batch 90.97 | loss  0.51 | cls  0.51 | err  0.16 | \n",
      "scGPT - INFO - | epoch   5 | 1800/2309 batches | lr 0.0001 | ms/batch 91.28 | loss  0.55 | cls  0.55 | err  0.17 | \n",
      "scGPT - INFO - | epoch   5 | 1900/2309 batches | lr 0.0001 | ms/batch 91.02 | loss  0.55 | cls  0.55 | err  0.16 | \n",
      "scGPT - INFO - | epoch   5 | 2000/2309 batches | lr 0.0001 | ms/batch 91.04 | loss  0.54 | cls  0.54 | err  0.17 | \n",
      "scGPT - INFO - | epoch   5 | 2100/2309 batches | lr 0.0001 | ms/batch 92.57 | loss  0.57 | cls  0.57 | err  0.16 | \n",
      "scGPT - INFO - | epoch   5 | 2200/2309 batches | lr 0.0001 | ms/batch 93.58 | loss  0.53 | cls  0.53 | err  0.16 | \n",
      "scGPT - INFO - | epoch   5 | 2300/2309 batches | lr 0.0001 | ms/batch 91.17 | loss  0.54 | cls  0.54 | err  0.14 | \n",
      "valid/mse 0.6120126824896595 valid/err 0.19003789929615592 valid/dab 0.0 valid/sum_mse_dab 0.6120126824896595 epoch 5\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 220.13s | valid loss/mse 0.6120 | err 0.1900\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   6, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   6 | 100/2309 batches | lr 0.0001 | ms/batch 94.57 | loss  0.60 | cls  0.60 | err  0.17 | \n",
      "scGPT - INFO - | epoch   6 | 200/2309 batches | lr 0.0001 | ms/batch 91.44 | loss  0.55 | cls  0.55 | err  0.17 | \n",
      "scGPT - INFO - | epoch   6 | 300/2309 batches | lr 0.0001 | ms/batch 91.07 | loss  0.53 | cls  0.53 | err  0.16 | \n",
      "scGPT - INFO - | epoch   6 | 400/2309 batches | lr 0.0001 | ms/batch 90.68 | loss  0.56 | cls  0.56 | err  0.15 | \n",
      "scGPT - INFO - | epoch   6 | 500/2309 batches | lr 0.0001 | ms/batch 90.77 | loss  0.41 | cls  0.41 | err  0.12 | \n",
      "scGPT - INFO - | epoch   6 | 600/2309 batches | lr 0.0001 | ms/batch 90.65 | loss  0.56 | cls  0.56 | err  0.15 | \n",
      "scGPT - INFO - | epoch   6 | 700/2309 batches | lr 0.0001 | ms/batch 91.17 | loss  0.55 | cls  0.55 | err  0.16 | \n",
      "scGPT - INFO - | epoch   6 | 800/2309 batches | lr 0.0001 | ms/batch 91.22 | loss  0.51 | cls  0.51 | err  0.14 | \n",
      "scGPT - INFO - | epoch   6 | 900/2309 batches | lr 0.0001 | ms/batch 90.29 | loss  0.57 | cls  0.57 | err  0.16 | \n",
      "scGPT - INFO - | epoch   6 | 1000/2309 batches | lr 0.0001 | ms/batch 90.62 | loss  0.54 | cls  0.54 | err  0.14 | \n",
      "scGPT - INFO - | epoch   6 | 1100/2309 batches | lr 0.0001 | ms/batch 90.64 | loss  0.50 | cls  0.50 | err  0.14 | \n",
      "scGPT - INFO - | epoch   6 | 1200/2309 batches | lr 0.0001 | ms/batch 90.27 | loss  0.57 | cls  0.57 | err  0.16 | \n",
      "scGPT - INFO - | epoch   6 | 1300/2309 batches | lr 0.0001 | ms/batch 90.61 | loss  0.65 | cls  0.65 | err  0.18 | \n",
      "scGPT - INFO - | epoch   6 | 1400/2309 batches | lr 0.0001 | ms/batch 90.68 | loss  0.54 | cls  0.54 | err  0.16 | \n",
      "scGPT - INFO - | epoch   6 | 1500/2309 batches | lr 0.0001 | ms/batch 90.50 | loss  0.49 | cls  0.49 | err  0.15 | \n",
      "scGPT - INFO - | epoch   6 | 1600/2309 batches | lr 0.0001 | ms/batch 90.72 | loss  0.48 | cls  0.48 | err  0.12 | \n",
      "scGPT - INFO - | epoch   6 | 1700/2309 batches | lr 0.0001 | ms/batch 90.63 | loss  0.49 | cls  0.49 | err  0.15 | \n",
      "scGPT - INFO - | epoch   6 | 1800/2309 batches | lr 0.0001 | ms/batch 90.44 | loss  0.48 | cls  0.48 | err  0.14 | \n",
      "scGPT - INFO - | epoch   6 | 1900/2309 batches | lr 0.0001 | ms/batch 90.66 | loss  0.49 | cls  0.49 | err  0.14 | \n",
      "scGPT - INFO - | epoch   6 | 2000/2309 batches | lr 0.0001 | ms/batch 91.27 | loss  0.47 | cls  0.47 | err  0.14 | \n",
      "scGPT - INFO - | epoch   6 | 2100/2309 batches | lr 0.0001 | ms/batch 91.38 | loss  0.53 | cls  0.53 | err  0.15 | \n",
      "scGPT - INFO - | epoch   6 | 2200/2309 batches | lr 0.0001 | ms/batch 93.40 | loss  0.49 | cls  0.49 | err  0.14 | \n",
      "scGPT - INFO - | epoch   6 | 2300/2309 batches | lr 0.0001 | ms/batch 92.13 | loss  0.51 | cls  0.51 | err  0.13 | \n",
      "valid/mse 0.5122474431507511 valid/err 0.1488900920411478 valid/dab 0.0 valid/sum_mse_dab 0.5122474431507511 epoch 6\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 219.28s | valid loss/mse 0.5122 | err 0.1489\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.5122\n",
      "random masking at epoch   7, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   7 | 100/2309 batches | lr 0.0001 | ms/batch 94.21 | loss  0.54 | cls  0.54 | err  0.14 | \n",
      "scGPT - INFO - | epoch   7 | 200/2309 batches | lr 0.0001 | ms/batch 93.29 | loss  0.49 | cls  0.49 | err  0.14 | \n",
      "scGPT - INFO - | epoch   7 | 300/2309 batches | lr 0.0001 | ms/batch 91.25 | loss  0.48 | cls  0.48 | err  0.14 | \n",
      "scGPT - INFO - | epoch   7 | 400/2309 batches | lr 0.0001 | ms/batch 90.68 | loss  0.52 | cls  0.52 | err  0.13 | \n",
      "scGPT - INFO - | epoch   7 | 500/2309 batches | lr 0.0001 | ms/batch 91.06 | loss  0.38 | cls  0.38 | err  0.10 | \n",
      "scGPT - INFO - | epoch   7 | 600/2309 batches | lr 0.0001 | ms/batch 91.76 | loss  0.53 | cls  0.53 | err  0.14 | \n",
      "scGPT - INFO - | epoch   7 | 700/2309 batches | lr 0.0001 | ms/batch 91.64 | loss  0.52 | cls  0.52 | err  0.14 | \n",
      "scGPT - INFO - | epoch   7 | 800/2309 batches | lr 0.0001 | ms/batch 91.42 | loss  0.46 | cls  0.46 | err  0.12 | \n",
      "scGPT - INFO - | epoch   7 | 900/2309 batches | lr 0.0001 | ms/batch 90.34 | loss  0.55 | cls  0.55 | err  0.15 | \n",
      "scGPT - INFO - | epoch   7 | 1000/2309 batches | lr 0.0001 | ms/batch 91.01 | loss  0.48 | cls  0.48 | err  0.12 | \n",
      "scGPT - INFO - | epoch   7 | 1100/2309 batches | lr 0.0001 | ms/batch 91.47 | loss  0.47 | cls  0.47 | err  0.13 | \n",
      "scGPT - INFO - | epoch   7 | 1200/2309 batches | lr 0.0001 | ms/batch 91.30 | loss  0.54 | cls  0.54 | err  0.15 | \n",
      "scGPT - INFO - | epoch   7 | 1300/2309 batches | lr 0.0001 | ms/batch 90.79 | loss  0.60 | cls  0.60 | err  0.16 | \n",
      "scGPT - INFO - | epoch   7 | 1400/2309 batches | lr 0.0001 | ms/batch 90.79 | loss  0.45 | cls  0.45 | err  0.14 | \n",
      "scGPT - INFO - | epoch   7 | 1500/2309 batches | lr 0.0001 | ms/batch 91.18 | loss  0.44 | cls  0.44 | err  0.13 | \n",
      "scGPT - INFO - | epoch   7 | 1600/2309 batches | lr 0.0001 | ms/batch 91.39 | loss  0.43 | cls  0.43 | err  0.12 | \n",
      "scGPT - INFO - | epoch   7 | 1700/2309 batches | lr 0.0001 | ms/batch 91.53 | loss  0.39 | cls  0.39 | err  0.11 | \n",
      "scGPT - INFO - | epoch   7 | 1800/2309 batches | lr 0.0001 | ms/batch 91.29 | loss  0.45 | cls  0.45 | err  0.13 | \n",
      "scGPT - INFO - | epoch   7 | 1900/2309 batches | lr 0.0001 | ms/batch 91.34 | loss  0.43 | cls  0.43 | err  0.12 | \n",
      "scGPT - INFO - | epoch   7 | 2000/2309 batches | lr 0.0001 | ms/batch 91.31 | loss  0.44 | cls  0.44 | err  0.12 | \n",
      "scGPT - INFO - | epoch   7 | 2100/2309 batches | lr 0.0001 | ms/batch 90.36 | loss  0.47 | cls  0.47 | err  0.13 | \n",
      "scGPT - INFO - | epoch   7 | 2200/2309 batches | lr 0.0001 | ms/batch 90.99 | loss  0.42 | cls  0.42 | err  0.12 | \n",
      "scGPT - INFO - | epoch   7 | 2300/2309 batches | lr 0.0001 | ms/batch 91.14 | loss  0.43 | cls  0.43 | err  0.11 | \n",
      "valid/mse 0.48219198732358026 valid/err 0.12831618841364376 valid/dab 0.0 valid/sum_mse_dab 0.48219198732358026 epoch 7\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 220.12s | valid loss/mse 0.4822 | err 0.1283\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4822\n",
      "random masking at epoch   8, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   8 | 100/2309 batches | lr 0.0000 | ms/batch 93.54 | loss  0.48 | cls  0.48 | err  0.13 | \n",
      "scGPT - INFO - | epoch   8 | 200/2309 batches | lr 0.0000 | ms/batch 90.93 | loss  0.44 | cls  0.44 | err  0.12 | \n",
      "scGPT - INFO - | epoch   8 | 300/2309 batches | lr 0.0000 | ms/batch 90.61 | loss  0.44 | cls  0.44 | err  0.13 | \n",
      "scGPT - INFO - | epoch   8 | 400/2309 batches | lr 0.0000 | ms/batch 90.15 | loss  0.46 | cls  0.46 | err  0.12 | \n",
      "scGPT - INFO - | epoch   8 | 500/2309 batches | lr 0.0000 | ms/batch 90.79 | loss  0.36 | cls  0.36 | err  0.09 | \n",
      "scGPT - INFO - | epoch   8 | 600/2309 batches | lr 0.0000 | ms/batch 91.10 | loss  0.49 | cls  0.49 | err  0.13 | \n",
      "scGPT - INFO - | epoch   8 | 700/2309 batches | lr 0.0000 | ms/batch 89.65 | loss  0.43 | cls  0.43 | err  0.12 | \n",
      "scGPT - INFO - | epoch   8 | 800/2309 batches | lr 0.0000 | ms/batch 90.94 | loss  0.42 | cls  0.42 | err  0.11 | \n",
      "scGPT - INFO - | epoch   8 | 900/2309 batches | lr 0.0000 | ms/batch 90.87 | loss  0.50 | cls  0.50 | err  0.13 | \n",
      "scGPT - INFO - | epoch   8 | 1000/2309 batches | lr 0.0000 | ms/batch 90.78 | loss  0.48 | cls  0.48 | err  0.13 | \n",
      "scGPT - INFO - | epoch   8 | 1100/2309 batches | lr 0.0000 | ms/batch 90.83 | loss  0.43 | cls  0.43 | err  0.12 | \n",
      "scGPT - INFO - | epoch   8 | 1200/2309 batches | lr 0.0000 | ms/batch 90.66 | loss  0.49 | cls  0.49 | err  0.14 | \n",
      "scGPT - INFO - | epoch   8 | 1300/2309 batches | lr 0.0000 | ms/batch 90.74 | loss  0.50 | cls  0.50 | err  0.14 | \n",
      "scGPT - INFO - | epoch   8 | 1400/2309 batches | lr 0.0000 | ms/batch 90.84 | loss  0.42 | cls  0.42 | err  0.11 | \n",
      "scGPT - INFO - | epoch   8 | 1500/2309 batches | lr 0.0000 | ms/batch 87.64 | loss  0.43 | cls  0.43 | err  0.12 | \n",
      "scGPT - INFO - | epoch   8 | 1600/2309 batches | lr 0.0000 | ms/batch 92.92 | loss  0.39 | cls  0.39 | err  0.10 | \n",
      "scGPT - INFO - | epoch   8 | 1700/2309 batches | lr 0.0000 | ms/batch 95.17 | loss  0.37 | cls  0.37 | err  0.10 | \n",
      "scGPT - INFO - | epoch   8 | 1800/2309 batches | lr 0.0000 | ms/batch 93.88 | loss  0.38 | cls  0.38 | err  0.10 | \n",
      "scGPT - INFO - | epoch   8 | 1900/2309 batches | lr 0.0000 | ms/batch 93.16 | loss  0.40 | cls  0.40 | err  0.10 | \n",
      "scGPT - INFO - | epoch   8 | 2000/2309 batches | lr 0.0000 | ms/batch 90.03 | loss  0.39 | cls  0.39 | err  0.10 | \n",
      "scGPT - INFO - | epoch   8 | 2100/2309 batches | lr 0.0000 | ms/batch 92.53 | loss  0.45 | cls  0.45 | err  0.12 | \n",
      "scGPT - INFO - | epoch   8 | 2200/2309 batches | lr 0.0000 | ms/batch 92.00 | loss  0.39 | cls  0.39 | err  0.11 | \n",
      "scGPT - INFO - | epoch   8 | 2300/2309 batches | lr 0.0000 | ms/batch 90.86 | loss  0.40 | cls  0.40 | err  0.09 | \n",
      "valid/mse 0.4646397896845178 valid/err 0.12019491066594477 valid/dab 0.0 valid/sum_mse_dab 0.4646397896845178 epoch 8\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 220.10s | valid loss/mse 0.4646 | err 0.1202\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4646\n",
      "random masking at epoch   9, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   9 | 100/2309 batches | lr 0.0000 | ms/batch 93.83 | loss  0.39 | cls  0.39 | err  0.10 | \n",
      "scGPT - INFO - | epoch   9 | 200/2309 batches | lr 0.0000 | ms/batch 90.67 | loss  0.44 | cls  0.44 | err  0.12 | \n",
      "scGPT - INFO - | epoch   9 | 300/2309 batches | lr 0.0000 | ms/batch 90.63 | loss  0.38 | cls  0.38 | err  0.10 | \n",
      "scGPT - INFO - | epoch   9 | 400/2309 batches | lr 0.0000 | ms/batch 94.04 | loss  0.43 | cls  0.43 | err  0.11 | \n",
      "scGPT - INFO - | epoch   9 | 500/2309 batches | lr 0.0000 | ms/batch 91.93 | loss  0.29 | cls  0.29 | err  0.07 | \n",
      "scGPT - INFO - | epoch   9 | 600/2309 batches | lr 0.0000 | ms/batch 90.29 | loss  0.46 | cls  0.46 | err  0.13 | \n",
      "scGPT - INFO - | epoch   9 | 700/2309 batches | lr 0.0000 | ms/batch 85.78 | loss  0.43 | cls  0.43 | err  0.12 | \n",
      "scGPT - INFO - | epoch   9 | 800/2309 batches | lr 0.0000 | ms/batch 80.58 | loss  0.40 | cls  0.40 | err  0.11 | \n",
      "scGPT - INFO - | epoch   9 | 900/2309 batches | lr 0.0000 | ms/batch 84.00 | loss  0.45 | cls  0.45 | err  0.11 | \n",
      "scGPT - INFO - | epoch   9 | 1000/2309 batches | lr 0.0000 | ms/batch 94.91 | loss  0.42 | cls  0.42 | err  0.10 | \n",
      "scGPT - INFO - | epoch   9 | 1100/2309 batches | lr 0.0000 | ms/batch 95.19 | loss  0.39 | cls  0.39 | err  0.10 | \n",
      "scGPT - INFO - | epoch   9 | 1200/2309 batches | lr 0.0000 | ms/batch 93.13 | loss  0.47 | cls  0.47 | err  0.12 | \n",
      "scGPT - INFO - | epoch   9 | 1300/2309 batches | lr 0.0000 | ms/batch 83.31 | loss  0.52 | cls  0.52 | err  0.14 | \n",
      "scGPT - INFO - | epoch   9 | 1400/2309 batches | lr 0.0000 | ms/batch 86.65 | loss  0.37 | cls  0.37 | err  0.09 | \n",
      "scGPT - INFO - | epoch   9 | 1500/2309 batches | lr 0.0000 | ms/batch 89.65 | loss  0.37 | cls  0.37 | err  0.10 | \n",
      "scGPT - INFO - | epoch   9 | 1600/2309 batches | lr 0.0000 | ms/batch 89.70 | loss  0.34 | cls  0.34 | err  0.09 | \n",
      "scGPT - INFO - | epoch   9 | 1700/2309 batches | lr 0.0000 | ms/batch 90.11 | loss  0.31 | cls  0.31 | err  0.09 | \n",
      "scGPT - INFO - | epoch   9 | 1800/2309 batches | lr 0.0000 | ms/batch 90.20 | loss  0.36 | cls  0.36 | err  0.10 | \n",
      "scGPT - INFO - | epoch   9 | 1900/2309 batches | lr 0.0000 | ms/batch 90.61 | loss  0.36 | cls  0.36 | err  0.09 | \n",
      "scGPT - INFO - | epoch   9 | 2000/2309 batches | lr 0.0000 | ms/batch 90.27 | loss  0.36 | cls  0.36 | err  0.09 | \n",
      "scGPT - INFO - | epoch   9 | 2100/2309 batches | lr 0.0000 | ms/batch 91.28 | loss  0.39 | cls  0.39 | err  0.11 | \n",
      "scGPT - INFO - | epoch   9 | 2200/2309 batches | lr 0.0000 | ms/batch 91.20 | loss  0.31 | cls  0.31 | err  0.09 | \n",
      "scGPT - INFO - | epoch   9 | 2300/2309 batches | lr 0.0000 | ms/batch 91.13 | loss  0.43 | cls  0.43 | err  0.09 | \n",
      "valid/mse 0.3962817608379332 valid/err 0.10070384407146725 valid/dab 0.0 valid/sum_mse_dab 0.3962817608379332 epoch 9\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 216.70s | valid loss/mse 0.3963 | err 0.1007\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.3963\n",
      "random masking at epoch  10, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  10 | 100/2309 batches | lr 0.0000 | ms/batch 94.33 | loss  0.37 | cls  0.37 | err  0.10 | \n",
      "scGPT - INFO - | epoch  10 | 200/2309 batches | lr 0.0000 | ms/batch 91.71 | loss  0.37 | cls  0.37 | err  0.10 | \n",
      "scGPT - INFO - | epoch  10 | 300/2309 batches | lr 0.0000 | ms/batch 91.57 | loss  0.43 | cls  0.43 | err  0.11 | \n",
      "scGPT - INFO - | epoch  10 | 400/2309 batches | lr 0.0000 | ms/batch 91.07 | loss  0.40 | cls  0.40 | err  0.10 | \n",
      "scGPT - INFO - | epoch  10 | 500/2309 batches | lr 0.0000 | ms/batch 90.94 | loss  0.27 | cls  0.27 | err  0.07 | \n",
      "scGPT - INFO - | epoch  10 | 600/2309 batches | lr 0.0000 | ms/batch 91.12 | loss  0.41 | cls  0.41 | err  0.10 | \n",
      "scGPT - INFO - | epoch  10 | 700/2309 batches | lr 0.0000 | ms/batch 91.04 | loss  0.35 | cls  0.35 | err  0.09 | \n",
      "scGPT - INFO - | epoch  10 | 800/2309 batches | lr 0.0000 | ms/batch 91.76 | loss  0.32 | cls  0.32 | err  0.08 | \n",
      "scGPT - INFO - | epoch  10 | 900/2309 batches | lr 0.0000 | ms/batch 92.51 | loss  0.44 | cls  0.44 | err  0.10 | \n",
      "scGPT - INFO - | epoch  10 | 1000/2309 batches | lr 0.0000 | ms/batch 91.40 | loss  0.34 | cls  0.34 | err  0.08 | \n",
      "scGPT - INFO - | epoch  10 | 1100/2309 batches | lr 0.0000 | ms/batch 91.61 | loss  0.34 | cls  0.34 | err  0.08 | \n",
      "scGPT - INFO - | epoch  10 | 1200/2309 batches | lr 0.0000 | ms/batch 91.52 | loss  0.44 | cls  0.44 | err  0.11 | \n",
      "scGPT - INFO - | epoch  10 | 1300/2309 batches | lr 0.0000 | ms/batch 93.38 | loss  0.44 | cls  0.44 | err  0.12 | \n",
      "scGPT - INFO - | epoch  10 | 1400/2309 batches | lr 0.0000 | ms/batch 95.11 | loss  0.33 | cls  0.33 | err  0.09 | \n",
      "scGPT - INFO - | epoch  10 | 1500/2309 batches | lr 0.0000 | ms/batch 94.46 | loss  0.31 | cls  0.31 | err  0.08 | \n",
      "scGPT - INFO - | epoch  10 | 1600/2309 batches | lr 0.0000 | ms/batch 92.99 | loss  0.32 | cls  0.32 | err  0.08 | \n",
      "scGPT - INFO - | epoch  10 | 1700/2309 batches | lr 0.0000 | ms/batch 94.81 | loss  0.26 | cls  0.26 | err  0.07 | \n",
      "scGPT - INFO - | epoch  10 | 1800/2309 batches | lr 0.0000 | ms/batch 91.13 | loss  0.36 | cls  0.36 | err  0.10 | \n",
      "scGPT - INFO - | epoch  10 | 1900/2309 batches | lr 0.0000 | ms/batch 89.76 | loss  0.27 | cls  0.27 | err  0.07 | \n",
      "scGPT - INFO - | epoch  10 | 2000/2309 batches | lr 0.0000 | ms/batch 94.62 | loss  0.33 | cls  0.33 | err  0.08 | \n",
      "scGPT - INFO - | epoch  10 | 2100/2309 batches | lr 0.0000 | ms/batch 92.14 | loss  0.39 | cls  0.39 | err  0.10 | \n",
      "scGPT - INFO - | epoch  10 | 2200/2309 batches | lr 0.0000 | ms/batch 91.19 | loss  0.27 | cls  0.27 | err  0.08 | \n",
      "scGPT - INFO - | epoch  10 | 2300/2309 batches | lr 0.0000 | ms/batch 91.65 | loss  0.36 | cls  0.36 | err  0.08 | \n",
      "valid/mse 0.3645263115114532 valid/err 0.08933405522468868 valid/dab 0.0 valid/sum_mse_dab 0.3645263115114532 epoch 10\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 222.01s | valid loss/mse 0.3645 | err 0.0893\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.3645\n",
      "random masking at epoch  11, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  11 | 100/2309 batches | lr 0.0000 | ms/batch 97.33 | loss  0.30 | cls  0.30 | err  0.09 | \n",
      "scGPT - INFO - | epoch  11 | 200/2309 batches | lr 0.0000 | ms/batch 95.58 | loss  0.35 | cls  0.35 | err  0.09 | \n",
      "scGPT - INFO - | epoch  11 | 300/2309 batches | lr 0.0000 | ms/batch 96.72 | loss  0.35 | cls  0.35 | err  0.09 | \n",
      "scGPT - INFO - | epoch  11 | 400/2309 batches | lr 0.0000 | ms/batch 96.39 | loss  0.35 | cls  0.35 | err  0.09 | \n",
      "scGPT - INFO - | epoch  11 | 500/2309 batches | lr 0.0000 | ms/batch 95.68 | loss  0.27 | cls  0.27 | err  0.08 | \n",
      "scGPT - INFO - | epoch  11 | 600/2309 batches | lr 0.0000 | ms/batch 95.26 | loss  0.37 | cls  0.37 | err  0.10 | \n",
      "scGPT - INFO - | epoch  11 | 700/2309 batches | lr 0.0000 | ms/batch 94.07 | loss  0.34 | cls  0.34 | err  0.08 | \n",
      "scGPT - INFO - | epoch  11 | 800/2309 batches | lr 0.0000 | ms/batch 91.43 | loss  0.30 | cls  0.30 | err  0.08 | \n",
      "scGPT - INFO - | epoch  11 | 900/2309 batches | lr 0.0000 | ms/batch 91.05 | loss  0.41 | cls  0.41 | err  0.10 | \n",
      "scGPT - INFO - | epoch  11 | 1000/2309 batches | lr 0.0000 | ms/batch 90.36 | loss  0.36 | cls  0.36 | err  0.09 | \n",
      "scGPT - INFO - | epoch  11 | 1100/2309 batches | lr 0.0000 | ms/batch 90.46 | loss  0.32 | cls  0.32 | err  0.08 | \n",
      "scGPT - INFO - | epoch  11 | 1200/2309 batches | lr 0.0000 | ms/batch 90.73 | loss  0.39 | cls  0.39 | err  0.09 | \n",
      "scGPT - INFO - | epoch  11 | 1300/2309 batches | lr 0.0000 | ms/batch 88.44 | loss  0.43 | cls  0.43 | err  0.10 | \n",
      "scGPT - INFO - | epoch  11 | 1400/2309 batches | lr 0.0000 | ms/batch 90.71 | loss  0.31 | cls  0.31 | err  0.07 | \n",
      "scGPT - INFO - | epoch  11 | 1500/2309 batches | lr 0.0000 | ms/batch 90.65 | loss  0.30 | cls  0.30 | err  0.07 | \n",
      "scGPT - INFO - | epoch  11 | 1600/2309 batches | lr 0.0000 | ms/batch 90.61 | loss  0.27 | cls  0.27 | err  0.07 | \n",
      "scGPT - INFO - | epoch  11 | 1700/2309 batches | lr 0.0000 | ms/batch 90.77 | loss  0.23 | cls  0.23 | err  0.07 | \n",
      "scGPT - INFO - | epoch  11 | 1800/2309 batches | lr 0.0000 | ms/batch 90.80 | loss  0.28 | cls  0.28 | err  0.07 | \n",
      "scGPT - INFO - | epoch  11 | 1900/2309 batches | lr 0.0000 | ms/batch 90.34 | loss  0.27 | cls  0.27 | err  0.06 | \n",
      "scGPT - INFO - | epoch  11 | 2000/2309 batches | lr 0.0000 | ms/batch 90.64 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  11 | 2100/2309 batches | lr 0.0000 | ms/batch 90.28 | loss  0.31 | cls  0.31 | err  0.07 | \n",
      "scGPT - INFO - | epoch  11 | 2200/2309 batches | lr 0.0000 | ms/batch 90.59 | loss  0.26 | cls  0.26 | err  0.07 | \n",
      "scGPT - INFO - | epoch  11 | 2300/2309 batches | lr 0.0000 | ms/batch 89.54 | loss  0.33 | cls  0.33 | err  0.07 | \n",
      "valid/mse 0.34315904644083456 valid/err 0.08554412560909583 valid/dab 0.0 valid/sum_mse_dab 0.34315904644083456 epoch 11\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 221.65s | valid loss/mse 0.3432 | err 0.0855\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.3432\n",
      "random masking at epoch  12, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  12 | 100/2309 batches | lr 0.0000 | ms/batch 93.09 | loss  0.31 | cls  0.31 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 200/2309 batches | lr 0.0000 | ms/batch 82.86 | loss  0.33 | cls  0.33 | err  0.08 | \n",
      "scGPT - INFO - | epoch  12 | 300/2309 batches | lr 0.0000 | ms/batch 86.75 | loss  0.32 | cls  0.32 | err  0.07 | \n",
      "scGPT - INFO - | epoch  12 | 400/2309 batches | lr 0.0000 | ms/batch 90.60 | loss  0.32 | cls  0.32 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 500/2309 batches | lr 0.0000 | ms/batch 81.64 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  12 | 600/2309 batches | lr 0.0000 | ms/batch 89.44 | loss  0.31 | cls  0.31 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 700/2309 batches | lr 0.0000 | ms/batch 89.64 | loss  0.35 | cls  0.35 | err  0.08 | \n",
      "scGPT - INFO - | epoch  12 | 800/2309 batches | lr 0.0000 | ms/batch 90.65 | loss  0.23 | cls  0.23 | err  0.06 | \n",
      "scGPT - INFO - | epoch  12 | 900/2309 batches | lr 0.0000 | ms/batch 90.66 | loss  0.34 | cls  0.34 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 1000/2309 batches | lr 0.0000 | ms/batch 90.18 | loss  0.30 | cls  0.30 | err  0.07 | \n",
      "scGPT - INFO - | epoch  12 | 1100/2309 batches | lr 0.0000 | ms/batch 90.67 | loss  0.32 | cls  0.32 | err  0.07 | \n",
      "scGPT - INFO - | epoch  12 | 1200/2309 batches | lr 0.0000 | ms/batch 90.52 | loss  0.35 | cls  0.35 | err  0.07 | \n",
      "scGPT - INFO - | epoch  12 | 1300/2309 batches | lr 0.0000 | ms/batch 90.54 | loss  0.34 | cls  0.34 | err  0.08 | \n",
      "scGPT - INFO - | epoch  12 | 1400/2309 batches | lr 0.0000 | ms/batch 90.70 | loss  0.30 | cls  0.30 | err  0.07 | \n",
      "scGPT - INFO - | epoch  12 | 1500/2309 batches | lr 0.0000 | ms/batch 90.30 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  12 | 1600/2309 batches | lr 0.0000 | ms/batch 90.66 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  12 | 1700/2309 batches | lr 0.0000 | ms/batch 90.68 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  12 | 1800/2309 batches | lr 0.0000 | ms/batch 90.76 | loss  0.26 | cls  0.26 | err  0.07 | \n",
      "scGPT - INFO - | epoch  12 | 1900/2309 batches | lr 0.0000 | ms/batch 90.15 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  12 | 2000/2309 batches | lr 0.0000 | ms/batch 89.92 | loss  0.26 | cls  0.26 | err  0.07 | \n",
      "scGPT - INFO - | epoch  12 | 2100/2309 batches | lr 0.0000 | ms/batch 90.63 | loss  0.29 | cls  0.29 | err  0.07 | \n",
      "scGPT - INFO - | epoch  12 | 2200/2309 batches | lr 0.0000 | ms/batch 90.65 | loss  0.25 | cls  0.25 | err  0.05 | \n",
      "scGPT - INFO - | epoch  12 | 2300/2309 batches | lr 0.0000 | ms/batch 90.64 | loss  0.32 | cls  0.32 | err  0.07 | \n",
      "valid/mse 0.27131465408178557 valid/err 0.06551164049810504 valid/dab 0.0 valid/sum_mse_dab 0.27131465408178557 epoch 12\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 216.17s | valid loss/mse 0.2713 | err 0.0655\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.2713\n",
      "random masking at epoch  13, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  13 | 100/2309 batches | lr 0.0000 | ms/batch 94.96 | loss  0.25 | cls  0.25 | err  0.06 | \n",
      "scGPT - INFO - | epoch  13 | 200/2309 batches | lr 0.0000 | ms/batch 91.02 | loss  0.29 | cls  0.29 | err  0.07 | \n",
      "scGPT - INFO - | epoch  13 | 300/2309 batches | lr 0.0000 | ms/batch 91.08 | loss  0.25 | cls  0.25 | err  0.06 | \n",
      "scGPT - INFO - | epoch  13 | 400/2309 batches | lr 0.0000 | ms/batch 91.16 | loss  0.31 | cls  0.31 | err  0.07 | \n",
      "scGPT - INFO - | epoch  13 | 500/2309 batches | lr 0.0000 | ms/batch 91.13 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  13 | 600/2309 batches | lr 0.0000 | ms/batch 91.16 | loss  0.31 | cls  0.31 | err  0.07 | \n",
      "scGPT - INFO - | epoch  13 | 700/2309 batches | lr 0.0000 | ms/batch 91.40 | loss  0.26 | cls  0.26 | err  0.05 | \n",
      "scGPT - INFO - | epoch  13 | 800/2309 batches | lr 0.0000 | ms/batch 90.17 | loss  0.23 | cls  0.23 | err  0.05 | \n",
      "scGPT - INFO - | epoch  13 | 900/2309 batches | lr 0.0000 | ms/batch 90.44 | loss  0.31 | cls  0.31 | err  0.08 | \n",
      "scGPT - INFO - | epoch  13 | 1000/2309 batches | lr 0.0000 | ms/batch 93.72 | loss  0.29 | cls  0.29 | err  0.08 | \n",
      "scGPT - INFO - | epoch  13 | 1100/2309 batches | lr 0.0000 | ms/batch 93.67 | loss  0.29 | cls  0.29 | err  0.07 | \n",
      "scGPT - INFO - | epoch  13 | 1200/2309 batches | lr 0.0000 | ms/batch 89.88 | loss  0.32 | cls  0.32 | err  0.07 | \n",
      "scGPT - INFO - | epoch  13 | 1300/2309 batches | lr 0.0000 | ms/batch 90.44 | loss  0.30 | cls  0.30 | err  0.07 | \n",
      "scGPT - INFO - | epoch  13 | 1400/2309 batches | lr 0.0000 | ms/batch 90.57 | loss  0.27 | cls  0.27 | err  0.06 | \n",
      "scGPT - INFO - | epoch  13 | 1500/2309 batches | lr 0.0000 | ms/batch 90.83 | loss  0.26 | cls  0.26 | err  0.06 | \n",
      "scGPT - INFO - | epoch  13 | 1600/2309 batches | lr 0.0000 | ms/batch 90.80 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  13 | 1700/2309 batches | lr 0.0000 | ms/batch 90.72 | loss  0.19 | cls  0.19 | err  0.06 | \n",
      "scGPT - INFO - | epoch  13 | 1800/2309 batches | lr 0.0000 | ms/batch 90.21 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  13 | 1900/2309 batches | lr 0.0000 | ms/batch 90.65 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  13 | 2000/2309 batches | lr 0.0000 | ms/batch 90.63 | loss  0.24 | cls  0.24 | err  0.05 | \n",
      "scGPT - INFO - | epoch  13 | 2100/2309 batches | lr 0.0000 | ms/batch 90.74 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  13 | 2200/2309 batches | lr 0.0000 | ms/batch 90.71 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  13 | 2300/2309 batches | lr 0.0000 | ms/batch 90.79 | loss  0.26 | cls  0.26 | err  0.06 | \n",
      "valid/mse 0.24425569384746582 valid/err 0.05305901461829995 valid/dab 0.0 valid/sum_mse_dab 0.24425569384746582 epoch 13\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 219.55s | valid loss/mse 0.2443 | err 0.0531\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.2443\n",
      "random masking at epoch  14, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  14 | 100/2309 batches | lr 0.0000 | ms/batch 93.59 | loss  0.25 | cls  0.25 | err  0.07 | \n",
      "scGPT - INFO - | epoch  14 | 200/2309 batches | lr 0.0000 | ms/batch 91.71 | loss  0.26 | cls  0.26 | err  0.06 | \n",
      "scGPT - INFO - | epoch  14 | 300/2309 batches | lr 0.0000 | ms/batch 92.01 | loss  0.26 | cls  0.26 | err  0.06 | \n",
      "scGPT - INFO - | epoch  14 | 400/2309 batches | lr 0.0000 | ms/batch 91.54 | loss  0.26 | cls  0.26 | err  0.06 | \n",
      "scGPT - INFO - | epoch  14 | 500/2309 batches | lr 0.0000 | ms/batch 91.69 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  14 | 600/2309 batches | lr 0.0000 | ms/batch 92.21 | loss  0.29 | cls  0.29 | err  0.07 | \n",
      "scGPT - INFO - | epoch  14 | 700/2309 batches | lr 0.0000 | ms/batch 91.83 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  14 | 800/2309 batches | lr 0.0000 | ms/batch 91.86 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  14 | 900/2309 batches | lr 0.0000 | ms/batch 91.63 | loss  0.29 | cls  0.29 | err  0.07 | \n",
      "scGPT - INFO - | epoch  14 | 1000/2309 batches | lr 0.0000 | ms/batch 90.96 | loss  0.26 | cls  0.26 | err  0.06 | \n",
      "scGPT - INFO - | epoch  14 | 1100/2309 batches | lr 0.0000 | ms/batch 90.84 | loss  0.25 | cls  0.25 | err  0.06 | \n",
      "scGPT - INFO - | epoch  14 | 1200/2309 batches | lr 0.0000 | ms/batch 90.81 | loss  0.31 | cls  0.31 | err  0.07 | \n",
      "scGPT - INFO - | epoch  14 | 1300/2309 batches | lr 0.0000 | ms/batch 90.81 | loss  0.30 | cls  0.30 | err  0.07 | \n",
      "scGPT - INFO - | epoch  14 | 1400/2309 batches | lr 0.0000 | ms/batch 91.39 | loss  0.24 | cls  0.24 | err  0.05 | \n",
      "scGPT - INFO - | epoch  14 | 1500/2309 batches | lr 0.0000 | ms/batch 91.68 | loss  0.23 | cls  0.23 | err  0.05 | \n",
      "scGPT - INFO - | epoch  14 | 1600/2309 batches | lr 0.0000 | ms/batch 91.56 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "scGPT - INFO - | epoch  14 | 1700/2309 batches | lr 0.0000 | ms/batch 91.06 | loss  0.16 | cls  0.16 | err  0.04 | \n",
      "scGPT - INFO - | epoch  14 | 1800/2309 batches | lr 0.0000 | ms/batch 91.61 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "scGPT - INFO - | epoch  14 | 1900/2309 batches | lr 0.0000 | ms/batch 91.74 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  14 | 2000/2309 batches | lr 0.0000 | ms/batch 91.71 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  14 | 2100/2309 batches | lr 0.0000 | ms/batch 90.82 | loss  0.25 | cls  0.25 | err  0.06 | \n",
      "scGPT - INFO - | epoch  14 | 2200/2309 batches | lr 0.0000 | ms/batch 91.41 | loss  0.15 | cls  0.15 | err  0.04 | \n",
      "scGPT - INFO - | epoch  14 | 2300/2309 batches | lr 0.0000 | ms/batch 91.57 | loss  0.26 | cls  0.26 | err  0.06 | \n",
      "valid/mse 0.19668951115818542 valid/err 0.047644829453167295 valid/dab 0.0 valid/sum_mse_dab 0.19668951115818542 epoch 14\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 220.44s | valid loss/mse 0.1967 | err 0.0476\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.1967\n",
      "random masking at epoch  15, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  15 | 100/2309 batches | lr 0.0000 | ms/batch 94.08 | loss  0.19 | cls  0.19 | err  0.05 | \n",
      "scGPT - INFO - | epoch  15 | 200/2309 batches | lr 0.0000 | ms/batch 91.07 | loss  0.25 | cls  0.25 | err  0.05 | \n",
      "scGPT - INFO - | epoch  15 | 300/2309 batches | lr 0.0000 | ms/batch 91.94 | loss  0.23 | cls  0.23 | err  0.06 | \n",
      "scGPT - INFO - | epoch  15 | 400/2309 batches | lr 0.0000 | ms/batch 91.92 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  15 | 500/2309 batches | lr 0.0000 | ms/batch 92.05 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "scGPT - INFO - | epoch  15 | 600/2309 batches | lr 0.0000 | ms/batch 91.44 | loss  0.26 | cls  0.26 | err  0.07 | \n",
      "scGPT - INFO - | epoch  15 | 700/2309 batches | lr 0.0000 | ms/batch 92.03 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  15 | 800/2309 batches | lr 0.0000 | ms/batch 92.04 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  15 | 900/2309 batches | lr 0.0000 | ms/batch 92.19 | loss  0.26 | cls  0.26 | err  0.06 | \n",
      "scGPT - INFO - | epoch  15 | 1000/2309 batches | lr 0.0000 | ms/batch 92.11 | loss  0.21 | cls  0.21 | err  0.06 | \n",
      "scGPT - INFO - | epoch  15 | 1100/2309 batches | lr 0.0000 | ms/batch 91.49 | loss  0.24 | cls  0.24 | err  0.05 | \n",
      "scGPT - INFO - | epoch  15 | 1200/2309 batches | lr 0.0000 | ms/batch 90.99 | loss  0.31 | cls  0.31 | err  0.06 | \n",
      "scGPT - INFO - | epoch  15 | 1300/2309 batches | lr 0.0000 | ms/batch 91.04 | loss  0.27 | cls  0.27 | err  0.06 | \n",
      "scGPT - INFO - | epoch  15 | 1400/2309 batches | lr 0.0000 | ms/batch 90.37 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "scGPT - INFO - | epoch  15 | 1500/2309 batches | lr 0.0000 | ms/batch 90.78 | loss  0.18 | cls  0.18 | err  0.05 | \n",
      "scGPT - INFO - | epoch  15 | 1600/2309 batches | lr 0.0000 | ms/batch 90.69 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  15 | 1700/2309 batches | lr 0.0000 | ms/batch 91.65 | loss  0.12 | cls  0.12 | err  0.04 | \n",
      "scGPT - INFO - | epoch  15 | 1800/2309 batches | lr 0.0000 | ms/batch 91.32 | loss  0.17 | cls  0.17 | err  0.05 | \n",
      "scGPT - INFO - | epoch  15 | 1900/2309 batches | lr 0.0000 | ms/batch 90.76 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  15 | 2000/2309 batches | lr 0.0000 | ms/batch 90.96 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "scGPT - INFO - | epoch  15 | 2100/2309 batches | lr 0.0000 | ms/batch 91.08 | loss  0.23 | cls  0.23 | err  0.04 | \n",
      "scGPT - INFO - | epoch  15 | 2200/2309 batches | lr 0.0000 | ms/batch 91.44 | loss  0.16 | cls  0.16 | err  0.04 | \n",
      "scGPT - INFO - | epoch  15 | 2300/2309 batches | lr 0.0000 | ms/batch 90.77 | loss  0.23 | cls  0.23 | err  0.05 | \n",
      "valid/mse 0.1850342290444702 valid/err 0.041689225771521385 valid/dab 0.0 valid/sum_mse_dab 0.1850342290444702 epoch 15\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 220.52s | valid loss/mse 0.1850 | err 0.0417\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.1850\n",
      "random masking at epoch  16, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  16 | 100/2309 batches | lr 0.0000 | ms/batch 94.79 | loss  0.23 | cls  0.23 | err  0.06 | \n",
      "scGPT - INFO - | epoch  16 | 200/2309 batches | lr 0.0000 | ms/batch 91.96 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "scGPT - INFO - | epoch  16 | 300/2309 batches | lr 0.0000 | ms/batch 91.57 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 400/2309 batches | lr 0.0000 | ms/batch 91.73 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "scGPT - INFO - | epoch  16 | 500/2309 batches | lr 0.0000 | ms/batch 98.28 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  16 | 600/2309 batches | lr 0.0000 | ms/batch 103.43 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 700/2309 batches | lr 0.0000 | ms/batch 91.98 | loss  0.21 | cls  0.21 | err  0.04 | \n",
      "scGPT - INFO - | epoch  16 | 800/2309 batches | lr 0.0000 | ms/batch 91.04 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  16 | 900/2309 batches | lr 0.0000 | ms/batch 91.03 | loss  0.29 | cls  0.29 | err  0.06 | \n",
      "scGPT - INFO - | epoch  16 | 1000/2309 batches | lr 0.0000 | ms/batch 91.09 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 1100/2309 batches | lr 0.0000 | ms/batch 91.36 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 1200/2309 batches | lr 0.0000 | ms/batch 91.83 | loss  0.27 | cls  0.27 | err  0.06 | \n",
      "scGPT - INFO - | epoch  16 | 1300/2309 batches | lr 0.0000 | ms/batch 91.73 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 1400/2309 batches | lr 0.0000 | ms/batch 91.70 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 1500/2309 batches | lr 0.0000 | ms/batch 91.71 | loss  0.14 | cls  0.14 | err  0.04 | \n",
      "scGPT - INFO - | epoch  16 | 1600/2309 batches | lr 0.0000 | ms/batch 91.90 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "scGPT - INFO - | epoch  16 | 1700/2309 batches | lr 0.0000 | ms/batch 91.98 | loss  0.12 | cls  0.12 | err  0.03 | \n",
      "scGPT - INFO - | epoch  16 | 1800/2309 batches | lr 0.0000 | ms/batch 91.97 | loss  0.16 | cls  0.16 | err  0.04 | \n",
      "scGPT - INFO - | epoch  16 | 1900/2309 batches | lr 0.0000 | ms/batch 91.59 | loss  0.15 | cls  0.15 | err  0.04 | \n",
      "scGPT - INFO - | epoch  16 | 2000/2309 batches | lr 0.0000 | ms/batch 91.05 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  16 | 2100/2309 batches | lr 0.0000 | ms/batch 91.02 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 2200/2309 batches | lr 0.0000 | ms/batch 91.04 | loss  0.13 | cls  0.13 | err  0.03 | \n",
      "scGPT - INFO - | epoch  16 | 2300/2309 batches | lr 0.0000 | ms/batch 90.93 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "valid/mse 0.17084251338556514 valid/err 0.03627504060638874 valid/dab 0.0 valid/sum_mse_dab 0.17084251338556514 epoch 16\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  16 | time: 222.55s | valid loss/mse 0.1708 | err 0.0363\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.1708\n",
      "random masking at epoch  17, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  17 | 100/2309 batches | lr 0.0000 | ms/batch 93.29 | loss  0.18 | cls  0.18 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 200/2309 batches | lr 0.0000 | ms/batch 90.99 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 300/2309 batches | lr 0.0000 | ms/batch 89.58 | loss  0.18 | cls  0.18 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 400/2309 batches | lr 0.0000 | ms/batch 90.02 | loss  0.23 | cls  0.23 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 500/2309 batches | lr 0.0000 | ms/batch 88.96 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 600/2309 batches | lr 0.0000 | ms/batch 91.49 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 700/2309 batches | lr 0.0000 | ms/batch 90.74 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 800/2309 batches | lr 0.0000 | ms/batch 90.80 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 900/2309 batches | lr 0.0000 | ms/batch 86.86 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 1000/2309 batches | lr 0.0000 | ms/batch 88.70 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 1100/2309 batches | lr 0.0000 | ms/batch 90.86 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 1200/2309 batches | lr 0.0000 | ms/batch 90.99 | loss  0.24 | cls  0.24 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 1300/2309 batches | lr 0.0000 | ms/batch 91.61 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 1400/2309 batches | lr 0.0000 | ms/batch 91.32 | loss  0.18 | cls  0.18 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 1500/2309 batches | lr 0.0000 | ms/batch 91.35 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 1600/2309 batches | lr 0.0000 | ms/batch 91.57 | loss  0.16 | cls  0.16 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 1700/2309 batches | lr 0.0000 | ms/batch 90.96 | loss  0.10 | cls  0.10 | err  0.02 | \n",
      "scGPT - INFO - | epoch  17 | 1800/2309 batches | lr 0.0000 | ms/batch 90.11 | loss  0.13 | cls  0.13 | err  0.03 | \n",
      "scGPT - INFO - | epoch  17 | 1900/2309 batches | lr 0.0000 | ms/batch 91.25 | loss  0.16 | cls  0.16 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 2000/2309 batches | lr 0.0000 | ms/batch 91.35 | loss  0.14 | cls  0.14 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 2100/2309 batches | lr 0.0000 | ms/batch 91.44 | loss  0.13 | cls  0.13 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 2200/2309 batches | lr 0.0000 | ms/batch 91.18 | loss  0.12 | cls  0.12 | err  0.03 | \n",
      "scGPT - INFO - | epoch  17 | 2300/2309 batches | lr 0.0000 | ms/batch 91.12 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "valid/mse 0.15352476193379505 valid/err 0.03086085544125609 valid/dab 0.0 valid/sum_mse_dab 0.15352476193379505 epoch 17\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  17 | time: 218.41s | valid loss/mse 0.1535 | err 0.0309\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.1535\n",
      "random masking at epoch  18, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  18 | 100/2309 batches | lr 0.0000 | ms/batch 94.07 | loss  0.16 | cls  0.16 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 200/2309 batches | lr 0.0000 | ms/batch 91.60 | loss  0.18 | cls  0.18 | err  0.03 | \n",
      "scGPT - INFO - | epoch  18 | 300/2309 batches | lr 0.0000 | ms/batch 90.90 | loss  0.14 | cls  0.14 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 400/2309 batches | lr 0.0000 | ms/batch 90.76 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 500/2309 batches | lr 0.0000 | ms/batch 90.57 | loss  0.15 | cls  0.15 | err  0.03 | \n",
      "scGPT - INFO - | epoch  18 | 600/2309 batches | lr 0.0000 | ms/batch 90.71 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  18 | 700/2309 batches | lr 0.0000 | ms/batch 90.29 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 800/2309 batches | lr 0.0000 | ms/batch 90.62 | loss  0.13 | cls  0.13 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 900/2309 batches | lr 0.0000 | ms/batch 90.79 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  18 | 1000/2309 batches | lr 0.0000 | ms/batch 90.54 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 1100/2309 batches | lr 0.0000 | ms/batch 90.78 | loss  0.17 | cls  0.17 | err  0.03 | \n",
      "scGPT - INFO - | epoch  18 | 1200/2309 batches | lr 0.0000 | ms/batch 91.31 | loss  0.24 | cls  0.24 | err  0.05 | \n",
      "scGPT - INFO - | epoch  18 | 1300/2309 batches | lr 0.0000 | ms/batch 91.06 | loss  0.17 | cls  0.17 | err  0.03 | \n",
      "scGPT - INFO - | epoch  18 | 1400/2309 batches | lr 0.0000 | ms/batch 90.22 | loss  0.15 | cls  0.15 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 1500/2309 batches | lr 0.0000 | ms/batch 90.27 | loss  0.14 | cls  0.14 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 1600/2309 batches | lr 0.0000 | ms/batch 91.08 | loss  0.17 | cls  0.17 | err  0.03 | \n",
      "scGPT - INFO - | epoch  18 | 1700/2309 batches | lr 0.0000 | ms/batch 91.28 | loss  0.10 | cls  0.10 | err  0.02 | \n",
      "scGPT - INFO - | epoch  18 | 1800/2309 batches | lr 0.0000 | ms/batch 91.32 | loss  0.13 | cls  0.13 | err  0.03 | \n",
      "scGPT - INFO - | epoch  18 | 1900/2309 batches | lr 0.0000 | ms/batch 91.56 | loss  0.11 | cls  0.11 | err  0.02 | \n",
      "scGPT - INFO - | epoch  18 | 2000/2309 batches | lr 0.0000 | ms/batch 91.34 | loss  0.16 | cls  0.16 | err  0.03 | \n",
      "scGPT - INFO - | epoch  18 | 2100/2309 batches | lr 0.0000 | ms/batch 91.20 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 2200/2309 batches | lr 0.0000 | ms/batch 91.30 | loss  0.13 | cls  0.13 | err  0.03 | \n",
      "scGPT - INFO - | epoch  18 | 2300/2309 batches | lr 0.0000 | ms/batch 91.01 | loss  0.14 | cls  0.14 | err  0.03 | \n",
      "valid/mse 0.1336700469012469 valid/err 0.02707092582566324 valid/dab 0.0 valid/sum_mse_dab 0.1336700469012469 epoch 18\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  18 | time: 219.26s | valid loss/mse 0.1337 | err 0.0271\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.1337\n",
      "random masking at epoch  19, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  19 | 100/2309 batches | lr 0.0000 | ms/batch 94.03 | loss  0.15 | cls  0.15 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 200/2309 batches | lr 0.0000 | ms/batch 91.47 | loss  0.17 | cls  0.17 | err  0.03 | \n",
      "scGPT - INFO - | epoch  19 | 300/2309 batches | lr 0.0000 | ms/batch 91.57 | loss  0.13 | cls  0.13 | err  0.03 | \n",
      "scGPT - INFO - | epoch  19 | 400/2309 batches | lr 0.0000 | ms/batch 91.25 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "scGPT - INFO - | epoch  19 | 500/2309 batches | lr 0.0000 | ms/batch 91.31 | loss  0.15 | cls  0.15 | err  0.03 | \n",
      "scGPT - INFO - | epoch  19 | 600/2309 batches | lr 0.0000 | ms/batch 91.21 | loss  0.23 | cls  0.23 | err  0.05 | \n",
      "scGPT - INFO - | epoch  19 | 700/2309 batches | lr 0.0000 | ms/batch 91.49 | loss  0.13 | cls  0.13 | err  0.03 | \n",
      "scGPT - INFO - | epoch  19 | 800/2309 batches | lr 0.0000 | ms/batch 91.19 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 900/2309 batches | lr 0.0000 | ms/batch 91.53 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 1000/2309 batches | lr 0.0000 | ms/batch 91.30 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 1100/2309 batches | lr 0.0000 | ms/batch 91.42 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 1200/2309 batches | lr 0.0000 | ms/batch 91.45 | loss  0.22 | cls  0.22 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 1300/2309 batches | lr 0.0000 | ms/batch 90.81 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 1400/2309 batches | lr 0.0000 | ms/batch 90.75 | loss  0.14 | cls  0.14 | err  0.03 | \n",
      "scGPT - INFO - | epoch  19 | 1500/2309 batches | lr 0.0000 | ms/batch 91.17 | loss  0.12 | cls  0.12 | err  0.03 | \n",
      "scGPT - INFO - | epoch  19 | 1600/2309 batches | lr 0.0000 | ms/batch 91.28 | loss  0.13 | cls  0.13 | err  0.03 | \n",
      "scGPT - INFO - | epoch  19 | 1700/2309 batches | lr 0.0000 | ms/batch 91.37 | loss  0.08 | cls  0.08 | err  0.02 | \n",
      "scGPT - INFO - | epoch  19 | 1800/2309 batches | lr 0.0000 | ms/batch 91.42 | loss  0.12 | cls  0.12 | err  0.03 | \n",
      "scGPT - INFO - | epoch  19 | 1900/2309 batches | lr 0.0000 | ms/batch 91.41 | loss  0.09 | cls  0.09 | err  0.02 | \n",
      "scGPT - INFO - | epoch  19 | 2000/2309 batches | lr 0.0000 | ms/batch 91.54 | loss  0.12 | cls  0.12 | err  0.03 | \n",
      "scGPT - INFO - | epoch  19 | 2100/2309 batches | lr 0.0000 | ms/batch 91.36 | loss  0.12 | cls  0.12 | err  0.03 | \n",
      "scGPT - INFO - | epoch  19 | 2200/2309 batches | lr 0.0000 | ms/batch 90.73 | loss  0.09 | cls  0.09 | err  0.02 | \n",
      "scGPT - INFO - | epoch  19 | 2300/2309 batches | lr 0.0000 | ms/batch 91.20 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "valid/mse 0.11618096033304967 valid/err 0.02490525175961018 valid/dab 0.0 valid/sum_mse_dab 0.11618096033304967 epoch 19\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  19 | time: 220.03s | valid loss/mse 0.1162 | err 0.0249\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.1162\n",
      "random masking at epoch  20, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  20 | 100/2309 batches | lr 0.0000 | ms/batch 92.21 | loss  0.12 | cls  0.12 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 200/2309 batches | lr 0.0000 | ms/batch 86.85 | loss  0.14 | cls  0.14 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 300/2309 batches | lr 0.0000 | ms/batch 88.94 | loss  0.14 | cls  0.14 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 400/2309 batches | lr 0.0000 | ms/batch 89.47 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 500/2309 batches | lr 0.0000 | ms/batch 90.38 | loss  0.11 | cls  0.11 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 600/2309 batches | lr 0.0000 | ms/batch 90.67 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "scGPT - INFO - | epoch  20 | 700/2309 batches | lr 0.0000 | ms/batch 90.72 | loss  0.14 | cls  0.14 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 800/2309 batches | lr 0.0000 | ms/batch 91.02 | loss  0.15 | cls  0.15 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 900/2309 batches | lr 0.0000 | ms/batch 90.59 | loss  0.15 | cls  0.15 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 1000/2309 batches | lr 0.0000 | ms/batch 91.07 | loss  0.13 | cls  0.13 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 1100/2309 batches | lr 0.0000 | ms/batch 91.02 | loss  0.17 | cls  0.17 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 1200/2309 batches | lr 0.0000 | ms/batch 91.10 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 1300/2309 batches | lr 0.0000 | ms/batch 91.00 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 1400/2309 batches | lr 0.0000 | ms/batch 90.87 | loss  0.14 | cls  0.14 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 1500/2309 batches | lr 0.0000 | ms/batch 91.13 | loss  0.11 | cls  0.11 | err  0.02 | \n",
      "scGPT - INFO - | epoch  20 | 1600/2309 batches | lr 0.0000 | ms/batch 91.09 | loss  0.14 | cls  0.14 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 1700/2309 batches | lr 0.0000 | ms/batch 90.79 | loss  0.10 | cls  0.10 | err  0.02 | \n",
      "scGPT - INFO - | epoch  20 | 1800/2309 batches | lr 0.0000 | ms/batch 90.62 | loss  0.08 | cls  0.08 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 1900/2309 batches | lr 0.0000 | ms/batch 90.49 | loss  0.09 | cls  0.09 | err  0.02 | \n",
      "scGPT - INFO - | epoch  20 | 2000/2309 batches | lr 0.0000 | ms/batch 90.39 | loss  0.12 | cls  0.12 | err  0.02 | \n",
      "scGPT - INFO - | epoch  20 | 2100/2309 batches | lr 0.0000 | ms/batch 90.45 | loss  0.12 | cls  0.12 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 2200/2309 batches | lr 0.0000 | ms/batch 90.35 | loss  0.05 | cls  0.05 | err  0.01 | \n",
      "scGPT - INFO - | epoch  20 | 2300/2309 batches | lr 0.0000 | ms/batch 90.61 | loss  0.14 | cls  0.14 | err  0.03 | \n",
      "valid/mse 0.12491752659880799 valid/err 0.02598808879263671 valid/dab 0.0 valid/sum_mse_dab 0.12491752659880799 epoch 20\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  20 | time: 217.93s | valid loss/mse 0.1249 | err 0.0260\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.478433Z",
     "start_time": "2025-01-02T05:09:58.571604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(model: nn.Module, adata: DataLoader) -> float:\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "    text=get_text_prompt(adata.obs,config.select_features,tokenizer)\n",
    "    celltypes_labels = adata.obs[\"celltype_id\"].tolist()  # make sure count from 0\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=include_zero_gene,\n",
    "    )\n",
    "\n",
    "    input_values_test = random_mask_value(\n",
    "        tokenized_test[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": input_values_test,\n",
    "        \"target_values\": tokenized_test[\"values\"],\n",
    "        \"batch_labels\": torch.from_numpy(batch_ids).long(),\n",
    "        \"celltype_labels\": torch.from_numpy(celltypes_labels).long(),\n",
    "        \"text\":text[0],\n",
    "        \"mask\":text[1]\n",
    "    }\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=SeqDataset(test_data_pt),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    predictions = evaluate(\n",
    "        model,\n",
    "        loader=test_loader,\n",
    "        return_raw=True,\n",
    "    )\n",
    "\n",
    "    # compute accuracy, precision, recall, f1\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    accuracy = accuracy_score(celltypes_labels, predictions)\n",
    "    precision = precision_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    recall = recall_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    macro_f1 = f1_score(celltypes_labels, predictions, average=\"macro\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, \"\n",
    "        f\"Macro F1: {macro_f1:.3f}\"\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"test/accuracy\": accuracy,\n",
    "        \"test/precision\": precision,\n",
    "        \"test/recall\": recall,\n",
    "        \"test/macro_f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "    return predictions, celltypes_labels, results"
   ],
   "id": "3f1622aa0cf28246",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.478539Z",
     "start_time": "2025-01-02T05:09:58.759528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions, labels, results = test(best_model, adata_test)\n",
    "adata_test_raw.obs[\"predictions\"] = [id2type[p] for p in predictions]\n",
    "\n",
    "# plot\n",
    "palette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] \n",
    "palette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "palette_ = {c: palette_[i] for i, c in enumerate(celltypes)}\n",
    "\n",
    "with plt.rc_context({\"figure.figsize\": (6, 4), \"figure.dpi\": (300)}):\n",
    "    sc.pl.umap(\n",
    "        adata_test_raw,\n",
    "        color=[\"celltype\", \"predictions\"],\n",
    "        palette=palette_,\n",
    "        show=False,\n",
    "    )\n",
    "    plt.savefig(save_dir / \"results.png\", dpi=300)\n",
    "\n",
    "save_dict = {\n",
    "    \"predictions\": predictions,\n",
    "    \"labels\": labels,\n",
    "    \"results\": results,\n",
    "    \"id_maps\": id2type\n",
    "}\n",
    "with open(save_dir / \"results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(save_dict, f)\n",
    "\n",
    "# results[\"test/cell_umap\"] = wandb.Image(\n",
    "#     str(save_dir / \"results.png\"),\n",
    "#     caption=f\"predictions macro f1 {results['test/macro_f1']:.3f}\",\n",
    "# )\n",
    "# wandb.log(results)\n",
    "\n",
    "print(results)\n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ],
   "id": "a3e6453e0204ea5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid/mse 1.3044919742910073 valid/err 0.19800353803386403 valid/dab 0.0 valid/sum_mse_dab 1.3044919742910073 epoch 20\n",
      "scGPT - INFO - Accuracy: 0.802, Precision: 0.757, Recall: 0.751, Macro F1: 0.748\n",
      "{'test/accuracy': 0.8019964619661359, 'test/precision': 0.7565989808367196, 'test/recall': 0.751049874271392, 'test/macro_f1': 0.748103859942816}\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T02:05:42.478620Z",
     "start_time": "2025-01-02T05:10:39.531461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# celltypes = list(celltypes)\n",
    "# for i in set([id2type[p] for p in predictions]):\n",
    "#     if i not in celltypes:\n",
    "#         celltypes.remove(i)\n",
    "# cm = confusion_matrix(labels, predictions)\n",
    "# cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "# cm = pd.DataFrame(cm, index=celltypes[:cm.shape[0]], columns=celltypes[:cm.shape[1]])\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# sns.heatmap(cm, annot=True, fmt=\".1f\", cmap=\"Blues\")\n",
    "# plt.savefig(save_dir / \"confusion_matrix.png\", dpi=300)\n",
    "\n",
    "# results[\"test/confusion_matrix\"] = wandb.Image(\n",
    "#     str(save_dir / \"confusion_matrix.png\"),\n",
    "#     caption=f\"confusion matrix\",\n",
    "# )"
   ],
   "id": "561219cd9081cc37",
   "outputs": [],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
