{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:25.953342Z",
     "start_time": "2025-01-02T09:17:25.785694Z"
    }
   },
   "source": "! export CUDA_VISIBLE_DEVICES=2",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:25.980082Z",
     "start_time": "2025-01-02T09:17:25.974446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ],
   "id": "dfb3e50c0e9b64b7",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:26.163909Z",
     "start_time": "2025-01-02T09:17:26.149079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import pickle\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "# import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "id": "2d25becdaf951753",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:26.201421Z",
     "start_time": "2025-01-02T09:17:26.193832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class config:\n",
    "    seed=0\n",
    "    dataset_name=\"immune\"\n",
    "    do_train=True\n",
    "    load_model=\"/home/xh/memVP/scgpt/checkpoint/scgpt-human\"\n",
    "    mask_ratio=0.0\n",
    "    epochs=20\n",
    "    n_bins=51\n",
    "    MVC=False # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0 # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0\n",
    "    lr=1e-4\n",
    "    batch_size=8\n",
    "    layer_size=128\n",
    "    nlayers=4  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=4  # number of heads in nn.MultiheadAttention\n",
    "    dropout=0.2  # dropout probability\n",
    "    schedule_ratio=0.9  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5\n",
    "    fast_transformer=True\n",
    "    pre_norm=False\n",
    "    amp=True  # Automatic Mixed Precision\n",
    "    include_zero_gene = False\n",
    "    freeze = False #freeze\n",
    "    DSBN = False # Domain-spec batchnorm\n"
   ],
   "id": "e50dc997067495b7",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:26.268468Z",
     "start_time": "2025-01-02T09:17:26.264693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# # 初始化wandb\n",
    "# run = wandb.init(\n",
    "#     config=hyperparameter_defaults,\n",
    "#     project=\"memVP\",\n",
    "#     reinit=True,\n",
    "# )\n",
    "# \n",
    "# # 获取配置\n",
    "# config = wandb.config\n",
    "# # print(config)\n",
    "# # \n",
    "# # \n",
    "# set_seed(config.seed)"
   ],
   "id": "4b3a42701f3d1729",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:26.348975Z",
     "start_time": "2025-01-02T09:17:26.335165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "include_zero_gene = config.include_zero_gene  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 3001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "\n",
    "# settings for training\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = True  # celltype classification objective\n",
    "ADV = False  # Adversarial training for batch correction\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = config.MVC  # Masked value prediction for cell embedding\n",
    "print(config.ecs_thres)\n",
    "ECS = config.ecs_thres > 0  # Elastic cell similarity objective\n",
    "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "mvc_decoder_style = \"inner product\"\n",
    "\n",
    "ecs_threshold = config.ecs_thres\n",
    "dab_weight = config.dab_weight\n",
    "\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for optimizer\n",
    "lr = config.lr  # TODO: test learning rate ratio between two tasks\n",
    "lr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "schedule_interval = 1\n",
    "\n",
    "# settings for the model\n",
    "fast_transformer = config.fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability\n",
    "\n",
    "# logging\n",
    "log_interval = 100  # iterations\n",
    "save_eval_interval = config.save_eval_interval  # epochs\n",
    "do_eval_scib_metrics = True"
   ],
   "id": "54797c03154b67fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:26.420897Z",
     "start_time": "2025-01-02T09:17:26.412931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False"
   ],
   "id": "6d31e01d372ad09c",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:26.493100Z",
     "start_time": "2025-01-02T09:17:26.485943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ],
   "id": "e6c6e3a061f168ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/dev_immune-Jan02-17-17\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:27.493658Z",
     "start_time": "2025-01-02T09:17:26.558076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if dataset_name == \"ms\":\n",
    "    data_dir = Path(\"../data/ms\")\n",
    "    adata = sc.read(data_dir / \"c_data.h5ad\")\n",
    "    adata_test = sc.read(data_dir / \"filtered_ms_adata.h5ad\")\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "elif dataset_name == \"covid\":        \n",
    "    data_dir = Path(\"/home/xh/data/covid-19\")\n",
    "    adata = sc.read(data_dir / \"filtered_covid_portal_train_20k.h5ad\")\n",
    "    print(adata.var)\n",
    "    adata_test = sc.read(data_dir / \"filtered_covid_portal_test_10k.h5ad\")\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"full_clustering\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"full_clustering\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    # adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    # adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "elif dataset_name == \"pbmc\":\n",
    "    data_dir = Path(\"/home/xh/data/pkmc-160k\")\n",
    "    adata = sc.read(data_dir / \"pbmc_gene.h5ad\")\n",
    "    train_bool = [x in ['P1', 'P3', 'P4', 'P7'] for x in adata.obs['donor']]\n",
    "    adata_test = adata[np.invert(train_bool)]  # 直接过滤adata，保留不在训练集中的数据\n",
    "    adata = adata[train_bool]\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"celltype.l3\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"celltype.l3\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    # adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    # adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "if  dataset_name == \"heart\":\n",
    "    data_dir = Path(\"/home/xh/data/others/\")\n",
    "    adata = sc.read(data_dir / \"train_heart_data.h5ad\")\n",
    "    adata_test = sc.read(data_dir / \"test_heart_data.h5ad\")  # 直接过滤adata，保留不在训练集中的数据\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"cell_type_leiden0.6\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"cell_type_leiden0.6\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    # adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    # adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "elif dataset_name == \"immune\":\n",
    "    data_dir = Path(\"/home/xh/data/immune\")\n",
    "    adata = sc.read(data_dir / \"train_data.h5ad\")\n",
    "    adata_test = sc.read(data_dir / \"test_data.h5ad\") \n",
    "    adata.obs[\"celltype\"] = adata.obs[\"Manually_curated_celltype\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"Manually_curated_celltype\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = adata.obs[\"organ__ontology_label\"]\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] =  adata.obs[\"organ__ontology_label\"]       \n",
    "    # adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    # adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "# make the batch category column\n",
    "batch_id_labels = adata.obs[\"batch_id\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "celltype_id_labels = adata.obs[\"celltype\"].astype(\"category\").cat.codes.values\n",
    "celltypes = adata.obs[\"celltype\"].unique()\n",
    "num_types = len(np.unique(celltype_id_labels))\n",
    "id2type = dict(enumerate(adata.obs[\"celltype\"].astype(\"category\").cat.categories))\n",
    "adata.obs[\"celltype_id\"] = celltype_id_labels\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ],
   "id": "add90adfe7b87555",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:27.925627Z",
     "start_time": "2025-01-02T09:17:27.717723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]"
   ],
   "id": "92cb3efd9b813377",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 24285/36601 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from /home/xh/memVP/scgpt/checkpoint/scgpt-human/best_model.pt, the model args will override the config /home/xh/memVP/scgpt/checkpoint/scgpt-human/args.json.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:49.478597Z",
     "start_time": "2025-01-02T09:17:27.972948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "\n",
    "adata_test = adata[adata.obs[\"str_batch\"] == \"1\"]\n",
    "adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n",
    "preprocessor(adata_test, batch_key=None)"
   ],
   "id": "f77ab1531d7e9e6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:50.601545Z",
     "start_time": "2025-01-02T09:17:49.634089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata.obs[\"celltype_id\"].tolist()  # make sure count from 0\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, celltypes_labels,batch_ids, test_size=0.1, shuffle=True\n",
    ")\n",
    "train_data=all_counts\n",
    "train_celltype_labels=celltypes_labels\n",
    "train_batch_labels=batch_ids"
   ],
   "id": "5172d880843e5b1a",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:50.759090Z",
     "start_time": "2025-01-02T09:17:50.751947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ],
   "id": "540dbf2b7289df81",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:54.652570Z",
     "start_time": "2025-01-02T09:17:50.793658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")"
   ],
   "id": "f3e688e78d1498bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 18466, \n",
      "\t feature length: 3001\n",
      "scGPT - INFO - valid set number of samples: 1847, \n",
      "\t feature length: 3001\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:54.699932Z",
     "start_time": "2025-01-02T09:17:54.689319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    tensor_celltype_labels_train = torch.from_numpy(train_celltype_labels).long()\n",
    "    tensor_celltype_labels_valid = torch.from_numpy(valid_celltype_labels).long()\n",
    "\n",
    "    if sort_seq_batch:  # TODO: update to random pick seq source in each traning batch\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "        tensor_celltype_labels_train = tensor_celltype_labels_train[train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "        tensor_celltype_labels_valid = tensor_celltype_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "        \"celltype_labels\": tensor_celltype_labels_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "        \"celltype_labels\": tensor_celltype_labels_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n"
   ],
   "id": "5aba0c7b5ba0935f",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:55.573781Z",
     "start_time": "2025-01-02T09:17:54.789143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=num_types if CLS else 1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=MVC,\n",
    "    do_dab=DAB,\n",
    "    use_batch_labels=INPUT_BATCH_LABELS,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=config.DSBN,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    mvc_decoder_style=mvc_decoder_style,\n",
    "    ecs_threshold=ecs_threshold,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "# Freeze all pre-decoder weights\n",
    "for name, para in model.named_parameters():\n",
    "    print(\"-\"*20)\n",
    "    print(f\"name: {name}\")\n",
    "    if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name:\n",
    "    # if config.freeze and \"encoder\" in name:\n",
    "        print(f\"freezing weights for: {name}\")\n",
    "        para.requires_grad = False\n",
    "\n",
    "post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "logger.info(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "logger.info(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "# wandb.log(\n",
    "#         {\n",
    "#             \"info/pre_freeze_param_count\": pre_freeze_param_count,\n",
    "#             \"info/post_freeze_param_count\": post_freeze_param_count,\n",
    "#         },\n",
    "# )\n",
    "\n",
    "model.to(device)\n",
    "# wandb.watch(model)\n",
    "\n",
    "if ADV:\n",
    "    discriminator = AdversarialDiscriminator(\n",
    "        d_model=embsize,\n",
    "        n_cls=num_batch_types,\n",
    "    ).to(device)\n"
   ],
   "id": "ba76afba428a2b85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "--------------------\n",
      "name: encoder.embedding.weight\n",
      "--------------------\n",
      "name: encoder.enc_norm.weight\n",
      "--------------------\n",
      "name: encoder.enc_norm.bias\n",
      "--------------------\n",
      "name: value_encoder.linear1.weight\n",
      "--------------------\n",
      "name: value_encoder.linear1.bias\n",
      "--------------------\n",
      "name: value_encoder.linear2.weight\n",
      "--------------------\n",
      "name: value_encoder.linear2.bias\n",
      "--------------------\n",
      "name: value_encoder.norm.weight\n",
      "--------------------\n",
      "name: value_encoder.norm.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.out_proj.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm2.bias\n",
      "--------------------\n",
      "name: decoder.fc.0.weight\n",
      "--------------------\n",
      "name: decoder.fc.0.bias\n",
      "--------------------\n",
      "name: decoder.fc.2.weight\n",
      "--------------------\n",
      "name: decoder.fc.2.bias\n",
      "--------------------\n",
      "name: decoder.fc.4.weight\n",
      "--------------------\n",
      "name: decoder.fc.4.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.0.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.0.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.2.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.2.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.3.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.3.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.5.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.5.bias\n",
      "--------------------\n",
      "name: cls_decoder.out_layer.weight\n",
      "--------------------\n",
      "name: cls_decoder.out_layer.bias\n",
      "scGPT - INFO - Total Pre freeze Params 51354670\n",
      "scGPT - INFO - Total Post freeze Params 51354670\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:55.599951Z",
     "start_time": "2025-01-02T09:17:55.591078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, schedule_interval, gamma=config.schedule_ratio\n",
    ")\n",
    "if DAB_separate_optim:\n",
    "    optimizer_dab = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler_dab = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_dab, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "if ADV:\n",
    "    criterion_adv = nn.CrossEntropyLoss()  # consider using label smoothing\n",
    "    optimizer_E = torch.optim.Adam(model.parameters(), lr=lr_ADV)\n",
    "    scheduler_E = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_E, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_ADV)\n",
    "    scheduler_D = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_D, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ],
   "id": "c8c45aa718d06241",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:17:55.698235Z",
     "start_time": "2025-01-02T09:17:55.661992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    (\n",
    "        total_loss,\n",
    "        total_mse,\n",
    "        total_cls,\n",
    "        total_cce,\n",
    "        total_mvc,\n",
    "        total_ecs,\n",
    "        total_dab,\n",
    "        total_adv_E,\n",
    "        total_adv_D,\n",
    "        total_zero_log_prob,\n",
    "        total_mvc_zero_log_prob,\n",
    "    ) = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "    total_error = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "        celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = 0.0\n",
    "            metrics_to_log = {}\n",
    "            if MLM:\n",
    "                loss_mse = criterion(\n",
    "                    output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mse\n",
    "                metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            if explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            if CLS:\n",
    "                loss_cls = criterion_cls(output_dict[\"cls_output\"], celltype_labels)\n",
    "                loss = loss + loss_cls\n",
    "                metrics_to_log.update({\"train/cls\": loss_cls.item()})\n",
    "\n",
    "                error_rate = 1 - (\n",
    "                    (output_dict[\"cls_output\"].argmax(1) == celltype_labels)\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                ) / celltype_labels.size(0)\n",
    "            if CCE:\n",
    "                loss_cce = 10 * output_dict[\"loss_cce\"]\n",
    "                loss = loss + loss_cce\n",
    "                metrics_to_log.update({\"train/cce\": loss_cce.item()})\n",
    "            if MVC:\n",
    "                loss_mvc = criterion(\n",
    "                    output_dict[\"mvc_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mvc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_mvc.item()})\n",
    "            if MVC and explicit_zero_prob:\n",
    "                loss_mvc_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mvc_zero_log_prob\n",
    "                metrics_to_log.update({\"train/mvc_nzlp\": loss_mvc_zero_log_prob.item()})\n",
    "            if ECS:\n",
    "                loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "                loss = loss + loss_ecs\n",
    "                metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "            if DAB:\n",
    "                # try weighting and separate optimizer\n",
    "                loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "                loss = loss + dab_weight * loss_dab\n",
    "                metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if ADV:\n",
    "            # rerun the model for adversarial training\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            # TRAINING DISCRIMINATOR\n",
    "            loss_adv_D = criterion_adv(\n",
    "                discriminator(output_dict[\"cell_emb\"].detach()), batch_labels\n",
    "            )\n",
    "            if epoch > adv_D_delay_epochs:\n",
    "                discriminator.zero_grad()\n",
    "                loss_adv_D.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "            # TRAINING ENCODER\n",
    "            loss_adv_E = -criterion_adv(\n",
    "                discriminator(output_dict[\"cell_emb\"]), batch_labels\n",
    "            )\n",
    "            # NOTE: the loss is negative here because we want to maximize\n",
    "            # the cross_entropy_loss, in other words, disguise against the discriminator\n",
    "            if epoch > adv_E_delay_epochs:\n",
    "                model.zero_grad()\n",
    "                discriminator.zero_grad()\n",
    "                loss_adv_E.backward()\n",
    "                optimizer_E.step()\n",
    "\n",
    "        # wandb.log(metrics_to_log)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item() if MLM else 0.0\n",
    "        total_cls += loss_cls.item() if CLS else 0.0\n",
    "        total_cce += loss_cce.item() if CCE else 0.0\n",
    "        total_mvc += loss_mvc.item() if MVC else 0.0\n",
    "        total_ecs += loss_ecs.item() if ECS else 0.0\n",
    "        total_dab += loss_dab.item() if DAB else 0.0\n",
    "        total_adv_E += loss_adv_E.item() if ADV else 0.0\n",
    "        total_adv_D += loss_adv_D.item() if ADV else 0.0\n",
    "        total_zero_log_prob += loss_zero_log_prob.item() if explicit_zero_prob else 0.0\n",
    "        total_mvc_zero_log_prob += (\n",
    "            loss_mvc_zero_log_prob.item() if MVC and explicit_zero_prob else 0.0\n",
    "        )\n",
    "        total_error += error_rate\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_cls = total_cls / log_interval if CLS else 0.0\n",
    "            cur_cce = total_cce / log_interval if CCE else 0.0\n",
    "            cur_mvc = total_mvc / log_interval if MVC else 0.0\n",
    "            cur_ecs = total_ecs / log_interval if ECS else 0.0\n",
    "            cur_dab = total_dab / log_interval if DAB else 0.0\n",
    "            cur_adv_E = total_adv_E / log_interval if ADV else 0.0\n",
    "            cur_adv_D = total_adv_D / log_interval if ADV else 0.0\n",
    "            cur_zero_log_prob = (\n",
    "                total_zero_log_prob / log_interval if explicit_zero_prob else 0.0\n",
    "            )\n",
    "            cur_mvc_zero_log_prob = (\n",
    "                total_mvc_zero_log_prob / log_interval\n",
    "                if MVC and explicit_zero_prob\n",
    "                else 0.0\n",
    "            )\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | \"\n",
    "                + (f\"mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\" if MLM else \"\")\n",
    "                + (f\"cls {cur_cls:5.2f} | \" if CLS else \"\")\n",
    "                + (f\"err {cur_error:5.2f} | \" if CLS else \"\")\n",
    "                + (f\"cce {cur_cce:5.2f} |\" if CCE else \"\")\n",
    "                + (f\"mvc {cur_mvc:5.2f} |\" if MVC else \"\")\n",
    "                + (f\"ecs {cur_ecs:5.2f} |\" if ECS else \"\")\n",
    "                + (f\"dab {cur_dab:5.2f} |\" if DAB else \"\")\n",
    "                + (f\"adv_E {cur_adv_E:5.2f} |\" if ADV else \"\")\n",
    "                + (f\"adv_D {cur_adv_D:5.2f} |\" if ADV else \"\")\n",
    "                + (f\"nzlp {cur_zero_log_prob:5.2f} |\" if explicit_zero_prob else \"\")\n",
    "                + (\n",
    "                    f\"mvc_nzlp {cur_mvc_zero_log_prob:5.2f} |\"\n",
    "                    if MVC and explicit_zero_prob\n",
    "                    else \"\"\n",
    "                )\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_cls = 0\n",
    "            total_cce = 0\n",
    "            total_mvc = 0\n",
    "            total_ecs = 0\n",
    "            total_dab = 0\n",
    "            total_adv_E = 0\n",
    "            total_adv_D = 0\n",
    "            total_zero_log_prob = 0\n",
    "            total_mvc_zero_log_prob = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def define_wandb_metrcis():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"test/avg_bio\", summary=\"max\")\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "            celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                    CLS=CLS,  # evaluation does not need CLS or CCE\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=do_sample_in_train,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "                output_values = output_dict[\"cls_output\"]\n",
    "                loss = criterion_cls(output_values, celltype_labels)\n",
    "\n",
    "                if DAB:\n",
    "                    loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            accuracy = (output_values.argmax(1) == celltype_labels).sum().item()\n",
    "            total_error += (1 - accuracy / len(input_gene_ids)) * len(input_gene_ids)\n",
    "            total_dab += loss_dab.item() * len(input_gene_ids) if DAB else 0.0\n",
    "            total_num += len(input_gene_ids)\n",
    "            preds = output_values.argmax(1).cpu().numpy()\n",
    "            predictions.append(preds)\n",
    "\n",
    "    # wandb.log(\n",
    "    #     {\n",
    "    #         \"valid/mse\": total_loss / total_num,\n",
    "    #         \"valid/err\": total_error / total_num,\n",
    "    #         \"valid/dab\": total_dab / total_num,\n",
    "    #         \"valid/sum_mse_dab\": (total_loss + dab_weight * total_dab) / total_num,\n",
    "    #         \"epoch\": epoch,\n",
    "    #     },\n",
    "    # )\n",
    "    print(\"valid/mse\",total_loss / total_num,\n",
    "            \"valid/err\", total_error / total_num,\n",
    "            \"valid/dab\", total_dab / total_num,\n",
    "            \"valid/sum_mse_dab\", (total_loss + dab_weight * total_dab) / total_num,\n",
    "            \"epoch\", epoch)\n",
    "\n",
    "    if return_raw:\n",
    "        return np.concatenate(predictions, axis=0)\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num"
   ],
   "id": "2e21092d13c4a8e9",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T12:14:23.002231Z",
     "start_time": "2025-01-02T09:17:55.737464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "# define_wandb_metrcis()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model,\n",
    "            loader=train_loader,\n",
    "        )\n",
    "    val_loss, val_err = evaluate(\n",
    "        model,\n",
    "        loader=valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | err {val_err:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    if DAB_separate_optim:\n",
    "        scheduler_dab.step()\n",
    "    if ADV:\n",
    "        scheduler_D.step()\n",
    "        scheduler_E.step()"
   ],
   "id": "d89e6508675fc40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   1, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   1 | 100/2309 batches | lr 0.0001 | ms/batch 227.57 | loss  3.41 | cls  3.41 | err  0.94 | \n",
      "scGPT - INFO - | epoch   1 | 200/2309 batches | lr 0.0001 | ms/batch 222.54 | loss  3.24 | cls  3.24 | err  0.92 | \n",
      "scGPT - INFO - | epoch   1 | 300/2309 batches | lr 0.0001 | ms/batch 222.68 | loss  3.21 | cls  3.21 | err  0.91 | \n",
      "scGPT - INFO - | epoch   1 | 400/2309 batches | lr 0.0001 | ms/batch 222.74 | loss  3.18 | cls  3.18 | err  0.89 | \n",
      "scGPT - INFO - | epoch   1 | 500/2309 batches | lr 0.0001 | ms/batch 222.68 | loss  3.25 | cls  3.25 | err  0.89 | \n",
      "scGPT - INFO - | epoch   1 | 600/2309 batches | lr 0.0001 | ms/batch 222.85 | loss  3.26 | cls  3.26 | err  0.89 | \n",
      "scGPT - INFO - | epoch   1 | 700/2309 batches | lr 0.0001 | ms/batch 222.85 | loss  3.20 | cls  3.20 | err  0.90 | \n",
      "scGPT - INFO - | epoch   1 | 800/2309 batches | lr 0.0001 | ms/batch 222.87 | loss  3.19 | cls  3.19 | err  0.92 | \n",
      "scGPT - INFO - | epoch   1 | 900/2309 batches | lr 0.0001 | ms/batch 222.90 | loss  3.21 | cls  3.21 | err  0.92 | \n",
      "scGPT - INFO - | epoch   1 | 1000/2309 batches | lr 0.0001 | ms/batch 222.85 | loss  3.21 | cls  3.21 | err  0.91 | \n",
      "scGPT - INFO - | epoch   1 | 1100/2309 batches | lr 0.0001 | ms/batch 222.82 | loss  3.14 | cls  3.14 | err  0.88 | \n",
      "scGPT - INFO - | epoch   1 | 1200/2309 batches | lr 0.0001 | ms/batch 222.89 | loss  3.21 | cls  3.21 | err  0.89 | \n",
      "scGPT - INFO - | epoch   1 | 1300/2309 batches | lr 0.0001 | ms/batch 222.95 | loss  3.16 | cls  3.16 | err  0.90 | \n",
      "scGPT - INFO - | epoch   1 | 1400/2309 batches | lr 0.0001 | ms/batch 222.91 | loss  3.19 | cls  3.19 | err  0.91 | \n",
      "scGPT - INFO - | epoch   1 | 1500/2309 batches | lr 0.0001 | ms/batch 222.87 | loss  3.14 | cls  3.14 | err  0.91 | \n",
      "scGPT - INFO - | epoch   1 | 1600/2309 batches | lr 0.0001 | ms/batch 222.70 | loss  3.18 | cls  3.18 | err  0.88 | \n",
      "scGPT - INFO - | epoch   1 | 1700/2309 batches | lr 0.0001 | ms/batch 222.16 | loss  3.21 | cls  3.21 | err  0.89 | \n",
      "scGPT - INFO - | epoch   1 | 1800/2309 batches | lr 0.0001 | ms/batch 222.60 | loss  3.19 | cls  3.19 | err  0.89 | \n",
      "scGPT - INFO - | epoch   1 | 1900/2309 batches | lr 0.0001 | ms/batch 222.18 | loss  3.23 | cls  3.23 | err  0.91 | \n",
      "scGPT - INFO - | epoch   1 | 2000/2309 batches | lr 0.0001 | ms/batch 222.13 | loss  3.19 | cls  3.19 | err  0.91 | \n",
      "scGPT - INFO - | epoch   1 | 2100/2309 batches | lr 0.0001 | ms/batch 222.15 | loss  3.17 | cls  3.17 | err  0.91 | \n",
      "scGPT - INFO - | epoch   1 | 2200/2309 batches | lr 0.0001 | ms/batch 222.71 | loss  3.21 | cls  3.21 | err  0.90 | \n",
      "scGPT - INFO - | epoch   1 | 2300/2309 batches | lr 0.0001 | ms/batch 222.86 | loss  3.08 | cls  3.08 | err  0.86 | \n",
      "valid/mse 2.77627518852143 valid/err 0.8597726042230645 valid/dab 0.0 valid/sum_mse_dab 2.77627518852143 epoch 1\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 528.69s | valid loss/mse 2.7763 | err 0.8598\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 2.7763\n",
      "random masking at epoch   2, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   2 | 100/2309 batches | lr 0.0001 | ms/batch 228.17 | loss  2.72 | cls  2.72 | err  0.82 | \n",
      "scGPT - INFO - | epoch   2 | 200/2309 batches | lr 0.0001 | ms/batch 222.91 | loss  2.50 | cls  2.50 | err  0.76 | \n",
      "scGPT - INFO - | epoch   2 | 300/2309 batches | lr 0.0001 | ms/batch 222.85 | loss  2.31 | cls  2.31 | err  0.71 | \n",
      "scGPT - INFO - | epoch   2 | 400/2309 batches | lr 0.0001 | ms/batch 223.00 | loss  2.18 | cls  2.18 | err  0.69 | \n",
      "scGPT - INFO - | epoch   2 | 500/2309 batches | lr 0.0001 | ms/batch 222.95 | loss  2.17 | cls  2.17 | err  0.65 | \n",
      "scGPT - INFO - | epoch   2 | 600/2309 batches | lr 0.0001 | ms/batch 222.22 | loss  2.01 | cls  2.01 | err  0.62 | \n",
      "scGPT - INFO - | epoch   2 | 700/2309 batches | lr 0.0001 | ms/batch 222.41 | loss  1.91 | cls  1.91 | err  0.61 | \n",
      "scGPT - INFO - | epoch   2 | 800/2309 batches | lr 0.0001 | ms/batch 223.06 | loss  1.89 | cls  1.89 | err  0.63 | \n",
      "scGPT - INFO - | epoch   2 | 900/2309 batches | lr 0.0001 | ms/batch 223.00 | loss  1.90 | cls  1.90 | err  0.62 | \n",
      "scGPT - INFO - | epoch   2 | 1000/2309 batches | lr 0.0001 | ms/batch 222.79 | loss  1.84 | cls  1.84 | err  0.58 | \n",
      "scGPT - INFO - | epoch   2 | 1100/2309 batches | lr 0.0001 | ms/batch 222.78 | loss  1.81 | cls  1.81 | err  0.56 | \n",
      "scGPT - INFO - | epoch   2 | 1200/2309 batches | lr 0.0001 | ms/batch 222.80 | loss  1.87 | cls  1.87 | err  0.59 | \n",
      "scGPT - INFO - | epoch   2 | 1300/2309 batches | lr 0.0001 | ms/batch 222.90 | loss  1.77 | cls  1.77 | err  0.60 | \n",
      "scGPT - INFO - | epoch   2 | 1400/2309 batches | lr 0.0001 | ms/batch 222.89 | loss  1.77 | cls  1.77 | err  0.54 | \n",
      "scGPT - INFO - | epoch   2 | 1500/2309 batches | lr 0.0001 | ms/batch 222.91 | loss  1.67 | cls  1.67 | err  0.55 | \n",
      "scGPT - INFO - | epoch   2 | 1600/2309 batches | lr 0.0001 | ms/batch 222.95 | loss  1.66 | cls  1.66 | err  0.49 | \n",
      "scGPT - INFO - | epoch   2 | 1700/2309 batches | lr 0.0001 | ms/batch 223.00 | loss  1.59 | cls  1.59 | err  0.50 | \n",
      "scGPT - INFO - | epoch   2 | 1800/2309 batches | lr 0.0001 | ms/batch 223.01 | loss  1.51 | cls  1.51 | err  0.47 | \n",
      "scGPT - INFO - | epoch   2 | 1900/2309 batches | lr 0.0001 | ms/batch 222.98 | loss  1.57 | cls  1.57 | err  0.51 | \n",
      "scGPT - INFO - | epoch   2 | 2000/2309 batches | lr 0.0001 | ms/batch 222.92 | loss  1.50 | cls  1.50 | err  0.47 | \n",
      "scGPT - INFO - | epoch   2 | 2100/2309 batches | lr 0.0001 | ms/batch 222.95 | loss  1.46 | cls  1.46 | err  0.46 | \n",
      "scGPT - INFO - | epoch   2 | 2200/2309 batches | lr 0.0001 | ms/batch 222.94 | loss  1.38 | cls  1.38 | err  0.43 | \n",
      "scGPT - INFO - | epoch   2 | 2300/2309 batches | lr 0.0001 | ms/batch 223.00 | loss  1.41 | cls  1.41 | err  0.43 | \n",
      "valid/mse 1.5431403223733742 valid/err 0.5175961017866811 valid/dab 0.0 valid/sum_mse_dab 1.5431403223733742 epoch 2\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 529.16s | valid loss/mse 1.5431 | err 0.5176\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 1.5431\n",
      "random masking at epoch   3, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   3 | 100/2309 batches | lr 0.0001 | ms/batch 228.16 | loss  1.43 | cls  1.43 | err  0.43 | \n",
      "scGPT - INFO - | epoch   3 | 200/2309 batches | lr 0.0001 | ms/batch 223.21 | loss  1.30 | cls  1.30 | err  0.42 | \n",
      "scGPT - INFO - | epoch   3 | 300/2309 batches | lr 0.0001 | ms/batch 223.20 | loss  1.33 | cls  1.33 | err  0.42 | \n",
      "scGPT - INFO - | epoch   3 | 400/2309 batches | lr 0.0001 | ms/batch 223.28 | loss  1.18 | cls  1.18 | err  0.36 | \n",
      "scGPT - INFO - | epoch   3 | 500/2309 batches | lr 0.0001 | ms/batch 223.11 | loss  1.21 | cls  1.21 | err  0.35 | \n",
      "scGPT - INFO - | epoch   3 | 600/2309 batches | lr 0.0001 | ms/batch 223.07 | loss  1.22 | cls  1.22 | err  0.38 | \n",
      "scGPT - INFO - | epoch   3 | 700/2309 batches | lr 0.0001 | ms/batch 223.15 | loss  1.23 | cls  1.23 | err  0.36 | \n",
      "scGPT - INFO - | epoch   3 | 800/2309 batches | lr 0.0001 | ms/batch 223.09 | loss  1.19 | cls  1.19 | err  0.36 | \n",
      "scGPT - INFO - | epoch   3 | 900/2309 batches | lr 0.0001 | ms/batch 223.07 | loss  1.23 | cls  1.23 | err  0.36 | \n",
      "scGPT - INFO - | epoch   3 | 1000/2309 batches | lr 0.0001 | ms/batch 223.13 | loss  1.16 | cls  1.16 | err  0.34 | \n",
      "scGPT - INFO - | epoch   3 | 1100/2309 batches | lr 0.0001 | ms/batch 223.16 | loss  1.09 | cls  1.09 | err  0.32 | \n",
      "scGPT - INFO - | epoch   3 | 1200/2309 batches | lr 0.0001 | ms/batch 223.16 | loss  1.25 | cls  1.25 | err  0.38 | \n",
      "scGPT - INFO - | epoch   3 | 1300/2309 batches | lr 0.0001 | ms/batch 223.12 | loss  1.15 | cls  1.15 | err  0.35 | \n",
      "scGPT - INFO - | epoch   3 | 1400/2309 batches | lr 0.0001 | ms/batch 223.13 | loss  1.10 | cls  1.10 | err  0.33 | \n",
      "scGPT - INFO - | epoch   3 | 1500/2309 batches | lr 0.0001 | ms/batch 223.11 | loss  1.02 | cls  1.02 | err  0.30 | \n",
      "scGPT - INFO - | epoch   3 | 1600/2309 batches | lr 0.0001 | ms/batch 223.15 | loss  1.11 | cls  1.11 | err  0.33 | \n",
      "scGPT - INFO - | epoch   3 | 1700/2309 batches | lr 0.0001 | ms/batch 223.11 | loss  1.06 | cls  1.06 | err  0.30 | \n",
      "scGPT - INFO - | epoch   3 | 1800/2309 batches | lr 0.0001 | ms/batch 223.20 | loss  1.05 | cls  1.05 | err  0.30 | \n",
      "scGPT - INFO - | epoch   3 | 1900/2309 batches | lr 0.0001 | ms/batch 223.13 | loss  1.13 | cls  1.13 | err  0.34 | \n",
      "scGPT - INFO - | epoch   3 | 2000/2309 batches | lr 0.0001 | ms/batch 223.10 | loss  1.04 | cls  1.04 | err  0.31 | \n",
      "scGPT - INFO - | epoch   3 | 2100/2309 batches | lr 0.0001 | ms/batch 223.17 | loss  1.00 | cls  1.00 | err  0.30 | \n",
      "scGPT - INFO - | epoch   3 | 2200/2309 batches | lr 0.0001 | ms/batch 223.19 | loss  0.97 | cls  0.97 | err  0.28 | \n",
      "scGPT - INFO - | epoch   3 | 2300/2309 batches | lr 0.0001 | ms/batch 223.17 | loss  1.04 | cls  1.04 | err  0.28 | \n",
      "valid/mse 1.1254592505931595 valid/err 0.3670817541959935 valid/dab 0.0 valid/sum_mse_dab 1.1254592505931595 epoch 3\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 529.76s | valid loss/mse 1.1255 | err 0.3671\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 1.1255\n",
      "random masking at epoch   4, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   4 | 100/2309 batches | lr 0.0001 | ms/batch 227.99 | loss  1.04 | cls  1.04 | err  0.30 | \n",
      "scGPT - INFO - | epoch   4 | 200/2309 batches | lr 0.0001 | ms/batch 222.97 | loss  0.98 | cls  0.98 | err  0.30 | \n",
      "scGPT - INFO - | epoch   4 | 300/2309 batches | lr 0.0001 | ms/batch 222.93 | loss  0.99 | cls  0.99 | err  0.29 | \n",
      "scGPT - INFO - | epoch   4 | 400/2309 batches | lr 0.0001 | ms/batch 223.02 | loss  0.93 | cls  0.93 | err  0.27 | \n",
      "scGPT - INFO - | epoch   4 | 500/2309 batches | lr 0.0001 | ms/batch 222.99 | loss  0.90 | cls  0.90 | err  0.26 | \n",
      "scGPT - INFO - | epoch   4 | 600/2309 batches | lr 0.0001 | ms/batch 222.96 | loss  1.01 | cls  1.01 | err  0.32 | \n",
      "scGPT - INFO - | epoch   4 | 700/2309 batches | lr 0.0001 | ms/batch 222.90 | loss  0.96 | cls  0.96 | err  0.28 | \n",
      "scGPT - INFO - | epoch   4 | 800/2309 batches | lr 0.0001 | ms/batch 222.89 | loss  0.94 | cls  0.94 | err  0.29 | \n",
      "scGPT - INFO - | epoch   4 | 900/2309 batches | lr 0.0001 | ms/batch 222.75 | loss  0.99 | cls  0.99 | err  0.28 | \n",
      "scGPT - INFO - | epoch   4 | 1000/2309 batches | lr 0.0001 | ms/batch 222.76 | loss  0.92 | cls  0.92 | err  0.26 | \n",
      "scGPT - INFO - | epoch   4 | 1100/2309 batches | lr 0.0001 | ms/batch 222.86 | loss  0.86 | cls  0.86 | err  0.25 | \n",
      "scGPT - INFO - | epoch   4 | 1200/2309 batches | lr 0.0001 | ms/batch 222.76 | loss  0.97 | cls  0.97 | err  0.28 | \n",
      "scGPT - INFO - | epoch   4 | 1300/2309 batches | lr 0.0001 | ms/batch 222.92 | loss  0.96 | cls  0.96 | err  0.29 | \n",
      "scGPT - INFO - | epoch   4 | 1400/2309 batches | lr 0.0001 | ms/batch 222.81 | loss  0.87 | cls  0.87 | err  0.27 | \n",
      "scGPT - INFO - | epoch   4 | 1500/2309 batches | lr 0.0001 | ms/batch 222.92 | loss  0.82 | cls  0.82 | err  0.25 | \n",
      "scGPT - INFO - | epoch   4 | 1600/2309 batches | lr 0.0001 | ms/batch 222.87 | loss  0.86 | cls  0.86 | err  0.24 | \n",
      "scGPT - INFO - | epoch   4 | 1700/2309 batches | lr 0.0001 | ms/batch 222.97 | loss  0.84 | cls  0.84 | err  0.25 | \n",
      "scGPT - INFO - | epoch   4 | 1800/2309 batches | lr 0.0001 | ms/batch 222.84 | loss  0.87 | cls  0.87 | err  0.26 | \n",
      "scGPT - INFO - | epoch   4 | 1900/2309 batches | lr 0.0001 | ms/batch 222.96 | loss  0.87 | cls  0.87 | err  0.28 | \n",
      "scGPT - INFO - | epoch   4 | 2000/2309 batches | lr 0.0001 | ms/batch 223.00 | loss  0.85 | cls  0.85 | err  0.25 | \n",
      "scGPT - INFO - | epoch   4 | 2100/2309 batches | lr 0.0001 | ms/batch 222.97 | loss  0.82 | cls  0.82 | err  0.26 | \n",
      "scGPT - INFO - | epoch   4 | 2200/2309 batches | lr 0.0001 | ms/batch 222.98 | loss  0.79 | cls  0.79 | err  0.25 | \n",
      "scGPT - INFO - | epoch   4 | 2300/2309 batches | lr 0.0001 | ms/batch 222.99 | loss  0.88 | cls  0.88 | err  0.26 | \n",
      "valid/mse 1.0084889510159372 valid/err 0.32647536545749867 valid/dab 0.0 valid/sum_mse_dab 1.0084889510159372 epoch 4\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 529.24s | valid loss/mse 1.0085 | err 0.3265\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 1.0085\n",
      "random masking at epoch   5, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   5 | 100/2309 batches | lr 0.0001 | ms/batch 227.80 | loss  0.87 | cls  0.87 | err  0.26 | \n",
      "scGPT - INFO - | epoch   5 | 200/2309 batches | lr 0.0001 | ms/batch 223.20 | loss  0.79 | cls  0.79 | err  0.24 | \n",
      "scGPT - INFO - | epoch   5 | 300/2309 batches | lr 0.0001 | ms/batch 223.19 | loss  0.82 | cls  0.82 | err  0.23 | \n",
      "scGPT - INFO - | epoch   5 | 400/2309 batches | lr 0.0001 | ms/batch 223.18 | loss  0.78 | cls  0.78 | err  0.23 | \n",
      "scGPT - INFO - | epoch   5 | 500/2309 batches | lr 0.0001 | ms/batch 223.15 | loss  0.74 | cls  0.74 | err  0.22 | \n",
      "scGPT - INFO - | epoch   5 | 600/2309 batches | lr 0.0001 | ms/batch 223.09 | loss  0.84 | cls  0.84 | err  0.25 | \n",
      "scGPT - INFO - | epoch   5 | 700/2309 batches | lr 0.0001 | ms/batch 223.12 | loss  0.85 | cls  0.85 | err  0.23 | \n",
      "scGPT - INFO - | epoch   5 | 800/2309 batches | lr 0.0001 | ms/batch 223.04 | loss  0.82 | cls  0.82 | err  0.24 | \n",
      "scGPT - INFO - | epoch   5 | 900/2309 batches | lr 0.0001 | ms/batch 223.13 | loss  0.81 | cls  0.81 | err  0.23 | \n",
      "scGPT - INFO - | epoch   5 | 1000/2309 batches | lr 0.0001 | ms/batch 223.11 | loss  0.81 | cls  0.81 | err  0.23 | \n",
      "scGPT - INFO - | epoch   5 | 1100/2309 batches | lr 0.0001 | ms/batch 223.10 | loss  0.72 | cls  0.72 | err  0.21 | \n",
      "scGPT - INFO - | epoch   5 | 1200/2309 batches | lr 0.0001 | ms/batch 223.04 | loss  0.84 | cls  0.84 | err  0.24 | \n",
      "scGPT - INFO - | epoch   5 | 1300/2309 batches | lr 0.0001 | ms/batch 223.11 | loss  0.82 | cls  0.82 | err  0.24 | \n",
      "scGPT - INFO - | epoch   5 | 1400/2309 batches | lr 0.0001 | ms/batch 223.13 | loss  0.73 | cls  0.73 | err  0.21 | \n",
      "scGPT - INFO - | epoch   5 | 1500/2309 batches | lr 0.0001 | ms/batch 223.11 | loss  0.67 | cls  0.67 | err  0.21 | \n",
      "scGPT - INFO - | epoch   5 | 1600/2309 batches | lr 0.0001 | ms/batch 223.12 | loss  0.73 | cls  0.73 | err  0.20 | \n",
      "scGPT - INFO - | epoch   5 | 1700/2309 batches | lr 0.0001 | ms/batch 223.09 | loss  0.74 | cls  0.74 | err  0.22 | \n",
      "scGPT - INFO - | epoch   5 | 1800/2309 batches | lr 0.0001 | ms/batch 222.99 | loss  0.73 | cls  0.73 | err  0.22 | \n",
      "scGPT - INFO - | epoch   5 | 1900/2309 batches | lr 0.0001 | ms/batch 223.03 | loss  0.73 | cls  0.73 | err  0.21 | \n",
      "scGPT - INFO - | epoch   5 | 2000/2309 batches | lr 0.0001 | ms/batch 223.04 | loss  0.72 | cls  0.72 | err  0.21 | \n",
      "scGPT - INFO - | epoch   5 | 2100/2309 batches | lr 0.0001 | ms/batch 222.98 | loss  0.70 | cls  0.70 | err  0.22 | \n",
      "scGPT - INFO - | epoch   5 | 2200/2309 batches | lr 0.0001 | ms/batch 222.99 | loss  0.66 | cls  0.66 | err  0.19 | \n",
      "scGPT - INFO - | epoch   5 | 2300/2309 batches | lr 0.0001 | ms/batch 222.99 | loss  0.81 | cls  0.81 | err  0.24 | \n",
      "valid/mse 0.828459916853169 valid/err 0.2566323768272875 valid/dab 0.0 valid/sum_mse_dab 0.828459916853169 epoch 5\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 529.61s | valid loss/mse 0.8285 | err 0.2566\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.8285\n",
      "random masking at epoch   6, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   6 | 100/2309 batches | lr 0.0001 | ms/batch 227.88 | loss  0.78 | cls  0.78 | err  0.23 | \n",
      "scGPT - INFO - | epoch   6 | 200/2309 batches | lr 0.0001 | ms/batch 222.92 | loss  0.67 | cls  0.67 | err  0.21 | \n",
      "scGPT - INFO - | epoch   6 | 300/2309 batches | lr 0.0001 | ms/batch 222.93 | loss  0.74 | cls  0.74 | err  0.19 | \n",
      "scGPT - INFO - | epoch   6 | 400/2309 batches | lr 0.0001 | ms/batch 222.90 | loss  0.68 | cls  0.68 | err  0.19 | \n",
      "scGPT - INFO - | epoch   6 | 500/2309 batches | lr 0.0001 | ms/batch 222.69 | loss  0.64 | cls  0.64 | err  0.19 | \n",
      "scGPT - INFO - | epoch   6 | 600/2309 batches | lr 0.0001 | ms/batch 222.67 | loss  0.74 | cls  0.74 | err  0.22 | \n",
      "scGPT - INFO - | epoch   6 | 700/2309 batches | lr 0.0001 | ms/batch 222.84 | loss  0.77 | cls  0.77 | err  0.21 | \n",
      "scGPT - INFO - | epoch   6 | 800/2309 batches | lr 0.0001 | ms/batch 222.87 | loss  0.69 | cls  0.69 | err  0.20 | \n",
      "scGPT - INFO - | epoch   6 | 900/2309 batches | lr 0.0001 | ms/batch 222.99 | loss  0.71 | cls  0.71 | err  0.20 | \n",
      "scGPT - INFO - | epoch   6 | 1000/2309 batches | lr 0.0001 | ms/batch 222.84 | loss  0.76 | cls  0.76 | err  0.23 | \n",
      "scGPT - INFO - | epoch   6 | 1100/2309 batches | lr 0.0001 | ms/batch 222.84 | loss  0.63 | cls  0.63 | err  0.17 | \n",
      "scGPT - INFO - | epoch   6 | 1200/2309 batches | lr 0.0001 | ms/batch 222.86 | loss  0.73 | cls  0.73 | err  0.21 | \n",
      "scGPT - INFO - | epoch   6 | 1300/2309 batches | lr 0.0001 | ms/batch 223.02 | loss  0.73 | cls  0.73 | err  0.20 | \n",
      "scGPT - INFO - | epoch   6 | 1400/2309 batches | lr 0.0001 | ms/batch 222.89 | loss  0.67 | cls  0.67 | err  0.20 | \n",
      "scGPT - INFO - | epoch   6 | 1500/2309 batches | lr 0.0001 | ms/batch 222.98 | loss  0.60 | cls  0.60 | err  0.19 | \n",
      "scGPT - INFO - | epoch   6 | 1600/2309 batches | lr 0.0001 | ms/batch 223.02 | loss  0.67 | cls  0.67 | err  0.18 | \n",
      "scGPT - INFO - | epoch   6 | 1700/2309 batches | lr 0.0001 | ms/batch 222.94 | loss  0.66 | cls  0.66 | err  0.19 | \n",
      "scGPT - INFO - | epoch   6 | 1800/2309 batches | lr 0.0001 | ms/batch 222.87 | loss  0.65 | cls  0.65 | err  0.19 | \n",
      "scGPT - INFO - | epoch   6 | 1900/2309 batches | lr 0.0001 | ms/batch 222.91 | loss  0.63 | cls  0.63 | err  0.19 | \n",
      "scGPT - INFO - | epoch   6 | 2000/2309 batches | lr 0.0001 | ms/batch 222.94 | loss  0.66 | cls  0.66 | err  0.19 | \n",
      "scGPT - INFO - | epoch   6 | 2100/2309 batches | lr 0.0001 | ms/batch 222.88 | loss  0.60 | cls  0.60 | err  0.17 | \n",
      "scGPT - INFO - | epoch   6 | 2200/2309 batches | lr 0.0001 | ms/batch 222.88 | loss  0.62 | cls  0.62 | err  0.18 | \n",
      "scGPT - INFO - | epoch   6 | 2300/2309 batches | lr 0.0001 | ms/batch 222.86 | loss  0.68 | cls  0.68 | err  0.20 | \n",
      "valid/mse 0.7731960815517207 valid/err 0.22360584731997835 valid/dab 0.0 valid/sum_mse_dab 0.7731960815517207 epoch 6\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 529.24s | valid loss/mse 0.7732 | err 0.2236\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.7732\n",
      "random masking at epoch   7, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   7 | 100/2309 batches | lr 0.0001 | ms/batch 227.94 | loss  0.68 | cls  0.68 | err  0.21 | \n",
      "scGPT - INFO - | epoch   7 | 200/2309 batches | lr 0.0001 | ms/batch 222.99 | loss  0.60 | cls  0.60 | err  0.18 | \n",
      "scGPT - INFO - | epoch   7 | 300/2309 batches | lr 0.0001 | ms/batch 223.03 | loss  0.67 | cls  0.67 | err  0.19 | \n",
      "scGPT - INFO - | epoch   7 | 400/2309 batches | lr 0.0001 | ms/batch 222.98 | loss  0.62 | cls  0.62 | err  0.17 | \n",
      "scGPT - INFO - | epoch   7 | 500/2309 batches | lr 0.0001 | ms/batch 222.90 | loss  0.56 | cls  0.56 | err  0.17 | \n",
      "scGPT - INFO - | epoch   7 | 600/2309 batches | lr 0.0001 | ms/batch 222.92 | loss  0.66 | cls  0.66 | err  0.19 | \n",
      "scGPT - INFO - | epoch   7 | 700/2309 batches | lr 0.0001 | ms/batch 223.01 | loss  0.67 | cls  0.67 | err  0.18 | \n",
      "scGPT - INFO - | epoch   7 | 800/2309 batches | lr 0.0001 | ms/batch 223.03 | loss  0.62 | cls  0.62 | err  0.19 | \n",
      "scGPT - INFO - | epoch   7 | 900/2309 batches | lr 0.0001 | ms/batch 222.94 | loss  0.66 | cls  0.66 | err  0.18 | \n",
      "scGPT - INFO - | epoch   7 | 1000/2309 batches | lr 0.0001 | ms/batch 222.97 | loss  0.70 | cls  0.70 | err  0.19 | \n",
      "scGPT - INFO - | epoch   7 | 1100/2309 batches | lr 0.0001 | ms/batch 223.02 | loss  0.58 | cls  0.58 | err  0.16 | \n",
      "scGPT - INFO - | epoch   7 | 1200/2309 batches | lr 0.0001 | ms/batch 222.98 | loss  0.67 | cls  0.67 | err  0.19 | \n",
      "scGPT - INFO - | epoch   7 | 1300/2309 batches | lr 0.0001 | ms/batch 223.09 | loss  0.66 | cls  0.66 | err  0.19 | \n",
      "scGPT - INFO - | epoch   7 | 1400/2309 batches | lr 0.0001 | ms/batch 222.98 | loss  0.60 | cls  0.60 | err  0.17 | \n",
      "scGPT - INFO - | epoch   7 | 1500/2309 batches | lr 0.0001 | ms/batch 223.03 | loss  0.49 | cls  0.49 | err  0.13 | \n",
      "scGPT - INFO - | epoch   7 | 1600/2309 batches | lr 0.0001 | ms/batch 222.96 | loss  0.59 | cls  0.59 | err  0.15 | \n",
      "scGPT - INFO - | epoch   7 | 1700/2309 batches | lr 0.0001 | ms/batch 222.97 | loss  0.59 | cls  0.59 | err  0.17 | \n",
      "scGPT - INFO - | epoch   7 | 1800/2309 batches | lr 0.0001 | ms/batch 222.93 | loss  0.60 | cls  0.60 | err  0.17 | \n",
      "scGPT - INFO - | epoch   7 | 1900/2309 batches | lr 0.0001 | ms/batch 222.97 | loss  0.56 | cls  0.56 | err  0.15 | \n",
      "scGPT - INFO - | epoch   7 | 2000/2309 batches | lr 0.0001 | ms/batch 222.98 | loss  0.53 | cls  0.53 | err  0.16 | \n",
      "scGPT - INFO - | epoch   7 | 2100/2309 batches | lr 0.0001 | ms/batch 223.00 | loss  0.53 | cls  0.53 | err  0.15 | \n",
      "scGPT - INFO - | epoch   7 | 2200/2309 batches | lr 0.0001 | ms/batch 223.18 | loss  0.55 | cls  0.55 | err  0.16 | \n",
      "scGPT - INFO - | epoch   7 | 2300/2309 batches | lr 0.0001 | ms/batch 223.16 | loss  0.61 | cls  0.61 | err  0.17 | \n",
      "valid/mse 0.6565395576539528 valid/err 0.18678938819707633 valid/dab 0.0 valid/sum_mse_dab 0.6565395576539528 epoch 7\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 529.44s | valid loss/mse 0.6565 | err 0.1868\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.6565\n",
      "random masking at epoch   8, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   8 | 100/2309 batches | lr 0.0000 | ms/batch 228.23 | loss  0.59 | cls  0.59 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 200/2309 batches | lr 0.0000 | ms/batch 223.21 | loss  0.52 | cls  0.52 | err  0.16 | \n",
      "scGPT - INFO - | epoch   8 | 300/2309 batches | lr 0.0000 | ms/batch 223.22 | loss  0.62 | cls  0.62 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 400/2309 batches | lr 0.0000 | ms/batch 223.13 | loss  0.54 | cls  0.54 | err  0.15 | \n",
      "scGPT - INFO - | epoch   8 | 500/2309 batches | lr 0.0000 | ms/batch 223.10 | loss  0.48 | cls  0.48 | err  0.14 | \n",
      "scGPT - INFO - | epoch   8 | 600/2309 batches | lr 0.0000 | ms/batch 223.04 | loss  0.57 | cls  0.57 | err  0.16 | \n",
      "scGPT - INFO - | epoch   8 | 700/2309 batches | lr 0.0000 | ms/batch 222.83 | loss  0.59 | cls  0.59 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 800/2309 batches | lr 0.0000 | ms/batch 223.04 | loss  0.57 | cls  0.57 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 900/2309 batches | lr 0.0000 | ms/batch 223.11 | loss  0.59 | cls  0.59 | err  0.17 | \n",
      "scGPT - INFO - | epoch   8 | 1000/2309 batches | lr 0.0000 | ms/batch 223.10 | loss  0.62 | cls  0.62 | err  0.16 | \n",
      "scGPT - INFO - | epoch   8 | 1100/2309 batches | lr 0.0000 | ms/batch 223.09 | loss  0.51 | cls  0.51 | err  0.14 | \n",
      "scGPT - INFO - | epoch   8 | 1200/2309 batches | lr 0.0000 | ms/batch 223.10 | loss  0.61 | cls  0.61 | err  0.16 | \n",
      "scGPT - INFO - | epoch   8 | 1300/2309 batches | lr 0.0000 | ms/batch 223.09 | loss  0.60 | cls  0.60 | err  0.16 | \n",
      "scGPT - INFO - | epoch   8 | 1400/2309 batches | lr 0.0000 | ms/batch 223.09 | loss  0.53 | cls  0.53 | err  0.15 | \n",
      "scGPT - INFO - | epoch   8 | 1500/2309 batches | lr 0.0000 | ms/batch 223.06 | loss  0.47 | cls  0.47 | err  0.13 | \n",
      "scGPT - INFO - | epoch   8 | 1600/2309 batches | lr 0.0000 | ms/batch 223.04 | loss  0.55 | cls  0.55 | err  0.15 | \n",
      "scGPT - INFO - | epoch   8 | 1700/2309 batches | lr 0.0000 | ms/batch 222.97 | loss  0.53 | cls  0.53 | err  0.15 | \n",
      "scGPT - INFO - | epoch   8 | 1800/2309 batches | lr 0.0000 | ms/batch 223.03 | loss  0.52 | cls  0.52 | err  0.15 | \n",
      "scGPT - INFO - | epoch   8 | 1900/2309 batches | lr 0.0000 | ms/batch 223.04 | loss  0.53 | cls  0.53 | err  0.15 | \n",
      "scGPT - INFO - | epoch   8 | 2000/2309 batches | lr 0.0000 | ms/batch 223.16 | loss  0.50 | cls  0.50 | err  0.14 | \n",
      "scGPT - INFO - | epoch   8 | 2100/2309 batches | lr 0.0000 | ms/batch 223.08 | loss  0.50 | cls  0.50 | err  0.14 | \n",
      "scGPT - INFO - | epoch   8 | 2200/2309 batches | lr 0.0000 | ms/batch 223.10 | loss  0.52 | cls  0.52 | err  0.15 | \n",
      "scGPT - INFO - | epoch   8 | 2300/2309 batches | lr 0.0000 | ms/batch 223.12 | loss  0.55 | cls  0.55 | err  0.15 | \n",
      "valid/mse 0.579018222596688 valid/err 0.16080129940443963 valid/dab 0.0 valid/sum_mse_dab 0.579018222596688 epoch 8\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 529.65s | valid loss/mse 0.5790 | err 0.1608\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.5790\n",
      "random masking at epoch   9, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   9 | 100/2309 batches | lr 0.0000 | ms/batch 227.99 | loss  0.53 | cls  0.53 | err  0.14 | \n",
      "scGPT - INFO - | epoch   9 | 200/2309 batches | lr 0.0000 | ms/batch 223.20 | loss  0.45 | cls  0.45 | err  0.13 | \n",
      "scGPT - INFO - | epoch   9 | 300/2309 batches | lr 0.0000 | ms/batch 223.10 | loss  0.56 | cls  0.56 | err  0.15 | \n",
      "scGPT - INFO - | epoch   9 | 400/2309 batches | lr 0.0000 | ms/batch 223.16 | loss  0.47 | cls  0.47 | err  0.12 | \n",
      "scGPT - INFO - | epoch   9 | 500/2309 batches | lr 0.0000 | ms/batch 223.10 | loss  0.43 | cls  0.43 | err  0.12 | \n",
      "scGPT - INFO - | epoch   9 | 600/2309 batches | lr 0.0000 | ms/batch 223.07 | loss  0.51 | cls  0.51 | err  0.14 | \n",
      "scGPT - INFO - | epoch   9 | 700/2309 batches | lr 0.0000 | ms/batch 223.11 | loss  0.53 | cls  0.53 | err  0.15 | \n",
      "scGPT - INFO - | epoch   9 | 800/2309 batches | lr 0.0000 | ms/batch 223.10 | loss  0.55 | cls  0.55 | err  0.15 | \n",
      "scGPT - INFO - | epoch   9 | 900/2309 batches | lr 0.0000 | ms/batch 223.11 | loss  0.57 | cls  0.57 | err  0.16 | \n",
      "scGPT - INFO - | epoch   9 | 1000/2309 batches | lr 0.0000 | ms/batch 223.11 | loss  0.54 | cls  0.54 | err  0.14 | \n",
      "scGPT - INFO - | epoch   9 | 1100/2309 batches | lr 0.0000 | ms/batch 223.06 | loss  0.47 | cls  0.47 | err  0.13 | \n",
      "scGPT - INFO - | epoch   9 | 1200/2309 batches | lr 0.0000 | ms/batch 223.05 | loss  0.52 | cls  0.52 | err  0.14 | \n",
      "scGPT - INFO - | epoch   9 | 1300/2309 batches | lr 0.0000 | ms/batch 223.18 | loss  0.52 | cls  0.52 | err  0.14 | \n",
      "scGPT - INFO - | epoch   9 | 1400/2309 batches | lr 0.0000 | ms/batch 223.17 | loss  0.49 | cls  0.49 | err  0.13 | \n",
      "scGPT - INFO - | epoch   9 | 1500/2309 batches | lr 0.0000 | ms/batch 223.10 | loss  0.41 | cls  0.41 | err  0.11 | \n",
      "scGPT - INFO - | epoch   9 | 1600/2309 batches | lr 0.0000 | ms/batch 223.20 | loss  0.47 | cls  0.47 | err  0.13 | \n",
      "scGPT - INFO - | epoch   9 | 1700/2309 batches | lr 0.0000 | ms/batch 223.22 | loss  0.45 | cls  0.45 | err  0.13 | \n",
      "scGPT - INFO - | epoch   9 | 1800/2309 batches | lr 0.0000 | ms/batch 223.15 | loss  0.47 | cls  0.47 | err  0.13 | \n",
      "scGPT - INFO - | epoch   9 | 1900/2309 batches | lr 0.0000 | ms/batch 223.14 | loss  0.49 | cls  0.49 | err  0.12 | \n",
      "scGPT - INFO - | epoch   9 | 2000/2309 batches | lr 0.0000 | ms/batch 223.14 | loss  0.43 | cls  0.43 | err  0.12 | \n",
      "scGPT - INFO - | epoch   9 | 2100/2309 batches | lr 0.0000 | ms/batch 223.21 | loss  0.45 | cls  0.45 | err  0.12 | \n",
      "scGPT - INFO - | epoch   9 | 2200/2309 batches | lr 0.0000 | ms/batch 223.16 | loss  0.43 | cls  0.43 | err  0.12 | \n",
      "scGPT - INFO - | epoch   9 | 2300/2309 batches | lr 0.0000 | ms/batch 223.21 | loss  0.52 | cls  0.52 | err  0.13 | \n",
      "valid/mse 0.558778188602306 valid/err 0.15105576610720087 valid/dab 0.0 valid/sum_mse_dab 0.558778188602306 epoch 9\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 529.75s | valid loss/mse 0.5588 | err 0.1511\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.5588\n",
      "random masking at epoch  10, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  10 | 100/2309 batches | lr 0.0000 | ms/batch 227.89 | loss  0.47 | cls  0.47 | err  0.12 | \n",
      "scGPT - INFO - | epoch  10 | 200/2309 batches | lr 0.0000 | ms/batch 223.02 | loss  0.41 | cls  0.41 | err  0.12 | \n",
      "scGPT - INFO - | epoch  10 | 300/2309 batches | lr 0.0000 | ms/batch 223.09 | loss  0.49 | cls  0.49 | err  0.12 | \n",
      "scGPT - INFO - | epoch  10 | 400/2309 batches | lr 0.0000 | ms/batch 222.98 | loss  0.42 | cls  0.42 | err  0.10 | \n",
      "scGPT - INFO - | epoch  10 | 500/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.42 | cls  0.42 | err  0.12 | \n",
      "scGPT - INFO - | epoch  10 | 600/2309 batches | lr 0.0000 | ms/batch 222.97 | loss  0.46 | cls  0.46 | err  0.13 | \n",
      "scGPT - INFO - | epoch  10 | 700/2309 batches | lr 0.0000 | ms/batch 222.84 | loss  0.50 | cls  0.50 | err  0.13 | \n",
      "scGPT - INFO - | epoch  10 | 800/2309 batches | lr 0.0000 | ms/batch 222.97 | loss  0.50 | cls  0.50 | err  0.14 | \n",
      "scGPT - INFO - | epoch  10 | 900/2309 batches | lr 0.0000 | ms/batch 222.81 | loss  0.53 | cls  0.53 | err  0.14 | \n",
      "scGPT - INFO - | epoch  10 | 1000/2309 batches | lr 0.0000 | ms/batch 222.95 | loss  0.50 | cls  0.50 | err  0.13 | \n",
      "scGPT - INFO - | epoch  10 | 1100/2309 batches | lr 0.0000 | ms/batch 222.97 | loss  0.42 | cls  0.42 | err  0.11 | \n",
      "scGPT - INFO - | epoch  10 | 1200/2309 batches | lr 0.0000 | ms/batch 223.01 | loss  0.50 | cls  0.50 | err  0.13 | \n",
      "scGPT - INFO - | epoch  10 | 1300/2309 batches | lr 0.0000 | ms/batch 223.00 | loss  0.50 | cls  0.50 | err  0.13 | \n",
      "scGPT - INFO - | epoch  10 | 1400/2309 batches | lr 0.0000 | ms/batch 222.77 | loss  0.45 | cls  0.45 | err  0.12 | \n",
      "scGPT - INFO - | epoch  10 | 1500/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.39 | cls  0.39 | err  0.10 | \n",
      "scGPT - INFO - | epoch  10 | 1600/2309 batches | lr 0.0000 | ms/batch 222.99 | loss  0.43 | cls  0.43 | err  0.11 | \n",
      "scGPT - INFO - | epoch  10 | 1700/2309 batches | lr 0.0000 | ms/batch 222.95 | loss  0.41 | cls  0.41 | err  0.11 | \n",
      "scGPT - INFO - | epoch  10 | 1800/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.40 | cls  0.40 | err  0.11 | \n",
      "scGPT - INFO - | epoch  10 | 1900/2309 batches | lr 0.0000 | ms/batch 222.89 | loss  0.44 | cls  0.44 | err  0.11 | \n",
      "scGPT - INFO - | epoch  10 | 2000/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.39 | cls  0.39 | err  0.11 | \n",
      "scGPT - INFO - | epoch  10 | 2100/2309 batches | lr 0.0000 | ms/batch 222.94 | loss  0.38 | cls  0.38 | err  0.11 | \n",
      "scGPT - INFO - | epoch  10 | 2200/2309 batches | lr 0.0000 | ms/batch 222.95 | loss  0.35 | cls  0.35 | err  0.08 | \n",
      "scGPT - INFO - | epoch  10 | 2300/2309 batches | lr 0.0000 | ms/batch 222.99 | loss  0.47 | cls  0.47 | err  0.12 | \n",
      "valid/mse 0.4843408138640971 valid/err 0.12669193286410396 valid/dab 0.0 valid/sum_mse_dab 0.4843408138640971 epoch 10\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 529.34s | valid loss/mse 0.4843 | err 0.1267\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4843\n",
      "random masking at epoch  11, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  11 | 100/2309 batches | lr 0.0000 | ms/batch 227.92 | loss  0.41 | cls  0.41 | err  0.10 | \n",
      "scGPT - INFO - | epoch  11 | 200/2309 batches | lr 0.0000 | ms/batch 222.94 | loss  0.36 | cls  0.36 | err  0.09 | \n",
      "scGPT - INFO - | epoch  11 | 300/2309 batches | lr 0.0000 | ms/batch 223.01 | loss  0.47 | cls  0.47 | err  0.12 | \n",
      "scGPT - INFO - | epoch  11 | 400/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.39 | cls  0.39 | err  0.10 | \n",
      "scGPT - INFO - | epoch  11 | 500/2309 batches | lr 0.0000 | ms/batch 222.91 | loss  0.35 | cls  0.35 | err  0.10 | \n",
      "scGPT - INFO - | epoch  11 | 600/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.41 | cls  0.41 | err  0.12 | \n",
      "scGPT - INFO - | epoch  11 | 700/2309 batches | lr 0.0000 | ms/batch 222.99 | loss  0.45 | cls  0.45 | err  0.11 | \n",
      "scGPT - INFO - | epoch  11 | 800/2309 batches | lr 0.0000 | ms/batch 222.94 | loss  0.48 | cls  0.48 | err  0.13 | \n",
      "scGPT - INFO - | epoch  11 | 900/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.50 | cls  0.50 | err  0.13 | \n",
      "scGPT - INFO - | epoch  11 | 1000/2309 batches | lr 0.0000 | ms/batch 222.86 | loss  0.44 | cls  0.44 | err  0.12 | \n",
      "scGPT - INFO - | epoch  11 | 1100/2309 batches | lr 0.0000 | ms/batch 222.91 | loss  0.37 | cls  0.37 | err  0.10 | \n",
      "scGPT - INFO - | epoch  11 | 1200/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.43 | cls  0.43 | err  0.11 | \n",
      "scGPT - INFO - | epoch  11 | 1300/2309 batches | lr 0.0000 | ms/batch 222.91 | loss  0.45 | cls  0.45 | err  0.12 | \n",
      "scGPT - INFO - | epoch  11 | 1400/2309 batches | lr 0.0000 | ms/batch 222.96 | loss  0.41 | cls  0.41 | err  0.10 | \n",
      "scGPT - INFO - | epoch  11 | 1500/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.35 | cls  0.35 | err  0.09 | \n",
      "scGPT - INFO - | epoch  11 | 1600/2309 batches | lr 0.0000 | ms/batch 222.88 | loss  0.38 | cls  0.38 | err  0.10 | \n",
      "scGPT - INFO - | epoch  11 | 1700/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.36 | cls  0.36 | err  0.10 | \n",
      "scGPT - INFO - | epoch  11 | 1800/2309 batches | lr 0.0000 | ms/batch 222.88 | loss  0.39 | cls  0.39 | err  0.10 | \n",
      "scGPT - INFO - | epoch  11 | 1900/2309 batches | lr 0.0000 | ms/batch 222.91 | loss  0.40 | cls  0.40 | err  0.10 | \n",
      "scGPT - INFO - | epoch  11 | 2000/2309 batches | lr 0.0000 | ms/batch 222.94 | loss  0.34 | cls  0.34 | err  0.10 | \n",
      "scGPT - INFO - | epoch  11 | 2100/2309 batches | lr 0.0000 | ms/batch 222.95 | loss  0.33 | cls  0.33 | err  0.09 | \n",
      "scGPT - INFO - | epoch  11 | 2200/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.34 | cls  0.34 | err  0.09 | \n",
      "scGPT - INFO - | epoch  11 | 2300/2309 batches | lr 0.0000 | ms/batch 222.94 | loss  0.44 | cls  0.44 | err  0.11 | \n",
      "valid/mse 0.4152009053751786 valid/err 0.10232809962100704 valid/dab 0.0 valid/sum_mse_dab 0.4152009053751786 epoch 11\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 529.28s | valid loss/mse 0.4152 | err 0.1023\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.4152\n",
      "random masking at epoch  12, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  12 | 100/2309 batches | lr 0.0000 | ms/batch 227.90 | loss  0.37 | cls  0.37 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 200/2309 batches | lr 0.0000 | ms/batch 223.01 | loss  0.35 | cls  0.35 | err  0.10 | \n",
      "scGPT - INFO - | epoch  12 | 300/2309 batches | lr 0.0000 | ms/batch 222.99 | loss  0.40 | cls  0.40 | err  0.10 | \n",
      "scGPT - INFO - | epoch  12 | 400/2309 batches | lr 0.0000 | ms/batch 223.01 | loss  0.37 | cls  0.37 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 500/2309 batches | lr 0.0000 | ms/batch 222.99 | loss  0.32 | cls  0.32 | err  0.08 | \n",
      "scGPT - INFO - | epoch  12 | 600/2309 batches | lr 0.0000 | ms/batch 222.81 | loss  0.36 | cls  0.36 | err  0.10 | \n",
      "scGPT - INFO - | epoch  12 | 700/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.44 | cls  0.44 | err  0.10 | \n",
      "scGPT - INFO - | epoch  12 | 800/2309 batches | lr 0.0000 | ms/batch 222.88 | loss  0.44 | cls  0.44 | err  0.10 | \n",
      "scGPT - INFO - | epoch  12 | 900/2309 batches | lr 0.0000 | ms/batch 222.83 | loss  0.47 | cls  0.47 | err  0.12 | \n",
      "scGPT - INFO - | epoch  12 | 1000/2309 batches | lr 0.0000 | ms/batch 222.83 | loss  0.39 | cls  0.39 | err  0.10 | \n",
      "scGPT - INFO - | epoch  12 | 1100/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.35 | cls  0.35 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 1200/2309 batches | lr 0.0000 | ms/batch 222.86 | loss  0.41 | cls  0.41 | err  0.10 | \n",
      "scGPT - INFO - | epoch  12 | 1300/2309 batches | lr 0.0000 | ms/batch 222.83 | loss  0.38 | cls  0.38 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 1400/2309 batches | lr 0.0000 | ms/batch 222.86 | loss  0.39 | cls  0.39 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 1500/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.35 | cls  0.35 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 1600/2309 batches | lr 0.0000 | ms/batch 222.82 | loss  0.34 | cls  0.34 | err  0.08 | \n",
      "scGPT - INFO - | epoch  12 | 1700/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.36 | cls  0.36 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 1800/2309 batches | lr 0.0000 | ms/batch 222.89 | loss  0.37 | cls  0.37 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 1900/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.38 | cls  0.38 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 2000/2309 batches | lr 0.0000 | ms/batch 222.96 | loss  0.34 | cls  0.34 | err  0.09 | \n",
      "scGPT - INFO - | epoch  12 | 2100/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.30 | cls  0.30 | err  0.07 | \n",
      "scGPT - INFO - | epoch  12 | 2200/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.31 | cls  0.31 | err  0.08 | \n",
      "scGPT - INFO - | epoch  12 | 2300/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.42 | cls  0.42 | err  0.10 | \n",
      "valid/mse 0.4220707541625114 valid/err 0.10395235517054684 valid/dab 0.0 valid/sum_mse_dab 0.4220707541625114 epoch 12\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 529.25s | valid loss/mse 0.4221 | err 0.1040\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  13, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  13 | 100/2309 batches | lr 0.0000 | ms/batch 227.93 | loss  0.35 | cls  0.35 | err  0.09 | \n",
      "scGPT - INFO - | epoch  13 | 200/2309 batches | lr 0.0000 | ms/batch 223.00 | loss  0.29 | cls  0.29 | err  0.08 | \n",
      "scGPT - INFO - | epoch  13 | 300/2309 batches | lr 0.0000 | ms/batch 222.96 | loss  0.36 | cls  0.36 | err  0.08 | \n",
      "scGPT - INFO - | epoch  13 | 400/2309 batches | lr 0.0000 | ms/batch 222.96 | loss  0.35 | cls  0.35 | err  0.08 | \n",
      "scGPT - INFO - | epoch  13 | 500/2309 batches | lr 0.0000 | ms/batch 222.89 | loss  0.29 | cls  0.29 | err  0.07 | \n",
      "scGPT - INFO - | epoch  13 | 600/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.34 | cls  0.34 | err  0.09 | \n",
      "scGPT - INFO - | epoch  13 | 700/2309 batches | lr 0.0000 | ms/batch 222.91 | loss  0.39 | cls  0.39 | err  0.10 | \n",
      "scGPT - INFO - | epoch  13 | 800/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.41 | cls  0.41 | err  0.10 | \n",
      "scGPT - INFO - | epoch  13 | 900/2309 batches | lr 0.0000 | ms/batch 222.67 | loss  0.40 | cls  0.40 | err  0.10 | \n",
      "scGPT - INFO - | epoch  13 | 1000/2309 batches | lr 0.0000 | ms/batch 222.45 | loss  0.37 | cls  0.37 | err  0.09 | \n",
      "scGPT - INFO - | epoch  13 | 1100/2309 batches | lr 0.0000 | ms/batch 222.78 | loss  0.32 | cls  0.32 | err  0.08 | \n",
      "scGPT - INFO - | epoch  13 | 1200/2309 batches | lr 0.0000 | ms/batch 222.84 | loss  0.38 | cls  0.38 | err  0.10 | \n",
      "scGPT - INFO - | epoch  13 | 1300/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.41 | cls  0.41 | err  0.10 | \n",
      "scGPT - INFO - | epoch  13 | 1400/2309 batches | lr 0.0000 | ms/batch 222.81 | loss  0.37 | cls  0.37 | err  0.09 | \n",
      "scGPT - INFO - | epoch  13 | 1500/2309 batches | lr 0.0000 | ms/batch 222.82 | loss  0.35 | cls  0.35 | err  0.09 | \n",
      "scGPT - INFO - | epoch  13 | 1600/2309 batches | lr 0.0000 | ms/batch 222.85 | loss  0.34 | cls  0.34 | err  0.08 | \n",
      "scGPT - INFO - | epoch  13 | 1700/2309 batches | lr 0.0000 | ms/batch 222.81 | loss  0.29 | cls  0.29 | err  0.07 | \n",
      "scGPT - INFO - | epoch  13 | 1800/2309 batches | lr 0.0000 | ms/batch 222.86 | loss  0.33 | cls  0.33 | err  0.08 | \n",
      "scGPT - INFO - | epoch  13 | 1900/2309 batches | lr 0.0000 | ms/batch 222.75 | loss  0.36 | cls  0.36 | err  0.08 | \n",
      "scGPT - INFO - | epoch  13 | 2000/2309 batches | lr 0.0000 | ms/batch 222.73 | loss  0.30 | cls  0.30 | err  0.08 | \n",
      "scGPT - INFO - | epoch  13 | 2100/2309 batches | lr 0.0000 | ms/batch 222.67 | loss  0.27 | cls  0.27 | err  0.07 | \n",
      "scGPT - INFO - | epoch  13 | 2200/2309 batches | lr 0.0000 | ms/batch 222.76 | loss  0.28 | cls  0.28 | err  0.07 | \n",
      "scGPT - INFO - | epoch  13 | 2300/2309 batches | lr 0.0000 | ms/batch 222.85 | loss  0.39 | cls  0.39 | err  0.09 | \n",
      "valid/mse 0.4329557665333789 valid/err 0.09907958852192746 valid/dab 0.0 valid/sum_mse_dab 0.4329557665333789 epoch 13\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 529.02s | valid loss/mse 0.4330 | err 0.0991\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  14, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  14 | 100/2309 batches | lr 0.0000 | ms/batch 227.95 | loss  0.29 | cls  0.29 | err  0.07 | \n",
      "scGPT - INFO - | epoch  14 | 200/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.26 | cls  0.26 | err  0.07 | \n",
      "scGPT - INFO - | epoch  14 | 300/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.32 | cls  0.32 | err  0.08 | \n",
      "scGPT - INFO - | epoch  14 | 400/2309 batches | lr 0.0000 | ms/batch 222.99 | loss  0.34 | cls  0.34 | err  0.07 | \n",
      "scGPT - INFO - | epoch  14 | 500/2309 batches | lr 0.0000 | ms/batch 222.96 | loss  0.28 | cls  0.28 | err  0.07 | \n",
      "scGPT - INFO - | epoch  14 | 600/2309 batches | lr 0.0000 | ms/batch 223.02 | loss  0.33 | cls  0.33 | err  0.08 | \n",
      "scGPT - INFO - | epoch  14 | 700/2309 batches | lr 0.0000 | ms/batch 222.98 | loss  0.35 | cls  0.35 | err  0.08 | \n",
      "scGPT - INFO - | epoch  14 | 800/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.39 | cls  0.39 | err  0.09 | \n",
      "scGPT - INFO - | epoch  14 | 900/2309 batches | lr 0.0000 | ms/batch 222.88 | loss  0.37 | cls  0.37 | err  0.09 | \n",
      "scGPT - INFO - | epoch  14 | 1000/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.32 | cls  0.32 | err  0.09 | \n",
      "scGPT - INFO - | epoch  14 | 1100/2309 batches | lr 0.0000 | ms/batch 222.95 | loss  0.30 | cls  0.30 | err  0.08 | \n",
      "scGPT - INFO - | epoch  14 | 1200/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.37 | cls  0.37 | err  0.10 | \n",
      "scGPT - INFO - | epoch  14 | 1300/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.35 | cls  0.35 | err  0.08 | \n",
      "scGPT - INFO - | epoch  14 | 1400/2309 batches | lr 0.0000 | ms/batch 222.89 | loss  0.34 | cls  0.34 | err  0.08 | \n",
      "scGPT - INFO - | epoch  14 | 1500/2309 batches | lr 0.0000 | ms/batch 222.86 | loss  0.31 | cls  0.31 | err  0.08 | \n",
      "scGPT - INFO - | epoch  14 | 1600/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.27 | cls  0.27 | err  0.07 | \n",
      "scGPT - INFO - | epoch  14 | 1700/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.28 | cls  0.28 | err  0.07 | \n",
      "scGPT - INFO - | epoch  14 | 1800/2309 batches | lr 0.0000 | ms/batch 222.95 | loss  0.31 | cls  0.31 | err  0.08 | \n",
      "scGPT - INFO - | epoch  14 | 1900/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.33 | cls  0.33 | err  0.07 | \n",
      "scGPT - INFO - | epoch  14 | 2000/2309 batches | lr 0.0000 | ms/batch 222.85 | loss  0.25 | cls  0.25 | err  0.07 | \n",
      "scGPT - INFO - | epoch  14 | 2100/2309 batches | lr 0.0000 | ms/batch 222.86 | loss  0.25 | cls  0.25 | err  0.06 | \n",
      "scGPT - INFO - | epoch  14 | 2200/2309 batches | lr 0.0000 | ms/batch 222.76 | loss  0.23 | cls  0.23 | err  0.05 | \n",
      "scGPT - INFO - | epoch  14 | 2300/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.33 | cls  0.33 | err  0.08 | \n",
      "valid/mse 0.3430924691747961 valid/err 0.07904710341093665 valid/dab 0.0 valid/sum_mse_dab 0.3430924691747961 epoch 14\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 529.20s | valid loss/mse 0.3431 | err 0.0790\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.3431\n",
      "random masking at epoch  15, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  15 | 100/2309 batches | lr 0.0000 | ms/batch 227.72 | loss  0.27 | cls  0.27 | err  0.06 | \n",
      "scGPT - INFO - | epoch  15 | 200/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  15 | 300/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.30 | cls  0.30 | err  0.07 | \n",
      "scGPT - INFO - | epoch  15 | 400/2309 batches | lr 0.0000 | ms/batch 222.99 | loss  0.30 | cls  0.30 | err  0.07 | \n",
      "scGPT - INFO - | epoch  15 | 500/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.25 | cls  0.25 | err  0.06 | \n",
      "scGPT - INFO - | epoch  15 | 600/2309 batches | lr 0.0000 | ms/batch 222.89 | loss  0.28 | cls  0.28 | err  0.07 | \n",
      "scGPT - INFO - | epoch  15 | 700/2309 batches | lr 0.0000 | ms/batch 222.95 | loss  0.33 | cls  0.33 | err  0.08 | \n",
      "scGPT - INFO - | epoch  15 | 800/2309 batches | lr 0.0000 | ms/batch 222.82 | loss  0.34 | cls  0.34 | err  0.09 | \n",
      "scGPT - INFO - | epoch  15 | 900/2309 batches | lr 0.0000 | ms/batch 222.91 | loss  0.32 | cls  0.32 | err  0.08 | \n",
      "scGPT - INFO - | epoch  15 | 1000/2309 batches | lr 0.0000 | ms/batch 222.88 | loss  0.30 | cls  0.30 | err  0.08 | \n",
      "scGPT - INFO - | epoch  15 | 1100/2309 batches | lr 0.0000 | ms/batch 222.88 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  15 | 1200/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.32 | cls  0.32 | err  0.08 | \n",
      "scGPT - INFO - | epoch  15 | 1300/2309 batches | lr 0.0000 | ms/batch 222.91 | loss  0.34 | cls  0.34 | err  0.08 | \n",
      "scGPT - INFO - | epoch  15 | 1400/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.31 | cls  0.31 | err  0.07 | \n",
      "scGPT - INFO - | epoch  15 | 1500/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.26 | cls  0.26 | err  0.07 | \n",
      "scGPT - INFO - | epoch  15 | 1600/2309 batches | lr 0.0000 | ms/batch 222.83 | loss  0.27 | cls  0.27 | err  0.07 | \n",
      "scGPT - INFO - | epoch  15 | 1700/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  15 | 1800/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.28 | cls  0.28 | err  0.07 | \n",
      "scGPT - INFO - | epoch  15 | 1900/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.32 | cls  0.32 | err  0.07 | \n",
      "scGPT - INFO - | epoch  15 | 2000/2309 batches | lr 0.0000 | ms/batch 222.91 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "scGPT - INFO - | epoch  15 | 2100/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  15 | 2200/2309 batches | lr 0.0000 | ms/batch 222.86 | loss  0.23 | cls  0.23 | err  0.05 | \n",
      "scGPT - INFO - | epoch  15 | 2300/2309 batches | lr 0.0000 | ms/batch 222.86 | loss  0.29 | cls  0.29 | err  0.08 | \n",
      "valid/mse 0.3648918121609419 valid/err 0.08391987005955603 valid/dab 0.0 valid/sum_mse_dab 0.3648918121609419 epoch 15\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 529.18s | valid loss/mse 0.3649 | err 0.0839\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  16, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  16 | 100/2309 batches | lr 0.0000 | ms/batch 227.56 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  16 | 200/2309 batches | lr 0.0000 | ms/batch 222.96 | loss  0.18 | cls  0.18 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 300/2309 batches | lr 0.0000 | ms/batch 222.95 | loss  0.27 | cls  0.27 | err  0.07 | \n",
      "scGPT - INFO - | epoch  16 | 400/2309 batches | lr 0.0000 | ms/batch 222.95 | loss  0.27 | cls  0.27 | err  0.06 | \n",
      "scGPT - INFO - | epoch  16 | 500/2309 batches | lr 0.0000 | ms/batch 222.88 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 600/2309 batches | lr 0.0000 | ms/batch 222.81 | loss  0.28 | cls  0.28 | err  0.07 | \n",
      "scGPT - INFO - | epoch  16 | 700/2309 batches | lr 0.0000 | ms/batch 222.82 | loss  0.30 | cls  0.30 | err  0.06 | \n",
      "scGPT - INFO - | epoch  16 | 800/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.32 | cls  0.32 | err  0.08 | \n",
      "scGPT - INFO - | epoch  16 | 900/2309 batches | lr 0.0000 | ms/batch 222.83 | loss  0.32 | cls  0.32 | err  0.07 | \n",
      "scGPT - INFO - | epoch  16 | 1000/2309 batches | lr 0.0000 | ms/batch 222.84 | loss  0.28 | cls  0.28 | err  0.07 | \n",
      "scGPT - INFO - | epoch  16 | 1100/2309 batches | lr 0.0000 | ms/batch 222.80 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 1200/2309 batches | lr 0.0000 | ms/batch 222.88 | loss  0.30 | cls  0.30 | err  0.07 | \n",
      "scGPT - INFO - | epoch  16 | 1300/2309 batches | lr 0.0000 | ms/batch 222.80 | loss  0.28 | cls  0.28 | err  0.07 | \n",
      "scGPT - INFO - | epoch  16 | 1400/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  16 | 1500/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  16 | 1600/2309 batches | lr 0.0000 | ms/batch 222.91 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 1700/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.21 | cls  0.21 | err  0.04 | \n",
      "scGPT - INFO - | epoch  16 | 1800/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.30 | cls  0.30 | err  0.06 | \n",
      "scGPT - INFO - | epoch  16 | 1900/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.30 | cls  0.30 | err  0.06 | \n",
      "scGPT - INFO - | epoch  16 | 2000/2309 batches | lr 0.0000 | ms/batch 222.91 | loss  0.19 | cls  0.19 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 2100/2309 batches | lr 0.0000 | ms/batch 222.89 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 2200/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.23 | cls  0.23 | err  0.05 | \n",
      "scGPT - INFO - | epoch  16 | 2300/2309 batches | lr 0.0000 | ms/batch 222.85 | loss  0.26 | cls  0.26 | err  0.06 | \n",
      "valid/mse 0.2970477651439619 valid/err 0.06876015159718463 valid/dab 0.0 valid/sum_mse_dab 0.2970477651439619 epoch 16\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  16 | time: 529.11s | valid loss/mse 0.2970 | err 0.0688\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.2970\n",
      "random masking at epoch  17, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  17 | 100/2309 batches | lr 0.0000 | ms/batch 228.06 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 200/2309 batches | lr 0.0000 | ms/batch 223.17 | loss  0.15 | cls  0.15 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 300/2309 batches | lr 0.0000 | ms/batch 223.11 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 400/2309 batches | lr 0.0000 | ms/batch 223.10 | loss  0.23 | cls  0.23 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 500/2309 batches | lr 0.0000 | ms/batch 222.98 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 600/2309 batches | lr 0.0000 | ms/batch 223.06 | loss  0.28 | cls  0.28 | err  0.07 | \n",
      "scGPT - INFO - | epoch  17 | 700/2309 batches | lr 0.0000 | ms/batch 223.02 | loss  0.31 | cls  0.31 | err  0.07 | \n",
      "scGPT - INFO - | epoch  17 | 800/2309 batches | lr 0.0000 | ms/batch 223.04 | loss  0.28 | cls  0.28 | err  0.07 | \n",
      "scGPT - INFO - | epoch  17 | 900/2309 batches | lr 0.0000 | ms/batch 223.05 | loss  0.30 | cls  0.30 | err  0.07 | \n",
      "scGPT - INFO - | epoch  17 | 1000/2309 batches | lr 0.0000 | ms/batch 223.02 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  17 | 1100/2309 batches | lr 0.0000 | ms/batch 223.09 | loss  0.19 | cls  0.19 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 1200/2309 batches | lr 0.0000 | ms/batch 223.09 | loss  0.25 | cls  0.25 | err  0.06 | \n",
      "scGPT - INFO - | epoch  17 | 1300/2309 batches | lr 0.0000 | ms/batch 223.03 | loss  0.27 | cls  0.27 | err  0.07 | \n",
      "scGPT - INFO - | epoch  17 | 1400/2309 batches | lr 0.0000 | ms/batch 223.06 | loss  0.23 | cls  0.23 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 1500/2309 batches | lr 0.0000 | ms/batch 223.11 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 1600/2309 batches | lr 0.0000 | ms/batch 223.16 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 1700/2309 batches | lr 0.0000 | ms/batch 223.08 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 1800/2309 batches | lr 0.0000 | ms/batch 223.07 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  17 | 1900/2309 batches | lr 0.0000 | ms/batch 223.00 | loss  0.26 | cls  0.26 | err  0.06 | \n",
      "scGPT - INFO - | epoch  17 | 2000/2309 batches | lr 0.0000 | ms/batch 223.07 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 2100/2309 batches | lr 0.0000 | ms/batch 223.06 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 2200/2309 batches | lr 0.0000 | ms/batch 223.09 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "scGPT - INFO - | epoch  17 | 2300/2309 batches | lr 0.0000 | ms/batch 223.12 | loss  0.25 | cls  0.25 | err  0.06 | \n",
      "valid/mse 0.26989088080416 valid/err 0.06172171088251218 valid/dab 0.0 valid/sum_mse_dab 0.26989088080416 epoch 17\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  17 | time: 529.60s | valid loss/mse 0.2699 | err 0.0617\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.2699\n",
      "random masking at epoch  18, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  18 | 100/2309 batches | lr 0.0000 | ms/batch 227.88 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 200/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.14 | cls  0.14 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 300/2309 batches | lr 0.0000 | ms/batch 222.89 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 400/2309 batches | lr 0.0000 | ms/batch 222.89 | loss  0.26 | cls  0.26 | err  0.05 | \n",
      "scGPT - INFO - | epoch  18 | 500/2309 batches | lr 0.0000 | ms/batch 222.83 | loss  0.16 | cls  0.16 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 600/2309 batches | lr 0.0000 | ms/batch 222.86 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  18 | 700/2309 batches | lr 0.0000 | ms/batch 222.84 | loss  0.30 | cls  0.30 | err  0.06 | \n",
      "scGPT - INFO - | epoch  18 | 800/2309 batches | lr 0.0000 | ms/batch 222.88 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  18 | 900/2309 batches | lr 0.0000 | ms/batch 222.85 | loss  0.27 | cls  0.27 | err  0.06 | \n",
      "scGPT - INFO - | epoch  18 | 1000/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.26 | cls  0.26 | err  0.06 | \n",
      "scGPT - INFO - | epoch  18 | 1100/2309 batches | lr 0.0000 | ms/batch 222.86 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "scGPT - INFO - | epoch  18 | 1200/2309 batches | lr 0.0000 | ms/batch 222.97 | loss  0.24 | cls  0.24 | err  0.05 | \n",
      "scGPT - INFO - | epoch  18 | 1300/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.26 | cls  0.26 | err  0.06 | \n",
      "scGPT - INFO - | epoch  18 | 1400/2309 batches | lr 0.0000 | ms/batch 222.96 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  18 | 1500/2309 batches | lr 0.0000 | ms/batch 222.91 | loss  0.19 | cls  0.19 | err  0.05 | \n",
      "scGPT - INFO - | epoch  18 | 1600/2309 batches | lr 0.0000 | ms/batch 222.87 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  18 | 1700/2309 batches | lr 0.0000 | ms/batch 222.91 | loss  0.17 | cls  0.17 | err  0.03 | \n",
      "scGPT - INFO - | epoch  18 | 1800/2309 batches | lr 0.0000 | ms/batch 222.85 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 1900/2309 batches | lr 0.0000 | ms/batch 222.84 | loss  0.24 | cls  0.24 | err  0.05 | \n",
      "scGPT - INFO - | epoch  18 | 2000/2309 batches | lr 0.0000 | ms/batch 222.88 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 2100/2309 batches | lr 0.0000 | ms/batch 222.89 | loss  0.16 | cls  0.16 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 2200/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "scGPT - INFO - | epoch  18 | 2300/2309 batches | lr 0.0000 | ms/batch 222.88 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "valid/mse 0.26025866558568567 valid/err 0.055224688684353006 valid/dab 0.0 valid/sum_mse_dab 0.26025866558568567 epoch 18\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  18 | time: 529.19s | valid loss/mse 0.2603 | err 0.0552\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.2603\n",
      "random masking at epoch  19, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  19 | 100/2309 batches | lr 0.0000 | ms/batch 228.02 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 200/2309 batches | lr 0.0000 | ms/batch 223.16 | loss  0.11 | cls  0.11 | err  0.03 | \n",
      "scGPT - INFO - | epoch  19 | 300/2309 batches | lr 0.0000 | ms/batch 223.15 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 400/2309 batches | lr 0.0000 | ms/batch 223.07 | loss  0.21 | cls  0.21 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 500/2309 batches | lr 0.0000 | ms/batch 223.12 | loss  0.16 | cls  0.16 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 600/2309 batches | lr 0.0000 | ms/batch 223.15 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  19 | 700/2309 batches | lr 0.0000 | ms/batch 223.17 | loss  0.25 | cls  0.25 | err  0.05 | \n",
      "scGPT - INFO - | epoch  19 | 800/2309 batches | lr 0.0000 | ms/batch 223.11 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  19 | 900/2309 batches | lr 0.0000 | ms/batch 223.07 | loss  0.24 | cls  0.24 | err  0.06 | \n",
      "scGPT - INFO - | epoch  19 | 1000/2309 batches | lr 0.0000 | ms/batch 223.03 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  19 | 1100/2309 batches | lr 0.0000 | ms/batch 223.09 | loss  0.16 | cls  0.16 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 1200/2309 batches | lr 0.0000 | ms/batch 223.04 | loss  0.21 | cls  0.21 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 1300/2309 batches | lr 0.0000 | ms/batch 223.09 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  19 | 1400/2309 batches | lr 0.0000 | ms/batch 223.02 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  19 | 1500/2309 batches | lr 0.0000 | ms/batch 223.07 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 1600/2309 batches | lr 0.0000 | ms/batch 223.04 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 1700/2309 batches | lr 0.0000 | ms/batch 223.03 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 1800/2309 batches | lr 0.0000 | ms/batch 223.04 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 1900/2309 batches | lr 0.0000 | ms/batch 223.06 | loss  0.23 | cls  0.23 | err  0.05 | \n",
      "scGPT - INFO - | epoch  19 | 2000/2309 batches | lr 0.0000 | ms/batch 223.05 | loss  0.15 | cls  0.15 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 2100/2309 batches | lr 0.0000 | ms/batch 223.28 | loss  0.14 | cls  0.14 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 2200/2309 batches | lr 0.0000 | ms/batch 223.11 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "scGPT - INFO - | epoch  19 | 2300/2309 batches | lr 0.0000 | ms/batch 223.04 | loss  0.20 | cls  0.20 | err  0.04 | \n",
      "valid/mse 0.22648395363162774 valid/err 0.04981050351922036 valid/dab 0.0 valid/sum_mse_dab 0.22648395363162774 epoch 19\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  19 | time: 529.62s | valid loss/mse 0.2265 | err 0.0498\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.2265\n",
      "random masking at epoch  20, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch  20 | 100/2309 batches | lr 0.0000 | ms/batch 227.81 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 200/2309 batches | lr 0.0000 | ms/batch 223.00 | loss  0.11 | cls  0.11 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 300/2309 batches | lr 0.0000 | ms/batch 222.90 | loss  0.16 | cls  0.16 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 400/2309 batches | lr 0.0000 | ms/batch 222.94 | loss  0.21 | cls  0.21 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 500/2309 batches | lr 0.0000 | ms/batch 222.88 | loss  0.13 | cls  0.13 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 600/2309 batches | lr 0.0000 | ms/batch 222.96 | loss  0.17 | cls  0.17 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 700/2309 batches | lr 0.0000 | ms/batch 222.93 | loss  0.22 | cls  0.22 | err  0.05 | \n",
      "scGPT - INFO - | epoch  20 | 800/2309 batches | lr 0.0000 | ms/batch 222.92 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 900/2309 batches | lr 0.0000 | ms/batch 222.98 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "scGPT - INFO - | epoch  20 | 1000/2309 batches | lr 0.0000 | ms/batch 222.79 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "scGPT - INFO - | epoch  20 | 1100/2309 batches | lr 0.0000 | ms/batch 222.98 | loss  0.15 | cls  0.15 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 1200/2309 batches | lr 0.0000 | ms/batch 222.98 | loss  0.20 | cls  0.20 | err  0.05 | \n",
      "scGPT - INFO - | epoch  20 | 1300/2309 batches | lr 0.0000 | ms/batch 222.94 | loss  0.19 | cls  0.19 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 1400/2309 batches | lr 0.0000 | ms/batch 222.97 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 1500/2309 batches | lr 0.0000 | ms/batch 222.79 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 1600/2309 batches | lr 0.0000 | ms/batch 223.03 | loss  0.18 | cls  0.18 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 1700/2309 batches | lr 0.0000 | ms/batch 223.01 | loss  0.16 | cls  0.16 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 1800/2309 batches | lr 0.0000 | ms/batch 222.98 | loss  0.16 | cls  0.16 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 1900/2309 batches | lr 0.0000 | ms/batch 223.00 | loss  0.21 | cls  0.21 | err  0.05 | \n",
      "scGPT - INFO - | epoch  20 | 2000/2309 batches | lr 0.0000 | ms/batch 222.99 | loss  0.14 | cls  0.14 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 2100/2309 batches | lr 0.0000 | ms/batch 222.96 | loss  0.13 | cls  0.13 | err  0.03 | \n",
      "scGPT - INFO - | epoch  20 | 2200/2309 batches | lr 0.0000 | ms/batch 222.98 | loss  0.15 | cls  0.15 | err  0.04 | \n",
      "scGPT - INFO - | epoch  20 | 2300/2309 batches | lr 0.0000 | ms/batch 222.95 | loss  0.18 | cls  0.18 | err  0.04 | \n",
      "valid/mse 0.22669127704520192 valid/err 0.04872766648619383 valid/dab 0.0 valid/sum_mse_dab 0.22669127704520192 epoch 20\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  20 | time: 529.37s | valid loss/mse 0.2267 | err 0.0487\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T12:14:23.092344Z",
     "start_time": "2025-01-02T12:14:23.083040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(model: nn.Module, adata: DataLoader) -> float:\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata.obs[\"celltype_id\"].tolist()  # make sure count from 0\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=include_zero_gene,\n",
    "    )\n",
    "\n",
    "    input_values_test = random_mask_value(\n",
    "        tokenized_test[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": input_values_test,\n",
    "        \"target_values\": tokenized_test[\"values\"],\n",
    "        \"batch_labels\": torch.from_numpy(batch_ids).long(),\n",
    "        \"celltype_labels\": torch.from_numpy(celltypes_labels).long(),\n",
    "    }\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=SeqDataset(test_data_pt),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    predictions = evaluate(\n",
    "        model,\n",
    "        loader=test_loader,\n",
    "        return_raw=True,\n",
    "    )\n",
    "\n",
    "    # compute accuracy, precision, recall, f1\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    accuracy = accuracy_score(celltypes_labels, predictions)\n",
    "    precision = precision_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    recall = recall_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    macro_f1 = f1_score(celltypes_labels, predictions, average=\"macro\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, \"\n",
    "        f\"Macro F1: {macro_f1:.3f}\"\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"test/accuracy\": accuracy,\n",
    "        \"test/precision\": precision,\n",
    "        \"test/recall\": recall,\n",
    "        \"test/macro_f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "    return predictions, celltypes_labels, results"
   ],
   "id": "3f1622aa0cf28246",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T12:15:20.401876Z",
     "start_time": "2025-01-02T12:14:23.145919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions, labels, results = test(best_model, adata_test)\n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "adata_test_raw.obs[\"predictions\"] = [id2type[p] for p in predictions]\n",
    "\n",
    "# plot\n",
    "palette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] \n",
    "palette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "palette_ = {c: palette_[i] for i, c in enumerate(celltypes)}\n",
    "\n",
    "with plt.rc_context({\"figure.figsize\": (6, 4), \"figure.dpi\": (300)}):\n",
    "    sc.pl.umap(\n",
    "        adata_test_raw,\n",
    "        color=[\"celltype\", \"predictions\"],\n",
    "        palette=palette_,\n",
    "        show=False,\n",
    "    )\n",
    "    plt.savefig(save_dir / \"results.png\", dpi=300)\n",
    "\n",
    "save_dict = {\n",
    "    \"predictions\": predictions,\n",
    "    \"labels\": labels,\n",
    "    \"results\": results,\n",
    "    \"id_maps\": id2type\n",
    "}\n",
    "with open(save_dir / \"results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(save_dict, f)\n",
    "\n",
    "# results[\"test/cell_umap\"] = wandb.Image(\n",
    "#     str(save_dir / \"results.png\"),\n",
    "#     caption=f\"predictions macro f1 {results['test/macro_f1']:.3f}\",\n",
    "# )\n",
    "# wandb.log(results)\n",
    "\n",
    "print(results)"
   ],
   "id": "a3e6453e0204ea5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid/mse 0.784816038466958 valid/err 0.1389941875157948 valid/dab 0.0 valid/sum_mse_dab 0.784816038466958 epoch 20\n",
      "scGPT - INFO - Accuracy: 0.861, Precision: 0.768, Recall: 0.733, Macro F1: 0.732\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Could not find 'umap' or 'X_umap' in .obsm\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m palette_ \u001B[38;5;241m=\u001B[39m {c: palette_[i] \u001B[38;5;28;01mfor\u001B[39;00m i, c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(celltypes)}\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m plt\u001B[38;5;241m.\u001B[39mrc_context({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfigure.figsize\u001B[39m\u001B[38;5;124m\"\u001B[39m: (\u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m4\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfigure.dpi\u001B[39m\u001B[38;5;124m\"\u001B[39m: (\u001B[38;5;241m300\u001B[39m)}):\n\u001B[0;32m---> 12\u001B[0m     \u001B[43msc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mumap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43madata_test_raw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcelltype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpredictions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpalette\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpalette_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m     plt\u001B[38;5;241m.\u001B[39msavefig(save_dir \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresults.png\u001B[39m\u001B[38;5;124m\"\u001B[39m, dpi\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m300\u001B[39m)\n\u001B[1;32m     20\u001B[0m save_dict \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredictions\u001B[39m\u001B[38;5;124m\"\u001B[39m: predictions,\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m: labels,\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m\"\u001B[39m: results,\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid_maps\u001B[39m\u001B[38;5;124m\"\u001B[39m: id2type\n\u001B[1;32m     25\u001B[0m }\n",
      "File \u001B[0;32m~/.conda/envs/MemVP-main/lib/python3.11/site-packages/scanpy/plotting/_tools/scatterplots.py:691\u001B[0m, in \u001B[0;36mumap\u001B[0;34m(adata, **kwargs)\u001B[0m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;129m@_wraps_plot_scatter\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;129m@_doc_params\u001B[39m(\n\u001B[1;32m    634\u001B[0m     adata_color_etc\u001B[38;5;241m=\u001B[39mdoc_adata_color_etc,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    638\u001B[0m )\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mumap\u001B[39m(adata: AnnData, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Figure \u001B[38;5;241m|\u001B[39m Axes \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[Axes] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    640\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\\\u001B[39;00m\n\u001B[1;32m    641\u001B[0m \u001B[38;5;124;03m    Scatter plot in UMAP basis.\u001B[39;00m\n\u001B[1;32m    642\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    689\u001B[0m \u001B[38;5;124;03m    tl.umap\u001B[39;00m\n\u001B[1;32m    690\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 691\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43madata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mumap\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/MemVP-main/lib/python3.11/site-packages/scanpy/plotting/_tools/scatterplots.py:145\u001B[0m, in \u001B[0;36membedding\u001B[0;34m(adata, basis, color, mask_obs, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, dimensions, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, marker, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m check_projection(projection)\n\u001B[1;32m    143\u001B[0m sanitize_anndata(adata)\n\u001B[0;32m--> 145\u001B[0m basis_values \u001B[38;5;241m=\u001B[39m \u001B[43m_get_basis\u001B[49m\u001B[43m(\u001B[49m\u001B[43madata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbasis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m dimensions \u001B[38;5;241m=\u001B[39m _components_to_dimensions(\n\u001B[1;32m    147\u001B[0m     components, dimensions, projection\u001B[38;5;241m=\u001B[39mprojection, total_dims\u001B[38;5;241m=\u001B[39mbasis_values\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    148\u001B[0m )\n\u001B[1;32m    149\u001B[0m args_3d \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(projection\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m3d\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m projection \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m3d\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m {}\n",
      "File \u001B[0;32m~/.conda/envs/MemVP-main/lib/python3.11/site-packages/scanpy/plotting/_tools/scatterplots.py:1174\u001B[0m, in \u001B[0;36m_get_basis\u001B[0;34m(adata, basis)\u001B[0m\n\u001B[1;32m   1172\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m adata\u001B[38;5;241m.\u001B[39mobsm[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbasis\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1173\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1174\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not find \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbasis\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mX_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbasis\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m in .obsm\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"Could not find 'umap' or 'X_umap' in .obsm\""
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T12:15:20.120392Z",
     "start_time": "2024-12-19T09:49:34.454267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "celltypes = list(celltypes)\n",
    "for i in set([id2type[p] for p in predictions]):\n",
    "    if i not in celltypes:\n",
    "        celltypes.remove(i)\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm = pd.DataFrame(cm, index=celltypes[:cm.shape[0]], columns=celltypes[:cm.shape[1]])\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\".1f\", cmap=\"Blues\")\n",
    "plt.savefig(save_dir / \"confusion_matrix.png\", dpi=300)\n",
    "\n",
    "# results[\"test/confusion_matrix\"] = wandb.Image(\n",
    "#     str(save_dir / \"confusion_matrix.png\"),\n",
    "#     caption=f\"confusion matrix\",\n",
    "# )"
   ],
   "id": "561219cd9081cc37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "image/png": {
       "width": 632,
       "height": 652
      }
     },
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
